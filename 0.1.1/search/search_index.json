{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>FRAM data package is a part of FRAM modelling framework developed by the Norwegian Water Resources and Energy Directorate (NVE). The package is responsible for connection between the database and the FRAM core package. It contains NVEs implementation of the Populator() class that populates the core model by translating the data from NVEs database into generic components so that the data can be further manipulated and sent to the power market model. </p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>To install the data package, run <code>pip install fram-data</code>. But we recommend  that you rather start by installing our simple demo to better understand how FRAM works.</p>"},{"location":"#nves-database","title":"NVEs database","text":"<p>NVEs database is a folder with data files with folder structure, file format and table format defined by NVE. See our demo database as an example. NVEs database is model-neutral meaning that the same data can be used to build different energy market models.</p> <p>FRAM is designed so that different users can use their own databases with file and table formats that suit them best. Than they need their own populator. </p>"},{"location":"#nves-populator","title":"NVEs populator","text":"<p>NVEs implementation of the populator supports file formats and table formats in NVEs database. Populator must be implemented using the populator interface defined in FRAM core.</p> <p>If you want to use your own database together with FRAM, you should create your own implementation of the populator. Use this package as an example.</p>"},{"location":"data_validation/","title":"Data Validation","text":"<p>This section explains how the framdata package performs validations of the dataset which is read by the system. For validation of the attribute tables we use the pandera Python package (https://pandera.readthedocs.io/en/stable/). Validation of time vector data is implemented in the NVETimeVectorLoader class and is optionally performed once while reading all data in a file.</p>"},{"location":"data_validation/#attribute-tables-validation","title":"Attribute Tables Validation","text":"<p>In the database_names folder in each {object name}Names file (concerned With creating objects from attribute tables) there is defined the {object name}Schema and an {object name}MetadataSchema classes. See for example the DemandNames module. These classes are responsible for table validation according to panderas DataFrame Models (https://pandera.readthedocs.io/en/stable/dataframe_models.html).</p> <p>With these Schemas one can use built in pandera checks or define your own if what you need is not covered.</p> <ul> <li> <p>To use built in checks, define a column with the pandera Field function under the table Schema. Available checks can be passed as arguments to the function. See for example:</p> </li> <li> <p>For specialized checks, a class method must be defined in the Schema class. This method takes either a single column (as a Series) or the Whole dataframe as arguments. The specialized validation is then implemented here. Return value should be a Series of bools representing values which have failed or passed. NB! If the input is a Series the names of the columns to check with this method must be listed in the pandera check decorator. If input is a DataFrame, the pandera dataframe_check decorator is used instead.</p> </li> </ul> <p>Error message description and formatting:</p> <ul> <li> <p>The validation_functions.py module in database_names contains commonly used checks for attribute tables and their metadata. Here one can also add user friendly descriptions of why a check fails using the STANDARD_CHECK_DESCRIPTION global variable. Keep in mind that it is the names of the validation methods defined in the schemas which must be defined here for the descriptions to show up during runtime. An alternative is to add descriptions in to the return dictionary of {Object ame}Names._get_unique_check_descriptions. This is meant for unique checks only defined and used by a single Schema.</p> </li> <li> <p>Validation methods which take a DataFrame as input need special formatting. Its purpose is to increase readability by converting the error dataframe from one message per error per column checked into one message per row where errors occured. This is performed by the {object name}Names._format_unique_checks method. As the columns are unique to each table, this method is necessarily specialized to the particular validation method. Therefore if a unique check of this sort is implemented, the formatting should be implemented along with it.</p> </li> </ul>"},{"location":"data_validation/#time-vector-validation","title":"Time Vector Validation","text":"<p>In the NVETimeVectorLoader class there is defined a method _validate_vector in which is implemented all checks to perform on every time vector in the Loader. This method takes as argument the ID of a vector defined in the Loader. From this ID, the vector values and index is retrieved and validated. New checks can be added here.</p>"},{"location":"editing_profiles/","title":"Editing Profiles","text":"<p>Profiles and other large time series (or time vectors) data with fine resolution are stored in HDF5/h5 files in the FRAM dataset to utilize the formats high reading performance. These are less straigth forward to open and change than the attribute files which are currently excel files. This section is therefore meant as a guide on how to create and edit HDF5 profiles in FRAM.</p>"},{"location":"editing_profiles/#file-structure","title":"File Structure","text":"<p>The first thing to know about the HDF5 format is that these files are structured in a system of so called groups and datasets. One can think of these as comparable to a hierarchichal file system of, respectively, folders and files. Groups reference other groups or datasets, while datasets contain the actual data. Tip for vscode users: the H5Web extension lets you open and view h5 files in a GUI in the vscode app.</p> <p>In the FRAM dataset, h5 files containing time vectors utilize groups and dataset in a specific way. There are mainly four different types of groups/datasets encountered at the first hierarchical level of the time vector files:</p> <ul> <li>index (Group): Used to define indexes for vectors where each index is supposed to only apply to a particular vector.</li> <li>common_index (Dataset): Contains one numpy array. This is a fallback index for vectors which have not defined their own index in the index group. Can also be used on purpose if many or all vectors have the same index.</li> <li>metadata (Group): Used to connect a specific set of metadata to a particular vector. This group can contain multiple groups which themselves contain datasets representing metadata fields.</li> <li>common_metadata (Group): Contains one set of metadata fields (stored as datasets) for all vectors without their own special metadata. Used in a similar way as common_index.</li> <li>vectors (Group): Contains datasets of numpy arrays with vector values connected to a unique ID. The same ID is used to connect the vector to a specific index or metadata.</li> </ul>"},{"location":"editing_profiles/#editing-h5-using-fram-data-api","title":"Editing h5 using FRAM-data API","text":"<p>Within the FRAM system's data package is the NVEH5TimeVectorEditor, offering methods for reading and editing h5 files.</p> <pre><code>from framdata.file_editors import NVEH5TimeVectorEditor\n\n# Read a h5 file into memory. To create a new file from scratch one can set source=None.\nfile_editor = NVEH5TimeVectorEditor(source=\"path/to/file.h5\")\n\n# You can for example scale your vector.\nvector_1 = file_editor.get_vector(\"v1\")\nfile_editor.set_vector(\"v1\", vector_1*0.8)\n\n# Set unit field in common metadata.\nfile_editor.set_common_metadata_by_key(\"Unit\", \"EUR/MWh\")\n\n# You can save to a new h5 file or overwrite the existing one.\nfile_editor.save_to_h5(\"path/to/file_2.h5\")\n</code></pre> <p>You can in principle set vectors as whatever numpy arrays you wish, but you should make sure they are the same length as their index, whether that is the common_index or a specific one. Otherwise the system will fail later when reading the file. </p> <p>Another method of editing the h5 files is to use the h5py package instead of the aforementioned API. You will in that case have to make extra care your files follow the defined file structure. Keep in mind that FRAM stores the index arrays and metadata in h5 files as type bytes (dtype of elements are bytes in case of index arrays) to avoid incompatible types with the h5 format. It is recommended that users follow this either by using the FRAM-data API or by explicitly converting types before saving with h5py. When loaded by FRAM, index arrays are cast as datetime objects and metadata keys as strings. Metadata values types can be read more about in Metadata.</p>"},{"location":"editing_profiles/#parquet-files","title":"Parquet files","text":"<p>FRAM also supports time vectors in parquet files. The formatting here is a little different. It is stored as a table with one column called \"DatetimeIndex\" which represents the index, and every other column representing a time vector. This means all vectors must have the same index within one file. Another restriction is that metadata must be the same for all vectors.</p> <p>NVEParquetTimeVectorEditor can be used to create and edit parquet time vectors. Alternatively the pyarrow and pandas python packages can be used to read parquet files. Keep in mind that to edit metadata, pyarrow must be used to access the tables binary metadata:</p> <pre><code>import pyarrow.parquet as pq\nimport pandas as pd\nfrom framdata.database_names.TimeVectorMetadataNames import TimeVectorMetadataNames as TvMn\n\n# accessing metadata\nmetadata = pq.ParquetFile(\"path/to/file.parquet\").schema_arrow.metadata\n\n# saving with metadata\ntable_df = pd.read_parquet(\"path/to/file.parquet\")\n\ntable = pa.Table.from_pandas(table_df)\n\n# add metadata to table schema, and ensure same encoding as FRAM uses.\nschema_with_meta = table.schema.with_metadata({str(k).encode(TvMn.ENCODING): str(v).encode(TvMn.ENCODING) for k, v in metadata.items()})\ntable = pa.Table.from_pandas(table_df, schema=schema_with_meta)\n\npq.write_table(table, \"path/to/file.parquet\")\n</code></pre>"},{"location":"metadata/","title":"Metadata","text":"<p>Metadata in the FRAM-data package and database is used either for helping users or to describe how time vectors behave within the FRAM-core system. This section is aimed at explaining the metadata one can find within the system.</p>"},{"location":"metadata/#attribute-metadata","title":"Attribute Metadata","text":"<p>In each exel file containing attributes of components, there is a \"Metadata\" sheet displaying a table. The columns of this table can be arranged into two types according to their purpose. </p> <p>User metadata: Optional information meant to help the user interact with the attribute table in the Data sheet.</p> <ul> <li>Reference - If there are references to IDs of for example time vectors located in other files one can write the name of the file here.</li> <li>Description - Description of the attribute column.</li> <li>Dtype - Meant to signify which data types, usually string or float, can be currently founc within the column.</li> </ul> <p>Constant time vector metadata: Metadata applied to float values written directly in columns instead if ID references. The system interprets this as a time vector with constant value.</p> <ul> <li>Attribute - The name of the column in the Attribute table (Data sheet) that the current row of metadata applies to.</li> <li>Unit - The unit to use for constant time vectors in this column.</li> <li>IsMaxLevel, IsZeroOneProfile, RefPeriodStartYear and RefPeriodNumberOfYears - Describing the constant time vector's level/profile status along with its reference period if it has one. Read more under Time Vector Metadata</li> </ul>"},{"location":"metadata/#time-vector-metadata","title":"Time Vector Metadata","text":"<p>Metadata in files containing time vectors can be thougth of as a dictionary. Some of the fields/keys defined and used by FRAM are required. Asence of these will result in getting Exceptions when the file is read by an NVETimeVectorLoader. Following is a list of the required and optional metadata fields for timevectors.</p> <p>Required fields:</p> <ul> <li>IsMaxLevel (bool|None) - Must be defined as True or False (None for profiles) for timevectors representing levels. A level is either the maximum of a time series in a given period or an average over a reference period.</li> <li>IsZeroOneProfile (bool|None) - Must be defined as True or False for profiles. A profile represents either a percentage of a max level or deviation from the level average with reference period in a given time step (called a mean one profile, since the average over the reference period is 1).</li> <li>Is52WeekYears (bool) - True if the time vector contains yearly data standardized with 52 weeks.</li> <li>ExtrapolateFirstPoint (bool) - Apply extrapolation to first point backwards in time if True.</li> <li>ExtrapolateLastPoint (bool) - Apply extrapolation to last point forwards in time if True.</li> <li>RefPeriodStartYear (int|None) - Start year of a reference period used to calculate average levels and mean one profiles. Must be defined as an integer if either IsMaxLevel or IsZeroOneProfile is False.</li> <li>RefPeriodNumberOfYears (int|None) - Number of years the reference period spans. Must be defined as integer if RefPeriodStartYear is defined.</li> <li>TimeZone (tzinfo|None) - Must be defined but can be None. This is to enforce a concious choice.</li> <li>Unit (str|None) - Must be defined but can be None.</li> </ul> <p>Optional fields:</p> <ul> <li>ID (str|None) - Only relevant for timevectors stored in excel in the horizontal format. Can be used to denote the name of the column containing vector IDs. If not defined or None, the column name \"ID\" is assumed.</li> <li>StartDateTime (datetime|None) - Start datetime in the index of the timevector.</li> <li>Frequency (timedelta|None) - The fixed frequency between datetimes the index. If defined it must be fixed, in other words the timedelta between a given point and its succeeding point must be the same for all points. </li> <li>NumberOfPoints (int|None) - Total number of points/length of the vector and its index.</li> </ul> <p>StartDateTime, Frequency and NumberOfPoints can be used as an optimization to represent the index more compactly. The system will find be able to find these values regardless by reading/calculating them if they are not defined in the metadata, but defining them here may improve performance when reading the data and setting up the Model.</p>"},{"location":"reference/","title":"Code Reference","text":""},{"location":"reference/#framdata","title":"<code>framdata</code>","text":""},{"location":"reference/#framdata.database_names","title":"<code>database_names</code>","text":""},{"location":"reference/#framdata.database_names.DatabaseNames","title":"<code>DatabaseNames</code>","text":"<p>Container for names and locations of files and folders in the NVE database.</p>"},{"location":"reference/#framdata.database_names.DatabaseNames.DatabaseNames","title":"<code>DatabaseNames</code>","text":"<p>               Bases: <code>Base</code></p> <p>Define names of files and folders in the NVE database and map files to folders.</p> Source code in <code>framdata/database_names/DatabaseNames.py</code> <pre><code>class DatabaseNames(Base):\n    \"\"\"Define names of files and folders in the NVE database and map files to folders.\"\"\"\n\n    # ---------- FILE EXTENSIONS ---------- #\n    ext_excel = \".xlsx\"\n    ext_h5 = \".h5\"\n    ext_parquet = \".parquet\"\n    ext_yaml = \".yaml\"\n\n    # ---------- SHEETS ---------- #\n    data_sheet = \"Data\"\n    metadata_sheet = \"Metadata\"\n\n    # ---------- SUFFIXES ---------- #\n    capacity = \".capacity\"\n    prices = \".prices\"\n    profiles = \".profiles\"\n    curves = \".curves\"\n\n    # ---------- DATABASE FOLDERs MAP ---------- #\n    db00 = \"db00_nodes\"\n    db01 = \"db01_nodes_time_vectors\"\n    db10 = \"db10_wind\"\n    db20 = \"db20_solar\"\n    db30 = \"db30_hydropower\"\n    db31 = \"db31_hydropower_time_vectors\"\n    db32 = \"db32_hydropower_curves\"\n    db40 = \"db40_thermal\"\n    # db41 = \"db41_thermal_time_vectors\"\n    db50 = \"db50_demand\"\n    # db51 = \"db51_demand_time_vectors\"\n    db60 = \"db60_transmission\"\n    # db61 = \"db61_transmission_time_vectors\"\n\n    db_folder_list: ClassVar[list] = [db00, db01, db10, db20, db30, db31, db32, db40, db50, db60]\n\n    # ---------- FILENAMES ---------- #\n    # ==== NODES ====\n    power_nodes = \"Power.Nodes\"\n    power_nodes_prices = \"Power.Nodes.prices\"\n    power_nodes_profiles = \"Power.Nodes.profiles\"\n\n    fuel_nodes = \"Fuel.Nodes\"\n    fuel_nodes_prices = \"Fuel.Nodes.prices\"\n    fuel_nodes_profiles = \"Fuel.Nodes.profiles\"\n\n    emission_nodes = \"Emission.Nodes\"\n    emission_nodes_prices = \"Emission.Nodes.prices\"\n    emission_nodes_profiles = \"Emission.Nodes.profiles\"\n\n    # ==== THERMAL ====\n    thermal_generators = \"Thermal.Generators\"\n    thermal_generators_capacity = \"Thermal.Generators.capacity\"\n    thermal_generators_profiles = \"Thermal.Generators.profiles\"\n\n    # ==== HYDROPOWER ====\n    # hydro attribute tables\n    hydro_modules = \"Hydropower.Modules\"\n    hydro_modules_volumecapacity = \"Hydropower.Modules.VolumeCapacity\"\n    hydro_modules_enekv_global_derived = \"Hydropower.Modules.enekv_global_derived\"\n    hydro_modules_reggrad_glob_derived = \"Hydropower.Modules.reggrad_glob_derived\"\n    hydro_modules_reggrad_lok_derived = \"Hydropower.Modules.reggrad_lok_derived\"\n    hydro_bypass = \"Hydropower.Bypass\"\n    hydro_generators = \"Hydropower.Generators\"\n    hydro_inflow = \"Hydropower.Inflow\"\n    hydro_inflow_yearvolume = \"Hydropower.Inflow.YearVolume\"\n    hydro_inflow_upstream_inflow_derived = \"Hydropower.Inflow.upstream_inflow_derived\"\n    hydro_pumps = \"Hydropower.Pumps\"\n    hydro_reservoirs = \"Hydropower.Reservoirs\"\n\n    # hydro time series\n    hydro_inflow_profiles = \"Hydropower.Inflow.profiles\"\n    hydro_bypass_operationalbounds_restrictions = \"Hydropower.Bypass.OperationalBounds.Restrictions\"\n    hydro_modules_operationalbounds_restrictions = \"Hydropower.Modules.OperationalBounds.Restrictions\"\n    hydro_reservoirs_operationalbounds_restrictions = \"Hydropower.Reservoirs.OperationalBounds.Restrictions\"\n    hydro_generators_energyeq_mid = \"Hydropower.Generators.EnergyEq_mid\"\n\n    # hydro curves\n    hydro_curves = \"Hydropower.curves\"\n    hydro_pqcurves = \"Hydropower.pqcurves\"\n\n    # ==== DEMAND ====\n    demand_consumers = \"Demand.Consumers\"\n    demand_consumers_capacity = \"Demand.Consumers.capacity\"\n    demand_consumers_normalprices = \"Demand.Consumers.normalprices\"\n    demand_consumers_profiles_weatheryears = \"Demand.Consumers.profiles.weatheryears\"\n    demand_consumers_profiles_oneyear = \"Demand.Consumers.profiles\"\n\n    # ==== WIND ====\n    wind_generators = \"Wind.Generators\"\n    wind_generators_capacity = \"Wind.Generators.capacity\"\n    wind_generators_profiles = \"Wind.Generators.profiles\"\n\n    # ==== SOLAR ====\n    solar_generators = \"Solar.Generators\"\n    solar_generators_capacity = \"Solar.Generators.capacity\"\n    solar_generators_profiles = \"Solar.Generators.profiles\"\n\n    # ==== Transmission ====\n    transmission_grid = \"Transmission.Grid\"\n    transmission_capacity = transmission_grid + \".capacity\"\n    transmission_loss = transmission_grid + \".loss\"\n    transmission_profiles = transmission_grid + \".profiles\"\n\n    # ---------- DATABASE FOLDER MAP ---------- #\n    db_folder_map: ClassVar[dict[str, list[str]]] = {\n        # ===: NODES ====,\n        power_nodes: db00,\n        fuel_nodes: db00,\n        emission_nodes: db00,\n        power_nodes_prices: db01,\n        fuel_nodes_prices: db01,\n        emission_nodes_prices: db01,\n        power_nodes_profiles: db01,\n        fuel_nodes_profiles: db01,\n        emission_nodes_profiles: db01,\n        # ===: HYDROPOWER ====,\n        # hydro attribute tables\n        hydro_modules: db30,\n        hydro_modules_volumecapacity: db30,\n        hydro_modules_enekv_global_derived: db30,\n        hydro_modules_reggrad_glob_derived: db30,\n        hydro_modules_reggrad_lok_derived: db30,\n        hydro_bypass: db30,\n        hydro_generators: db30,\n        hydro_inflow: db30,\n        hydro_inflow_yearvolume: db30,\n        hydro_inflow_upstream_inflow_derived: db30,\n        hydro_pumps: db30,\n        hydro_reservoirs: db30,\n        # hydro time series\n        hydro_inflow_profiles: db31,\n        hydro_bypass_operationalbounds_restrictions: db31,\n        hydro_modules_operationalbounds_restrictions: db31,\n        hydro_reservoirs_operationalbounds_restrictions: db31,\n        hydro_generators_energyeq_mid: db31,\n        # hydro curves\n        hydro_curves: db32,\n        hydro_pqcurves: db32,\n        # ==== THERMAL ====,\n        thermal_generators: db40,\n        thermal_generators_capacity: db40,\n        thermal_generators_profiles: db40,\n        # ==== DEMAND ====,\n        demand_consumers: db50,\n        demand_consumers_capacity: db50,\n        demand_consumers_normalprices: db50,\n        demand_consumers_profiles_weatheryears: db50,\n        demand_consumers_profiles_oneyear: db50,\n        # ==== WIND ====,\n        wind_generators: db10,\n        wind_generators_capacity: db10,\n        wind_generators_profiles: db10,\n        # ==== SOLAR ====\n        solar_generators: db20,\n        solar_generators_capacity: db20,\n        solar_generators_profiles: db20,\n        # ==== Transmission ====\n        transmission_grid: db60,\n        transmission_capacity: db60,\n        transmission_loss: db60,\n        transmission_profiles: db60,\n    }\n\n    @classmethod\n    def get_relative_folder_path(cls, file_id: str) -&gt; Path:\n        \"\"\"\n        Get the relative database folder path for a given file_id.\n\n        The relative path consists of database folder and file name.\n\n        Args:\n            file_id (str): Identifier for the file to retrieve.\n\n        Returns:\n            Path: The database folder name.\n\n        \"\"\"\n        try:\n            return Path(cls.db_folder_map[file_id])\n        except KeyError as e:\n            message = f\"File id '{file_id}' not found in database folder map.\"\n\n            raise KeyError(message) from e\n\n    @classmethod\n    def get_file_name(cls, source: Path, db_folder: str, file_id: str) -&gt; str | None:\n        \"\"\"\n        Get the name of a file, with extension, from a file ID and a path.\n\n        Args:\n            source (Path): Root path of the database.\n            db_folder (str): Database folder to look for the file in.\n            file_id (str): ID of file, i.e the name of the file without extension.\n\n        Raises:\n            RuntimeError: If multiple files with the same ID but different extensions are found.\n\n        Returns:\n            str | None: File ID and extension combined. If file is not found, return None.\n\n        \"\"\"\n        db_path = source / db_folder\n        if not db_path.exists():\n            message = f\"The database folder {db_path} does not exist.\"\n            raise FileNotFoundError(message)\n        candidate_extentions = set()\n        for file_path in db_path.iterdir():\n            if file_path.is_file() and file_path.stem == file_id:\n                candidate_extentions.add(file_path.suffix)\n        if len(candidate_extentions) &gt; 1:  # Multiple files of same ID. Ambiguous\n            message = (\n                f\"Found multiple files with ID {file_id} (with different extensions: {candidate_extentions}) in database folder {db_path}.\"\n                \" File names must be unique.\"\n            )\n            raise RuntimeError(message)\n        if len(candidate_extentions) == 0:  # No matching files.\n            return None\n            # message = f\"Found no file with ID {file_id} in database folder {db_path}.\"\n            # raise FileNotFoundError(message)\n\n        (extension,) = candidate_extentions  # We have only one candidate, so we extract it.\n        return file_id + extension\n</code></pre>"},{"location":"reference/#framdata.database_names.DatabaseNames.DatabaseNames.get_file_name","title":"<code>get_file_name(source: Path, db_folder: str, file_id: str) -&gt; str | None</code>  <code>classmethod</code>","text":"<p>Get the name of a file, with extension, from a file ID and a path.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path</code> <p>Root path of the database.</p> required <code>db_folder</code> <code>str</code> <p>Database folder to look for the file in.</p> required <code>file_id</code> <code>str</code> <p>ID of file, i.e the name of the file without extension.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If multiple files with the same ID but different extensions are found.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: File ID and extension combined. If file is not found, return None.</p> Source code in <code>framdata/database_names/DatabaseNames.py</code> <pre><code>@classmethod\ndef get_file_name(cls, source: Path, db_folder: str, file_id: str) -&gt; str | None:\n    \"\"\"\n    Get the name of a file, with extension, from a file ID and a path.\n\n    Args:\n        source (Path): Root path of the database.\n        db_folder (str): Database folder to look for the file in.\n        file_id (str): ID of file, i.e the name of the file without extension.\n\n    Raises:\n        RuntimeError: If multiple files with the same ID but different extensions are found.\n\n    Returns:\n        str | None: File ID and extension combined. If file is not found, return None.\n\n    \"\"\"\n    db_path = source / db_folder\n    if not db_path.exists():\n        message = f\"The database folder {db_path} does not exist.\"\n        raise FileNotFoundError(message)\n    candidate_extentions = set()\n    for file_path in db_path.iterdir():\n        if file_path.is_file() and file_path.stem == file_id:\n            candidate_extentions.add(file_path.suffix)\n    if len(candidate_extentions) &gt; 1:  # Multiple files of same ID. Ambiguous\n        message = (\n            f\"Found multiple files with ID {file_id} (with different extensions: {candidate_extentions}) in database folder {db_path}.\"\n            \" File names must be unique.\"\n        )\n        raise RuntimeError(message)\n    if len(candidate_extentions) == 0:  # No matching files.\n        return None\n        # message = f\"Found no file with ID {file_id} in database folder {db_path}.\"\n        # raise FileNotFoundError(message)\n\n    (extension,) = candidate_extentions  # We have only one candidate, so we extract it.\n    return file_id + extension\n</code></pre>"},{"location":"reference/#framdata.database_names.DatabaseNames.DatabaseNames.get_relative_folder_path","title":"<code>get_relative_folder_path(file_id: str) -&gt; Path</code>  <code>classmethod</code>","text":"<p>Get the relative database folder path for a given file_id.</p> <p>The relative path consists of database folder and file name.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>Identifier for the file to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The database folder name.</p> Source code in <code>framdata/database_names/DatabaseNames.py</code> <pre><code>@classmethod\ndef get_relative_folder_path(cls, file_id: str) -&gt; Path:\n    \"\"\"\n    Get the relative database folder path for a given file_id.\n\n    The relative path consists of database folder and file name.\n\n    Args:\n        file_id (str): Identifier for the file to retrieve.\n\n    Returns:\n        Path: The database folder name.\n\n    \"\"\"\n    try:\n        return Path(cls.db_folder_map[file_id])\n    except KeyError as e:\n        message = f\"File id '{file_id}' not found in database folder map.\"\n\n        raise KeyError(message) from e\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames","title":"<code>DemandNames</code>","text":"<p>Contains classes defining the demand table and validations.</p>"},{"location":"reference/#framdata.database_names.DemandNames.DemandMetadataSchema","title":"<code>DemandMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Demand.Consumers file.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class DemandMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Demand.Consumers file.\"\"\"\n\n    @pa.dataframe_check\n    @classmethod\n    def check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n        Args:\n            df (Dataframe): DataFrame used to check value for \"unit\".\n\n        Returns:\n            Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n        \"\"\"\n        return check_unit_is_str_for_attributes(df, [DemandNames.capacity_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandMetadataSchema.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame used to check value for \"unit\".</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n    Args:\n        df (Dataframe): DataFrame used to check value for \"unit\".\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return check_unit_is_str_for_attributes(df, [DemandNames.capacity_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames","title":"<code>DemandNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Container class for describing the demand attribute table's names, structure, and convertion to Demand Component.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class DemandNames(_BaseComponentsNames):\n    \"\"\"Container class for describing the demand attribute table's names, structure, and convertion to Demand Component.\"\"\"\n\n    id_col = \"ConsumerID\"\n    node_col = \"PowerNode\"\n    reserve_price_col = \"ReservePrice\"\n    price_elasticity_col = \"PriceElasticity\"\n    min_price_col = \"MinPriceLimit\"\n    max_price_col = \"MaxPriceLimit\"\n    normal_price_col = \"NormalPrice\"\n    capacity_profile_col = \"CapacityProfile\"\n    temperature_profile_col = \"TemperatureProfile\"\n    capacity_col = \"Capacity\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        node_col,\n        reserve_price_col,\n        price_elasticity_col,\n        min_price_col,\n        max_price_col,\n        normal_price_col,\n        capacity_profile_col,\n        temperature_profile_col,\n        capacity_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        node_col,\n        reserve_price_col,\n        price_elasticity_col,\n        min_price_col,\n        max_price_col,\n        normal_price_col,\n        capacity_profile_col,\n        temperature_profile_col,\n        capacity_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Demand]:\n        \"\"\"\n        Create a Demand component from a table row in the Demand.Consumers file.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Demand object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n        Returns:\n            dict[str, Demand]: A dictionary with the consumer_id as key and the demand component as value.\n\n        \"\"\"\n        elastic_demand_cols = [\n            DemandNames.price_elasticity_col,\n            DemandNames.min_price_col,\n            DemandNames.max_price_col,\n            DemandNames.normal_price_col,\n        ]\n        columns_to_parse = [\n            DemandNames.reserve_price_col,\n            DemandNames.capacity_profile_col,\n            DemandNames.temperature_profile_col,\n            DemandNames.capacity_col,\n        ]\n        columns_to_parse.extend(elastic_demand_cols)\n\n        arg_user_code = DemandNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        elastic_demand_values = [value for key, value in arg_user_code.items() if key in elastic_demand_cols]\n        if all(value is not None for value in elastic_demand_values):\n            elastic_demand = ElasticDemand(\n                price_elasticity=Elasticity(level=arg_user_code[DemandNames.price_elasticity_col]),\n                min_price=Price(level=arg_user_code[DemandNames.min_price_col]),\n                normal_price=Price(level=arg_user_code[DemandNames.normal_price_col]),\n                max_price=Price(level=arg_user_code[DemandNames.max_price_col]),\n            )\n            reserve_price = None\n        elif arg_user_code[DemandNames.reserve_price_col] is not None:\n            elastic_demand = None\n            reserve_price = ReservePrice(level=arg_user_code[DemandNames.reserve_price_col])\n        else:\n            elastic_demand = None\n            reserve_price = None\n        demand = Demand(\n            node=row[indices[DemandNames.node_col]],\n            capacity=MaxFlowVolume(\n                level=arg_user_code[DemandNames.capacity_col],\n                profile=arg_user_code[DemandNames.capacity_profile_col],\n            ),\n            reserve_price=reserve_price,\n            elastic_demand=elastic_demand,\n            temperature_profile=arg_user_code[DemandNames.temperature_profile_col],\n        )\n        DemandNames._add_meta(demand, row, indices, meta_columns)\n\n        return {row[indices[DemandNames.id_col]]: demand}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.\n\n        Returns:\n            DemandSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand attribute data.\n\n        \"\"\"\n        return DemandSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for metadata in the Demand.Consumers file.\n\n        Returns:\n            DemandMetadataSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand metadata.\n\n        \"\"\"\n        return DemandMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Demand schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n        \"\"\"\n        return {\n            DemandSchema.check_elastic_demand.__name__: (\"Missing elastic demand value.\", True),\n        }\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Demand schemas.\n\n        This method processes validation errors that come from a dataframe-level check on elastic demand columns in the\n        attribute data schema. The default reporting on failed dataframe-level checks in Pandera's standard error\n        reports DataFrame (errors) is not very user-friendly. It can contain uneccassary rows about columns that are not\n        relevant to the check and will not include rows about the columns relevant to the check if those columns have\n        missing values. This method removes uneccassary rows from the error dataframe and ensures that rows with\n        information abot the elastic demand columns that fail the check are included.\n\n        Args:\n            errors (pd.DataFrame): DataFrame containing validation errors. Pandera's standard error reports DataFrame.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        if DemandSchema.check_elastic_demand.__name__ in errors[DemandNames.COL_CHECK].to_numpy():\n            check_rows = errors.loc[errors[DemandNames.COL_CHECK] == DemandSchema.check_elastic_demand.__name__]\n            errors = errors[~(errors[DemandNames.COL_CHECK] == DemandSchema.check_elastic_demand.__name__)]\n            elastic_demand_columns = [\n                DemandNames.price_elasticity_col,\n                DemandNames.min_price_col,\n                DemandNames.max_price_col,\n                DemandNames.normal_price_col,\n            ]\n            check_description_str = check_rows[DemandNames.COL_CHECK_DESC].unique()[0]\n            elastic_demand_rows = []\n            for idx in check_rows[DemandNames.COL_IDX].unique():\n                check_case = check_rows[check_rows[DemandNames.COL_IDX] == idx]\n                for col in elastic_demand_columns:\n                    if col not in list(check_case[DemandNames.COL_COLUMN].unique()):\n                        elastic_demand_rows.append(\n                            [\n                                col,\n                                DemandSchema.check_elastic_demand.__name__,\n                                None,\n                                idx,\n                                check_description_str,\n                                True,\n                            ],\n                        )\n            errors = pd.concat([errors, pd.DataFrame(elastic_demand_rows, columns=errors.columns)], ignore_index=True)\n        return errors\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Demand]</code>  <code>staticmethod</code>","text":"<p>Create a Demand component from a table row in the Demand.Consumers file.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Demand object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>list[str]</code> <p>Set of columns which defines memberships in meta groups for aggregation.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Demand]</code> <p>dict[str, Demand]: A dictionary with the consumer_id as key and the demand component as value.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Demand]:\n    \"\"\"\n    Create a Demand component from a table row in the Demand.Consumers file.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Demand object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n    Returns:\n        dict[str, Demand]: A dictionary with the consumer_id as key and the demand component as value.\n\n    \"\"\"\n    elastic_demand_cols = [\n        DemandNames.price_elasticity_col,\n        DemandNames.min_price_col,\n        DemandNames.max_price_col,\n        DemandNames.normal_price_col,\n    ]\n    columns_to_parse = [\n        DemandNames.reserve_price_col,\n        DemandNames.capacity_profile_col,\n        DemandNames.temperature_profile_col,\n        DemandNames.capacity_col,\n    ]\n    columns_to_parse.extend(elastic_demand_cols)\n\n    arg_user_code = DemandNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    elastic_demand_values = [value for key, value in arg_user_code.items() if key in elastic_demand_cols]\n    if all(value is not None for value in elastic_demand_values):\n        elastic_demand = ElasticDemand(\n            price_elasticity=Elasticity(level=arg_user_code[DemandNames.price_elasticity_col]),\n            min_price=Price(level=arg_user_code[DemandNames.min_price_col]),\n            normal_price=Price(level=arg_user_code[DemandNames.normal_price_col]),\n            max_price=Price(level=arg_user_code[DemandNames.max_price_col]),\n        )\n        reserve_price = None\n    elif arg_user_code[DemandNames.reserve_price_col] is not None:\n        elastic_demand = None\n        reserve_price = ReservePrice(level=arg_user_code[DemandNames.reserve_price_col])\n    else:\n        elastic_demand = None\n        reserve_price = None\n    demand = Demand(\n        node=row[indices[DemandNames.node_col]],\n        capacity=MaxFlowVolume(\n            level=arg_user_code[DemandNames.capacity_col],\n            profile=arg_user_code[DemandNames.capacity_profile_col],\n        ),\n        reserve_price=reserve_price,\n        elastic_demand=elastic_demand,\n        temperature_profile=arg_user_code[DemandNames.temperature_profile_col],\n    )\n    DemandNames._add_meta(demand, row, indices, meta_columns)\n\n    return {row[indices[DemandNames.id_col]]: demand}\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.</p> <p>Returns:</p> Name Type Description <code>DemandSchema</code> <code>DataFrameModel</code> <p>Pandera DataFrameModel schema for Demand attribute data.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.\n\n    Returns:\n        DemandSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand attribute data.\n\n    \"\"\"\n    return DemandSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for metadata in the Demand.Consumers file.</p> <p>Returns:</p> Name Type Description <code>DemandMetadataSchema</code> <code>DataFrameModel</code> <p>Pandera DataFrameModel schema for Demand metadata.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for metadata in the Demand.Consumers file.\n\n    Returns:\n        DemandMetadataSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand metadata.\n\n    \"\"\"\n    return DemandMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema","title":"<code>DemandSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class DemandSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.\"\"\"\n\n    ConsumerID: Series[str] = pa.Field(unique=True, nullable=False)\n    PowerNode: Series[str] = pa.Field(nullable=False)\n    ReservePrice: Series[Any] = pa.Field(nullable=True)\n    PriceElasticity: Series[Any] = pa.Field(nullable=True)\n    MinPriceLimit: Series[Any] = pa.Field(nullable=True)\n    MaxPriceLimit: Series[Any] = pa.Field(nullable=True)\n    NormalPrice: Series[Any] = pa.Field(nullable=True)\n    CapacityProfile: Series[Any] = pa.Field(nullable=True)\n    TemperatureProfile: Series[Any] = pa.Field(nullable=True)\n    Capacity: Series[Any] = pa.Field(nullable=False)\n\n    @pa.check(DemandNames.capacity_col)\n    @classmethod\n    def dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n        return dtype_str_int_float(series)\n\n    @pa.check(\n        DemandNames.reserve_price_col,\n        DemandNames.price_elasticity_col,\n        DemandNames.min_price_col,\n        DemandNames.max_price_col,\n        DemandNames.normal_price_col,\n        DemandNames.capacity_profile_col,\n        DemandNames.temperature_profile_col,\n    )\n    @classmethod\n    def dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n        return dtype_str_int_float_none(series)\n\n    @pa.check(DemandNames.price_elasticity_col)\n    @classmethod\n    def numeric_values_less_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are less than or equal to zero.\"\"\"\n        return numeric_values_less_than_or_equal_to(series, 0)\n\n    @pa.check(\n        DemandNames.reserve_price_col,\n        DemandNames.min_price_col,\n        DemandNames.max_price_col,\n        DemandNames.normal_price_col,\n        DemandNames.capacity_col,\n    )\n    @classmethod\n    def numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n        return numeric_values_greater_than_or_equal_to(series, 0)\n\n    @pa.check(DemandNames.capacity_profile_col)\n    @classmethod\n    def numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n        return numeric_values_are_between_or_equal_to(series, 0, 1)\n\n    @pa.dataframe_check\n    @classmethod\n    def check_elastic_demand(cls, df: DataFrame) -&gt; Series[bool]:\n        \"\"\"Check that all elastic demand values are present if one or more is.\"\"\"\n        elastic_demand = df[\n            [\n                DemandNames.price_elasticity_col,\n                DemandNames.min_price_col,\n                DemandNames.max_price_col,\n                DemandNames.normal_price_col,\n            ]\n        ]\n\n        check = elastic_demand.apply(\n            lambda row: all(value is not None for value in row) if any(value is not None for value in row) else True,\n            axis=1,\n        ).tolist()\n        return pd.Series(check)\n\n    class Config:\n        \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n        unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.Config","title":"<code>Config</code>","text":"<p>Schema-wide configuration for the DemandSchema class.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class Config:\n    \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n    unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.check_elastic_demand","title":"<code>check_elastic_demand(df: DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that all elastic demand values are present if one or more is.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_elastic_demand(cls, df: DataFrame) -&gt; Series[bool]:\n    \"\"\"Check that all elastic demand values are present if one or more is.\"\"\"\n    elastic_demand = df[\n        [\n            DemandNames.price_elasticity_col,\n            DemandNames.min_price_col,\n            DemandNames.max_price_col,\n            DemandNames.normal_price_col,\n        ]\n    ]\n\n    check = elastic_demand.apply(\n        lambda row: all(value is not None for value in row) if any(value is not None for value in row) else True,\n        axis=1,\n    ).tolist()\n    return pd.Series(check)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int or float.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(DemandNames.capacity_col)\n@classmethod\ndef dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n    return dtype_str_int_float(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int, float or None.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(\n    DemandNames.reserve_price_col,\n    DemandNames.price_elasticity_col,\n    DemandNames.min_price_col,\n    DemandNames.max_price_col,\n    DemandNames.normal_price_col,\n    DemandNames.capacity_profile_col,\n    DemandNames.temperature_profile_col,\n)\n@classmethod\ndef dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n    return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.numeric_values_are_between_or_equal_to_0_and_1","title":"<code>numeric_values_are_between_or_equal_to_0_and_1(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are between zero and one or equal to zero and one.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(DemandNames.capacity_profile_col)\n@classmethod\ndef numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n    return numeric_values_are_between_or_equal_to(series, 0, 1)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.numeric_values_greater_than_or_equal_to_0","title":"<code>numeric_values_greater_than_or_equal_to_0(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are greater than or equal to zero.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(\n    DemandNames.reserve_price_col,\n    DemandNames.min_price_col,\n    DemandNames.max_price_col,\n    DemandNames.normal_price_col,\n    DemandNames.capacity_col,\n)\n@classmethod\ndef numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n    return numeric_values_greater_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.numeric_values_less_than_or_equal_to_0","title":"<code>numeric_values_less_than_or_equal_to_0(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are less than or equal to zero.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(DemandNames.price_elasticity_col)\n@classmethod\ndef numeric_values_less_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are less than or equal to zero.\"\"\"\n    return numeric_values_less_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.H5Names","title":"<code>H5Names</code>","text":"<p>Define names and fields used in H5 files.</p>"},{"location":"reference/#framdata.database_names.H5Names.H5Names","title":"<code>H5Names</code>","text":"<p>Container class for names used in H5 files.</p> Source code in <code>framdata/database_names/H5Names.py</code> <pre><code>class H5Names:\n    \"\"\"Container class for names used in H5 files.\"\"\"\n\n    INDEX_GROUP = \"index\"\n    METADATA_GROUP = \"metadata\"\n    VECTORS_GROUP = \"vectors\"\n    COMMON_PREFIX = \"common_\"\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames","title":"<code>HydroBypassNames</code>","text":"<p>Contain the BypassNames class and related Pandera schemas for handling hydropower bypass data.</p> <p>Includes attribute and metadata schemas.</p>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassMetadataSchema","title":"<code>HydroBypassMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Bypass file.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>class HydroBypassMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Bypass file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames","title":"<code>HydroBypassNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Define naming conventions and attribute object creation for HydroBypass object, which is an attribute of the HydroModule.</p> <p>Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables, and formatting validation errors specific to generator schemas.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>class HydroBypassNames(_BaseComponentsNames):\n    \"\"\"\n    Define naming conventions and attribute object creation for HydroBypass object, which is an attribute of the HydroModule.\n\n    Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables,\n    and formatting validation errors specific to generator schemas.\n\n    \"\"\"\n\n    id_col = \"BypassID\"\n    to_col = \"BypassTo\"\n    cap_col = \"Capacity\"\n    min_bnd_col = \"MinOperationalBypass\"\n    min_penalty_col = \"MinViolationPenalty\"\n\n    columns: ClassVar[list[str]] = [id_col, to_col, cap_col, min_bnd_col, min_penalty_col]\n\n    ref_columns: ClassVar[list[str]] = [to_col, cap_col, min_bnd_col, min_penalty_col]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, HydroBypass]:\n        \"\"\"\n        Create a HydroBypass object.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, HydroBypass]: A dictionary with the bypass ID as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroBypassNames.id_col,\n            HydroBypassNames.to_col,\n            HydroBypassNames.cap_col,\n            HydroBypassNames.min_bnd_col,\n            HydroBypassNames.min_penalty_col,\n        ]\n\n        arg_user_code = HydroBypassNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        bypass = HydroBypass(\n            to_module=row[indices[HydroBypassNames.to_col]],\n            # capacity=SoftFlowCapacity(\n            #     level_input=arg_user_code[BypassNames.cap_col],\n            #     min_profile_input=arg_user_code[BypassNames.min_bnd_col],\n            #     min_penalty=arg_user_code[BypassNames.min_penalty_col],\n            # ),\n            capacity=MaxFlowVolume(level=arg_user_code[HydroBypassNames.cap_col]),\n        )\n\n        meta = {}\n        HydroBypassNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroBypassNames.id_col]]: (bypass, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass attribute data.\n\n        \"\"\"\n        return HydroBypassSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Bypass file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass metadata.\n\n        \"\"\"\n        return HydroBypassMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Bypass schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Bypass schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, HydroBypass]</code>  <code>staticmethod</code>","text":"<p>Create a HydroBypass object.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, HydroBypass]</code> <p>dict[str, HydroBypass]: A dictionary with the bypass ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, HydroBypass]:\n    \"\"\"\n    Create a HydroBypass object.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, HydroBypass]: A dictionary with the bypass ID as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroBypassNames.id_col,\n        HydroBypassNames.to_col,\n        HydroBypassNames.cap_col,\n        HydroBypassNames.min_bnd_col,\n        HydroBypassNames.min_penalty_col,\n    ]\n\n    arg_user_code = HydroBypassNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    bypass = HydroBypass(\n        to_module=row[indices[HydroBypassNames.to_col]],\n        # capacity=SoftFlowCapacity(\n        #     level_input=arg_user_code[BypassNames.cap_col],\n        #     min_profile_input=arg_user_code[BypassNames.min_bnd_col],\n        #     min_penalty=arg_user_code[BypassNames.min_penalty_col],\n        # ),\n        capacity=MaxFlowVolume(level=arg_user_code[HydroBypassNames.cap_col]),\n    )\n\n    meta = {}\n    HydroBypassNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroBypassNames.id_col]]: (bypass, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass attribute data.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass attribute data.\n\n    \"\"\"\n    return HydroBypassSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Bypass file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass metadata.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Bypass file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass metadata.\n\n    \"\"\"\n    return HydroBypassMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassSchema","title":"<code>HydroBypassSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>class HydroBypassSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames","title":"<code>HydroGeneratorNames</code>","text":"<p>Define the GeneratorNames class and related Pandera schemas for hydropower generator data.</p> <p>Provides: - GeneratorNames: class for handling generator component names and schema validation. - GeneratorSchema: Pandera schema for generator attribute data. - GeneratorMetadataSchema: Pandera schema for generator metadata.</p>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.GeneratorMetadataSchema","title":"<code>GeneratorMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Generators file.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>class GeneratorMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Generators file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.GeneratorSchema","title":"<code>GeneratorSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>class GeneratorSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames","title":"<code>HydroGeneratorNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Handles generator component names and schema validation for hydropower generator data.</p> <p>Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables, and formatting validation errors specific to generator schemas.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>class HydroGeneratorNames(_BaseComponentsNames):\n    \"\"\"\n    Handles generator component names and schema validation for hydropower generator data.\n\n    Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables,\n    and formatting validation errors specific to generator schemas.\n    \"\"\"\n\n    id_col = \"GeneratorID\"\n    node_col = \"PowerNode\"\n    pq_curve_col = \"PQCurve\"\n    tailw_elev_col = \"TailwaterElevation\"\n    head_nom_col = \"NominalHead\"\n    en_eq_col = \"EnergyEq\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        node_col,\n        pq_curve_col,\n        tailw_elev_col,\n        head_nom_col,\n        en_eq_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        node_col,\n        pq_curve_col,\n        tailw_elev_col,\n        head_nom_col,\n        en_eq_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, tuple[HydroGenerator, dict[str, Meta]]]:\n        \"\"\"\n        Create a hydro generator attribute object.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, dict[str, Meta]]: A dictionary with the generator ID as key and the attribute object and metadata as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroGeneratorNames.pq_curve_col,\n            HydroGeneratorNames.tailw_elev_col,\n            HydroGeneratorNames.head_nom_col,\n            HydroGeneratorNames.en_eq_col,\n        ]\n\n        arg_user_code = HydroGeneratorNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        generator = HydroGenerator(\n            power_node=row[indices[HydroGeneratorNames.node_col]],\n            energy_equivalent=Conversion(level=arg_user_code[HydroGeneratorNames.en_eq_col]),\n            pq_curve=arg_user_code[HydroGeneratorNames.pq_curve_col],\n            tailwater_elevation=arg_user_code[HydroGeneratorNames.tailw_elev_col],\n            nominal_head=arg_user_code[HydroGeneratorNames.head_nom_col],\n        )\n\n        meta = {}\n        HydroGeneratorNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroGeneratorNames.id_col]]: (generator, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Generator attribute data.\n\n        \"\"\"\n        return GeneratorSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Generator metadata.\n\n        \"\"\"\n        return GeneratorMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Generator schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Generator schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, tuple[HydroGenerator, dict[str, Meta]]]</code>  <code>staticmethod</code>","text":"<p>Create a hydro generator attribute object.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, tuple[HydroGenerator, dict[str, Meta]]]</code> <p>dict[str, dict[str, Meta]]: A dictionary with the generator ID as key and the attribute object and metadata as value.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, tuple[HydroGenerator, dict[str, Meta]]]:\n    \"\"\"\n    Create a hydro generator attribute object.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, dict[str, Meta]]: A dictionary with the generator ID as key and the attribute object and metadata as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroGeneratorNames.pq_curve_col,\n        HydroGeneratorNames.tailw_elev_col,\n        HydroGeneratorNames.head_nom_col,\n        HydroGeneratorNames.en_eq_col,\n    ]\n\n    arg_user_code = HydroGeneratorNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    generator = HydroGenerator(\n        power_node=row[indices[HydroGeneratorNames.node_col]],\n        energy_equivalent=Conversion(level=arg_user_code[HydroGeneratorNames.en_eq_col]),\n        pq_curve=arg_user_code[HydroGeneratorNames.pq_curve_col],\n        tailwater_elevation=arg_user_code[HydroGeneratorNames.tailw_elev_col],\n        nominal_head=arg_user_code[HydroGeneratorNames.head_nom_col],\n    )\n\n    meta = {}\n    HydroGeneratorNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroGeneratorNames.id_col]]: (generator, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Generator attribute data.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Generator attribute data.\n\n    \"\"\"\n    return GeneratorSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Generator metadata.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Generator metadata.\n\n    \"\"\"\n    return GeneratorMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames","title":"<code>HydroInflowNames</code>","text":"<p>Define the InflowNames class and related Pandera schemas for handling hydropower inflow data.</p> <p>Includes attribute and metadata schemas.</p>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames","title":"<code>HydroInflowNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Convert hydropower inflow data to attribute objects for HydroModules. Handle attribute and metadata schema validation.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>class HydroInflowNames(_BaseComponentsNames):\n    \"\"\"Convert hydropower inflow data to attribute objects for HydroModules. Handle attribute and metadata schema validation.\"\"\"\n\n    id_col = \"InflowID\"\n    yr_vol_col = \"YearlyVolume\"\n    profile_col = \"InflowProfileID\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        yr_vol_col,\n        profile_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        yr_vol_col,\n        profile_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, AvgFlowVolume]:\n        \"\"\"\n        Create a hydro inflow component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, Component]: A dictionary with the inflow ID as key and the module unit as value.\n\n        \"\"\"\n        if HydroInflowNames._ref_period_lacks_profiles(row, indices, [HydroInflowNames.profile_col], meta_data):\n            return {row[indices[HydroInflowNames.id_col]]: None}\n        columns_to_parse = [\n            HydroInflowNames.yr_vol_col,\n            HydroInflowNames.profile_col,\n        ]\n\n        arg_user_code = HydroInflowNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        inflow = AvgFlowVolume(\n            level=arg_user_code[HydroInflowNames.yr_vol_col],\n            profile=arg_user_code[HydroInflowNames.profile_col],\n        )\n\n        meta = {}\n        HydroInflowNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroInflowNames.id_col]]: (inflow, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow attribute data.\n\n        \"\"\"\n        return InflowSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Inflow file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow metadata.\n\n        \"\"\"\n        return InflowMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Inflow schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Inflow schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, AvgFlowVolume]</code>  <code>staticmethod</code>","text":"<p>Create a hydro inflow component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, AvgFlowVolume]</code> <p>dict[str, Component]: A dictionary with the inflow ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, AvgFlowVolume]:\n    \"\"\"\n    Create a hydro inflow component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, Component]: A dictionary with the inflow ID as key and the module unit as value.\n\n    \"\"\"\n    if HydroInflowNames._ref_period_lacks_profiles(row, indices, [HydroInflowNames.profile_col], meta_data):\n        return {row[indices[HydroInflowNames.id_col]]: None}\n    columns_to_parse = [\n        HydroInflowNames.yr_vol_col,\n        HydroInflowNames.profile_col,\n    ]\n\n    arg_user_code = HydroInflowNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    inflow = AvgFlowVolume(\n        level=arg_user_code[HydroInflowNames.yr_vol_col],\n        profile=arg_user_code[HydroInflowNames.profile_col],\n    )\n\n    meta = {}\n    HydroInflowNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroInflowNames.id_col]]: (inflow, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow attribute data.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow attribute data.\n\n    \"\"\"\n    return InflowSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Inflow file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow metadata.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Inflow file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow metadata.\n\n    \"\"\"\n    return InflowMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.InflowMetadataSchema","title":"<code>InflowMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Inflow file.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>class InflowMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Inflow file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.InflowSchema","title":"<code>InflowSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>class InflowSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames","title":"<code>HydroModulesNames</code>","text":"<p>Defines schema, names, and component creation logic for hydropower modules.</p> <p>This module provides: - HydroModulesNames: class for column names and component creation for hydropower modules. - HydroModuleSchema: Pandera schema for attribute data. - HydroModuleMetadataSchema: Pandera schema for metadata.</p>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModuleMetadataSchema","title":"<code>HydroModuleMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Modules file.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>class HydroModuleMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Modules file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModuleSchema","title":"<code>HydroModuleSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>class HydroModuleSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames","title":"<code>HydroModulesNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Provides column names, schema accessors, and component creation logic for hydropower modules.</p> <p>This class defines constants for column names, methods for creating HydroModule components from data rows, and accessors for Pandera schemas used for validation of attribute and metadata tables.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>class HydroModulesNames(_BaseComponentsNames):\n    \"\"\"\n    Provides column names, schema accessors, and component creation logic for hydropower modules.\n\n    This class defines constants for column names, methods for creating HydroModule components from data rows,\n    and accessors for Pandera schemas used for validation of attribute and metadata tables.\n    \"\"\"\n\n    filename = \"Hydropower.Modules\"\n\n    id_col = \"ModuleID\"\n    pump_col = \"Pump\"\n    gen_col = \"Generator\"\n    res_col = \"Reservoir\"\n    byp_col = \"Bypass\"\n    hyd_code_col = \"HydraulicCoupling\"\n    inflow_col = \"Inflow\"\n    rel_to_col = \"ReleaseTo\"\n    spill_to_col = \"SpillTo\"\n    rel_cap_col = \"CapacityRelease\"\n    min_bnd_col = \"MinOperationalRelease\"\n    max_bnd_col = \"MaxOperationalRelease\"\n    min_penalty_col = \"MinViolationPenalty\"\n    max_penalty_col = \"MaxViolationPenalty\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        pump_col,\n        gen_col,\n        res_col,\n        byp_col,\n        hyd_code_col,\n        inflow_col,\n        rel_to_col,\n        spill_to_col,\n        rel_cap_col,\n        min_bnd_col,\n        max_bnd_col,\n        min_penalty_col,\n        max_penalty_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        rel_to_col,\n        spill_to_col,\n        rel_cap_col,\n        min_bnd_col,\n        max_bnd_col,\n        min_penalty_col,\n        max_penalty_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Component]:\n        \"\"\"\n        Create a hydro module component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): Dictionary of attributes to link to the HydroModule.\n\n        Returns:\n            dict[str, Component]: A dictionary with the module_id as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroModulesNames.rel_cap_col,\n            HydroModulesNames.min_bnd_col,\n            HydroModulesNames.max_bnd_col,\n            HydroModulesNames.min_penalty_col,\n            HydroModulesNames.max_penalty_col,\n        ]\n        name = row[indices[HydroModulesNames.id_col]]\n        inflow_name = indices[HydroModulesNames.inflow_col]\n        pump_name = indices[HydroModulesNames.pump_col]\n        gen_name = indices[HydroModulesNames.gen_col]\n        res_name = indices[HydroModulesNames.res_col]\n        byp_name = indices[HydroModulesNames.byp_col]\n        arg_user_code = HydroModulesNames._parse_args(row, indices, columns_to_parse, meta_data)\n        inflow, inflow_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[inflow_name],\n            name,\n            HydroModule,\n            AvgFlowVolume,\n        )\n        pump, pump_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[pump_name],\n            name,\n            HydroModule,\n            HydroPump,\n        )\n        generator, generator_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[gen_name],\n            name,\n            HydroModule,\n            HydroGenerator,\n        )\n        reservoir, reservoir_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[res_name],\n            name,\n            HydroModule,\n            HydroReservoir,\n        )\n        bypass, bypass_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[byp_name],\n            name,\n            HydroModule,\n            HydroBypass,\n        )\n        module = HydroModule(\n            release_capacity=MaxFlowVolume(level=arg_user_code[HydroModulesNames.rel_cap_col]),\n            hydraulic_coupling=row[indices[HydroModulesNames.hyd_code_col]],\n            inflow=inflow,\n            pump=pump,\n            generator=generator,\n            reservoir=reservoir,\n            bypass=bypass,\n            release_to=row[indices[HydroModulesNames.rel_to_col]],\n            spill_to=row[indices[HydroModulesNames.spill_to_col]],\n        )\n\n        if \"EnergyEqDownstream\" in meta_columns:\n            HydroModulesNames._add_meta(module, row, indices, [\"EnergyEqDownstream\"], unit=\"kWh/m3\")\n\n        meta_columns = [c for c in meta_columns if c != \"EnergyEqDownstream\"]\n        HydroModulesNames._add_meta(module, row, indices, meta_columns)  # fails because Modules want floats in Meta.\n\n        attr_meta = {\n            inflow_name: inflow_meta,\n            pump_name: pump_meta,\n            gen_name: generator_meta,\n            res_name: reservoir_meta,\n            byp_name: bypass_meta,\n        }\n        HydroModulesNames._merge_attribute_meta(\n            name,\n            module,\n            {k: v for k, v in attr_meta.items() if k and v},\n        )\n\n        return {name: module}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule attribute data.\n\n        \"\"\"\n        return HydroModuleSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Modules file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule metadata.\n\n        \"\"\"\n        return HydroModuleMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the HydroModule schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the HydroModule schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Component]</code>  <code>staticmethod</code>","text":"<p>Create a hydro module component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>Dictionary of attributes to link to the HydroModule.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Component]</code> <p>dict[str, Component]: A dictionary with the module_id as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Component]:\n    \"\"\"\n    Create a hydro module component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): Dictionary of attributes to link to the HydroModule.\n\n    Returns:\n        dict[str, Component]: A dictionary with the module_id as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroModulesNames.rel_cap_col,\n        HydroModulesNames.min_bnd_col,\n        HydroModulesNames.max_bnd_col,\n        HydroModulesNames.min_penalty_col,\n        HydroModulesNames.max_penalty_col,\n    ]\n    name = row[indices[HydroModulesNames.id_col]]\n    inflow_name = indices[HydroModulesNames.inflow_col]\n    pump_name = indices[HydroModulesNames.pump_col]\n    gen_name = indices[HydroModulesNames.gen_col]\n    res_name = indices[HydroModulesNames.res_col]\n    byp_name = indices[HydroModulesNames.byp_col]\n    arg_user_code = HydroModulesNames._parse_args(row, indices, columns_to_parse, meta_data)\n    inflow, inflow_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[inflow_name],\n        name,\n        HydroModule,\n        AvgFlowVolume,\n    )\n    pump, pump_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[pump_name],\n        name,\n        HydroModule,\n        HydroPump,\n    )\n    generator, generator_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[gen_name],\n        name,\n        HydroModule,\n        HydroGenerator,\n    )\n    reservoir, reservoir_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[res_name],\n        name,\n        HydroModule,\n        HydroReservoir,\n    )\n    bypass, bypass_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[byp_name],\n        name,\n        HydroModule,\n        HydroBypass,\n    )\n    module = HydroModule(\n        release_capacity=MaxFlowVolume(level=arg_user_code[HydroModulesNames.rel_cap_col]),\n        hydraulic_coupling=row[indices[HydroModulesNames.hyd_code_col]],\n        inflow=inflow,\n        pump=pump,\n        generator=generator,\n        reservoir=reservoir,\n        bypass=bypass,\n        release_to=row[indices[HydroModulesNames.rel_to_col]],\n        spill_to=row[indices[HydroModulesNames.spill_to_col]],\n    )\n\n    if \"EnergyEqDownstream\" in meta_columns:\n        HydroModulesNames._add_meta(module, row, indices, [\"EnergyEqDownstream\"], unit=\"kWh/m3\")\n\n    meta_columns = [c for c in meta_columns if c != \"EnergyEqDownstream\"]\n    HydroModulesNames._add_meta(module, row, indices, meta_columns)  # fails because Modules want floats in Meta.\n\n    attr_meta = {\n        inflow_name: inflow_meta,\n        pump_name: pump_meta,\n        gen_name: generator_meta,\n        res_name: reservoir_meta,\n        byp_name: bypass_meta,\n    }\n    HydroModulesNames._merge_attribute_meta(\n        name,\n        module,\n        {k: v for k, v in attr_meta.items() if k and v},\n    )\n\n    return {name: module}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule attribute data.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule attribute data.\n\n    \"\"\"\n    return HydroModuleSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Modules file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule metadata.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Modules file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule metadata.\n\n    \"\"\"\n    return HydroModuleMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames","title":"<code>HydroPumpNames</code>","text":"<p>Define the PumpNames class and related Pandera schemas for handling hydropower pump data.</p> <p>Includes attribute and metadata validation for the Hydropower.Pumps file.</p>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames","title":"<code>HydroPumpNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Handle naming conventions, schema definitions, and component creation for hydropower pump data.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>class HydroPumpNames(_BaseComponentsNames):\n    \"\"\"Handle naming conventions, schema definitions, and component creation for hydropower pump data.\"\"\"\n\n    id_col = \"PumpID\"\n    node_col = \"PowerNode\"\n    pump_from_col = \"PumpFrom\"\n    pump_to_col = \"PumpTo\"\n    power_capacity_col = \"PowerCapacity\"\n    vol_capacity_col = \"Capacity\"\n    energy_equiv_col = \"EnergyEq\"\n    h_min_col = \"HeadMin\"\n    h_max_col = \"HeadMax\"\n    q_min_col = \"QMin\"\n    q_max_col = \"QMax\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        node_col,\n        pump_from_col,\n        pump_to_col,\n        power_capacity_col,\n        vol_capacity_col,\n        energy_equiv_col,\n        h_min_col,\n        h_max_col,\n        q_min_col,\n        q_max_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        node_col,\n        pump_from_col,\n        pump_to_col,\n        power_capacity_col,\n        vol_capacity_col,\n        energy_equiv_col,\n        h_min_col,\n        h_max_col,\n        q_min_col,\n        q_max_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, HydroPump]:\n        \"\"\"\n        Create a HydroPump object from a row in the Hydropower.Pumps table.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, HydroPump]: A dictionary with the pump ID as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroPumpNames.power_capacity_col,\n            HydroPumpNames.vol_capacity_col,\n            HydroPumpNames.energy_equiv_col,\n            HydroPumpNames.h_min_col,\n            HydroPumpNames.h_max_col,\n            HydroPumpNames.q_min_col,\n            HydroPumpNames.q_max_col,\n        ]\n\n        arg_user_code = HydroPumpNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        pump = HydroPump(\n            power_node=row[indices[HydroPumpNames.node_col]],\n            from_module=row[indices[HydroPumpNames.pump_from_col]],\n            to_module=row[indices[HydroPumpNames.pump_to_col]],\n            water_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.vol_capacity_col]),\n            energy_equivalent=Conversion(level=arg_user_code[HydroPumpNames.energy_equiv_col]),\n            power_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.power_capacity_col]),\n            head_max=arg_user_code[HydroPumpNames.h_max_col],\n            head_min=arg_user_code[HydroPumpNames.h_min_col],\n            q_max=arg_user_code[HydroPumpNames.q_max_col],\n            q_min=arg_user_code[HydroPumpNames.q_min_col],\n        )\n\n        meta = {}\n        HydroPumpNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroPumpNames.id_col]]: (pump, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Pump attribute data.\n\n        \"\"\"\n        return PumpSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Pumps file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Pump metadata.\n\n        \"\"\"\n        return PumpMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Pump schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Pump schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, HydroPump]</code>  <code>staticmethod</code>","text":"<p>Create a HydroPump object from a row in the Hydropower.Pumps table.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, HydroPump]</code> <p>dict[str, HydroPump]: A dictionary with the pump ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, HydroPump]:\n    \"\"\"\n    Create a HydroPump object from a row in the Hydropower.Pumps table.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, HydroPump]: A dictionary with the pump ID as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroPumpNames.power_capacity_col,\n        HydroPumpNames.vol_capacity_col,\n        HydroPumpNames.energy_equiv_col,\n        HydroPumpNames.h_min_col,\n        HydroPumpNames.h_max_col,\n        HydroPumpNames.q_min_col,\n        HydroPumpNames.q_max_col,\n    ]\n\n    arg_user_code = HydroPumpNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    pump = HydroPump(\n        power_node=row[indices[HydroPumpNames.node_col]],\n        from_module=row[indices[HydroPumpNames.pump_from_col]],\n        to_module=row[indices[HydroPumpNames.pump_to_col]],\n        water_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.vol_capacity_col]),\n        energy_equivalent=Conversion(level=arg_user_code[HydroPumpNames.energy_equiv_col]),\n        power_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.power_capacity_col]),\n        head_max=arg_user_code[HydroPumpNames.h_max_col],\n        head_min=arg_user_code[HydroPumpNames.h_min_col],\n        q_max=arg_user_code[HydroPumpNames.q_max_col],\n        q_min=arg_user_code[HydroPumpNames.q_min_col],\n    )\n\n    meta = {}\n    HydroPumpNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroPumpNames.id_col]]: (pump, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Pump attribute data.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Pump attribute data.\n\n    \"\"\"\n    return PumpSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Pumps file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Pump metadata.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Pumps file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Pump metadata.\n\n    \"\"\"\n    return PumpMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.PumpMetadataSchema","title":"<code>PumpMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Pumps file.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>class PumpMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Pumps file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.PumpSchema","title":"<code>PumpSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>class PumpSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames","title":"<code>HydroReservoirNames</code>","text":"<p>Module for handling reservoir names and schemas in hydropower data.</p> <p>This module defines the ReservoirNames class for managing reservoir attributes, and provides Pandera schemas for validating reservoir attribute and metadata tables.</p>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirMetadataSchema","title":"<code>HydroReservoirMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Reservoirs file.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>class HydroReservoirMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Reservoirs file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames","title":"<code>HydroReservoirNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Class for managing reservoir attribute names and providing methods for schema validation and component creation.</p> <p>This class defines column names for reservoir attributes, methods for creating HydroReservoir components, and functions to retrieve Pandera schemas for validating reservoir attribute and metadata tables.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>class HydroReservoirNames(_BaseComponentsNames):\n    \"\"\"\n    Class for managing reservoir attribute names and providing methods for schema validation and component creation.\n\n    This class defines column names for reservoir attributes, methods for creating HydroReservoir components,\n    and functions to retrieve Pandera schemas for validating reservoir attribute and metadata tables.\n    \"\"\"\n\n    id_col = \"ReservoirID\"\n    capacity_col = \"Capacity\"\n    res_curve_col = \"ReservoirCurve\"\n    min_res_col = \"MinOperationalFilling\"\n    min_penalty_col = \"MinViolationPenalty\"\n    max_res_col = \"MaxOperationalFilling\"\n    max_penalty_col = \"MaxViolationPenalty\"\n    res_buf_col = \"TargetFilling\"\n    buf_penalty_col = \"TargetViolationPenalty\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        capacity_col,\n        res_curve_col,\n        min_res_col,\n        max_res_col,\n        res_buf_col,\n        min_penalty_col,\n        max_penalty_col,\n        buf_penalty_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        capacity_col,\n        res_curve_col,\n        min_res_col,\n        max_res_col,\n        res_buf_col,\n        min_penalty_col,\n        max_penalty_col,\n        buf_penalty_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, HydroReservoir]:\n        \"\"\"\n        Create a HydroReservoir object.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, HydroReservoir]: A dictionary with the inflow ID as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroReservoirNames.capacity_col,\n            HydroReservoirNames.res_curve_col,\n            HydroReservoirNames.min_res_col,\n            HydroReservoirNames.max_res_col,\n            HydroReservoirNames.res_buf_col,\n            HydroReservoirNames.min_penalty_col,\n            HydroReservoirNames.max_penalty_col,\n            HydroReservoirNames.buf_penalty_col,\n        ]\n\n        arg_user_code = HydroReservoirNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        reservoir_curve = ReservoirCurve(arg_user_code[HydroReservoirNames.res_curve_col])\n\n        reservoir = HydroReservoir(\n            capacity=StockVolume(level=arg_user_code[HydroReservoirNames.capacity_col]),\n            reservoir_curve=reservoir_curve,\n        )\n\n        meta = {}\n        HydroReservoirNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroReservoirNames.id_col]]: (reservoir, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir attribute data.\n\n        \"\"\"\n        return HydroReservoirSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Reservoirs file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir metadata.\n\n        \"\"\"\n        return HydroReservoirMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Reservoir schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Reservoir schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, HydroReservoir]</code>  <code>staticmethod</code>","text":"<p>Create a HydroReservoir object.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, HydroReservoir]</code> <p>dict[str, HydroReservoir]: A dictionary with the inflow ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, HydroReservoir]:\n    \"\"\"\n    Create a HydroReservoir object.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, HydroReservoir]: A dictionary with the inflow ID as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroReservoirNames.capacity_col,\n        HydroReservoirNames.res_curve_col,\n        HydroReservoirNames.min_res_col,\n        HydroReservoirNames.max_res_col,\n        HydroReservoirNames.res_buf_col,\n        HydroReservoirNames.min_penalty_col,\n        HydroReservoirNames.max_penalty_col,\n        HydroReservoirNames.buf_penalty_col,\n    ]\n\n    arg_user_code = HydroReservoirNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    reservoir_curve = ReservoirCurve(arg_user_code[HydroReservoirNames.res_curve_col])\n\n    reservoir = HydroReservoir(\n        capacity=StockVolume(level=arg_user_code[HydroReservoirNames.capacity_col]),\n        reservoir_curve=reservoir_curve,\n    )\n\n    meta = {}\n    HydroReservoirNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroReservoirNames.id_col]]: (reservoir, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir attribute data.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir attribute data.\n\n    \"\"\"\n    return HydroReservoirSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Reservoirs file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir metadata.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Reservoirs file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir metadata.\n\n    \"\"\"\n    return HydroReservoirMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirSchema","title":"<code>HydroReservoirSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>class HydroReservoirSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames","title":"<code>ThermalNames</code>","text":"<p>Classes defining Thermal tables.</p>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalMetadataSchema","title":"<code>ThermalMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Thermal.Generators file.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>class ThermalMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Thermal.Generators file.\"\"\"\n\n    @pa.dataframe_check\n    @classmethod\n    def check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n        Args:\n            df (Dataframe): DataFrame used to check value for \"unit\".\n\n        Returns:\n            Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n        \"\"\"\n        return check_unit_is_str_for_attributes(\n            df,\n            [\n                ThermalNames.emission_coeff_col,\n                ThermalNames.capacity_col,\n                ThermalNames.voc_col,\n                ThermalNames.start_costs_col,\n                # ThermalNames.ramp_up_col, # ?\n                # ThermalNames.ramp_down_col, # ?\n            ],\n        )\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalMetadataSchema.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame used to check value for \"unit\".</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n    Args:\n        df (Dataframe): DataFrame used to check value for \"unit\".\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return check_unit_is_str_for_attributes(\n        df,\n        [\n            ThermalNames.emission_coeff_col,\n            ThermalNames.capacity_col,\n            ThermalNames.voc_col,\n            ThermalNames.start_costs_col,\n            # ThermalNames.ramp_up_col, # ?\n            # ThermalNames.ramp_down_col, # ?\n        ],\n    )\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames","title":"<code>ThermalNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Container class for describing the Thermal attribute table's names and structure.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>class ThermalNames(_BaseComponentsNames):\n    \"\"\"Container class for describing the Thermal attribute table's names and structure.\"\"\"\n\n    id_col = \"ThermalID\"\n    main_unit_col = \"MainUnit\"\n    nice_name_col = \"NiceName\"\n    power_node_col = \"PowerNode\"\n    fuel_node_col = \"FuelNode\"\n    emission_node_col = \"EmissionNode\"\n    emission_coeff_col = \"EmissionCoefficient\"\n    type_col = \"Type\"\n    capacity_col = \"Capacity\"\n    full_load_col = \"FullLoadEfficiency\"\n    part_load_col = \"PartLoadEfficiency\"\n    voc_col = \"VOC\"\n    start_costs_col = \"StartCosts\"\n    start_hours_col = \"StartHours\"\n    min_stable_load_col = \"MinStableLoad\"\n    min_op_bound_col = \"MinOperationalBound\"\n    max_op_bound_col = \"MaxOperationalBound\"\n    ramp_up_col = \"RampUp\"\n    ramp_down_col = \"RampDown\"\n\n    # Should include rampup/down data in Thermal, when we get data for this\n    columns: ClassVar[list[str]] = [\n        id_col,\n        nice_name_col,\n        type_col,\n        main_unit_col,\n        power_node_col,\n        fuel_node_col,\n        emission_node_col,\n        capacity_col,\n        full_load_col,\n        part_load_col,\n        voc_col,\n        start_costs_col,\n        start_hours_col,\n        min_stable_load_col,\n        min_op_bound_col,\n        max_op_bound_col,\n        emission_coeff_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        power_node_col,\n        fuel_node_col,\n        emission_node_col,\n        capacity_col,\n        full_load_col,\n        part_load_col,\n        voc_col,\n        start_costs_col,\n        start_hours_col,\n        min_stable_load_col,\n        min_op_bound_col,\n        max_op_bound_col,\n        emission_coeff_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Thermal]:\n        \"\"\"\n        Create a thermal unit component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Thermal object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n        Returns:\n            dict[str, Thermal]: A dictionary with the thermal_id as key and the thermal unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            ThermalNames.emission_node_col,\n            ThermalNames.capacity_col,\n            ThermalNames.full_load_col,\n            ThermalNames.part_load_col,\n            ThermalNames.voc_col,\n            ThermalNames.start_costs_col,\n            ThermalNames.start_hours_col,\n            ThermalNames.min_stable_load_col,\n            ThermalNames.min_op_bound_col,\n            ThermalNames.max_op_bound_col,\n            ThermalNames.emission_coeff_col,\n        ]\n\n        arg_user_code = ThermalNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        no_start_up_costs_condition = (\n            (arg_user_code[ThermalNames.start_costs_col] is None)\n            or (arg_user_code[ThermalNames.min_stable_load_col] is None)\n            or (arg_user_code[ThermalNames.start_hours_col] is None)\n            or (arg_user_code[ThermalNames.part_load_col] is None)\n        )\n        start_up_cost = (\n            None\n            if no_start_up_costs_condition\n            else StartUpCost(\n                startup_cost=Cost(level=arg_user_code[ThermalNames.start_costs_col]),\n                min_stable_load=Proportion(level=arg_user_code[ThermalNames.min_stable_load_col]),\n                start_hours=Hours(level=arg_user_code[ThermalNames.start_hours_col]),\n                part_load_efficiency=Efficiency(level=arg_user_code[ThermalNames.part_load_col]),\n            )\n        )\n\n        voc = (\n            None\n            if arg_user_code[ThermalNames.voc_col] is None\n            else Cost(\n                level=arg_user_code[ThermalNames.voc_col],\n                profile=None,\n            )\n        )\n\n        min_capacity = (\n            None\n            if arg_user_code[ThermalNames.min_op_bound_col] is None\n            else MaxFlowVolume(\n                level=arg_user_code[ThermalNames.capacity_col],\n                profile=arg_user_code[ThermalNames.min_op_bound_col],\n            )\n        )\n\n        thermal = Thermal(\n            power_node=row[indices[ThermalNames.power_node_col]],\n            fuel_node=row[indices[ThermalNames.fuel_node_col]],\n            efficiency=Efficiency(level=arg_user_code[ThermalNames.full_load_col]),\n            emission_node=row[indices[ThermalNames.emission_node_col]],\n            emission_coefficient=Conversion(level=arg_user_code[FuelNodesNames.emission_coefficient_col]),\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[ThermalNames.capacity_col],\n                profile=arg_user_code[ThermalNames.max_op_bound_col],\n            ),\n            min_capacity=min_capacity,\n            voc=voc,\n            startupcost=start_up_cost,\n        )\n        ThermalNames._add_meta(thermal, row, indices, meta_columns)\n\n        return {row[indices[ThermalNames.id_col]]: thermal}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Thermal attribute data.\n\n        \"\"\"\n        return ThermalSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Thermal.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n        \"\"\"\n        return ThermalMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Thermal schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Thermal schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Thermal]</code>  <code>staticmethod</code>","text":"<p>Create a thermal unit component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Thermal object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Thermal]</code> <p>dict[str, Thermal]: A dictionary with the thermal_id as key and the thermal unit as value.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Thermal]:\n    \"\"\"\n    Create a thermal unit component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Thermal object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n    Returns:\n        dict[str, Thermal]: A dictionary with the thermal_id as key and the thermal unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        ThermalNames.emission_node_col,\n        ThermalNames.capacity_col,\n        ThermalNames.full_load_col,\n        ThermalNames.part_load_col,\n        ThermalNames.voc_col,\n        ThermalNames.start_costs_col,\n        ThermalNames.start_hours_col,\n        ThermalNames.min_stable_load_col,\n        ThermalNames.min_op_bound_col,\n        ThermalNames.max_op_bound_col,\n        ThermalNames.emission_coeff_col,\n    ]\n\n    arg_user_code = ThermalNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    no_start_up_costs_condition = (\n        (arg_user_code[ThermalNames.start_costs_col] is None)\n        or (arg_user_code[ThermalNames.min_stable_load_col] is None)\n        or (arg_user_code[ThermalNames.start_hours_col] is None)\n        or (arg_user_code[ThermalNames.part_load_col] is None)\n    )\n    start_up_cost = (\n        None\n        if no_start_up_costs_condition\n        else StartUpCost(\n            startup_cost=Cost(level=arg_user_code[ThermalNames.start_costs_col]),\n            min_stable_load=Proportion(level=arg_user_code[ThermalNames.min_stable_load_col]),\n            start_hours=Hours(level=arg_user_code[ThermalNames.start_hours_col]),\n            part_load_efficiency=Efficiency(level=arg_user_code[ThermalNames.part_load_col]),\n        )\n    )\n\n    voc = (\n        None\n        if arg_user_code[ThermalNames.voc_col] is None\n        else Cost(\n            level=arg_user_code[ThermalNames.voc_col],\n            profile=None,\n        )\n    )\n\n    min_capacity = (\n        None\n        if arg_user_code[ThermalNames.min_op_bound_col] is None\n        else MaxFlowVolume(\n            level=arg_user_code[ThermalNames.capacity_col],\n            profile=arg_user_code[ThermalNames.min_op_bound_col],\n        )\n    )\n\n    thermal = Thermal(\n        power_node=row[indices[ThermalNames.power_node_col]],\n        fuel_node=row[indices[ThermalNames.fuel_node_col]],\n        efficiency=Efficiency(level=arg_user_code[ThermalNames.full_load_col]),\n        emission_node=row[indices[ThermalNames.emission_node_col]],\n        emission_coefficient=Conversion(level=arg_user_code[FuelNodesNames.emission_coefficient_col]),\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[ThermalNames.capacity_col],\n            profile=arg_user_code[ThermalNames.max_op_bound_col],\n        ),\n        min_capacity=min_capacity,\n        voc=voc,\n        startupcost=start_up_cost,\n    )\n    ThermalNames._add_meta(thermal, row, indices, meta_columns)\n\n    return {row[indices[ThermalNames.id_col]]: thermal}\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Thermal attribute data.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Thermal attribute data.\n\n    \"\"\"\n    return ThermalSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Thermal.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Thermal.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n    \"\"\"\n    return ThermalMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalSchema","title":"<code>ThermalSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>class ThermalSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.\"\"\"\n\n    ThermalID: Series[str] = pa.Field(unique=True, nullable=False)\n    PowerNode: Series[str] = pa.Field(nullable=False)\n    FuelNode: Series[str] = pa.Field(nullable=False)\n    EmissionCoefficient: Series[Any] = pa.Field(nullable=True)\n    EmissionNode: Series[str] = pa.Field(nullable=True)\n    Capacity: Series[Any] = pa.Field(nullable=False)\n    FullLoadEfficiency: Series[Any] = pa.Field(nullable=True)\n    PartLoadEfficiency: Series[Any] = pa.Field(nullable=True)\n    VOC: Series[Any] = pa.Field(nullable=True)\n    StartCosts: Series[Any] = pa.Field(nullable=True)\n    StartHours: Series[Any] = pa.Field(nullable=True)\n    MinStableLoad: Series[Any] = pa.Field(nullable=True)\n    MinOperationalBound: Series[Any] = pa.Field(nullable=True)\n    MaxOperationalBound: Series[Any] = pa.Field(nullable=True)\n    RampUp: Series[Any] = pa.Field(nullable=True)\n    RampDown: Series[Any] = pa.Field(nullable=True)\n\n    @pa.check(ThermalNames.capacity_col)\n    @classmethod\n    def dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n        return dtype_str_int_float(series)\n\n    @pa.check(\n        ThermalNames.emission_coeff_col,\n        ThermalNames.full_load_col,\n        ThermalNames.part_load_col,\n        ThermalNames.voc_col,\n        ThermalNames.start_costs_col,\n        ThermalNames.start_hours_col,\n        ThermalNames.min_stable_load_col,\n        ThermalNames.max_op_bound_col,\n        ThermalNames.min_op_bound_col,\n        ThermalNames.ramp_up_col,\n        ThermalNames.ramp_down_col,\n    )\n    @classmethod\n    def dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n        return dtype_str_int_float_none(series)\n\n    @pa.check(\n        ThermalNames.capacity_col,\n        ThermalNames.full_load_col,\n        ThermalNames.part_load_col,\n        ThermalNames.voc_col,\n        ThermalNames.emission_coeff_col,\n    )\n    @classmethod\n    def numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n        return numeric_values_greater_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalSchema.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int or float.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@pa.check(ThermalNames.capacity_col)\n@classmethod\ndef dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n    return dtype_str_int_float(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalSchema.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int, float or None.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@pa.check(\n    ThermalNames.emission_coeff_col,\n    ThermalNames.full_load_col,\n    ThermalNames.part_load_col,\n    ThermalNames.voc_col,\n    ThermalNames.start_costs_col,\n    ThermalNames.start_hours_col,\n    ThermalNames.min_stable_load_col,\n    ThermalNames.max_op_bound_col,\n    ThermalNames.min_op_bound_col,\n    ThermalNames.ramp_up_col,\n    ThermalNames.ramp_down_col,\n)\n@classmethod\ndef dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n    return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalSchema.numeric_values_greater_than_or_equal_to_0","title":"<code>numeric_values_greater_than_or_equal_to_0(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are greater than or equal to zero.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@pa.check(\n    ThermalNames.capacity_col,\n    ThermalNames.full_load_col,\n    ThermalNames.part_load_col,\n    ThermalNames.voc_col,\n    ThermalNames.emission_coeff_col,\n)\n@classmethod\ndef numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n    return numeric_values_greater_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames","title":"<code>TimeVectorMetadataNames</code>","text":"<p>Contains names of fields in time vector metadata.</p>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames.TimeVectorMetadataNames","title":"<code>TimeVectorMetadataNames</code>","text":"<p>Denote available fields in time vector metadata, and provide functionality for time vector metadata processing.</p> <p>The processing is concerned with casting the metadata fields to correct types and decoding the fields and/or values if they are stored as bytes.</p> Source code in <code>framdata/database_names/TimeVectorMetadataNames.py</code> <pre><code>class TimeVectorMetadataNames:\n    \"\"\"\n    Denote available fields in time vector metadata, and provide functionality for time vector metadata processing.\n\n    The processing is concerned with casting the metadata fields to correct types and decoding the fields and/or values if they are stored as bytes.\n\n    \"\"\"\n\n    ENCODING = \"utf-8\"\n\n    DATETIME_COL = \"DateTime\"\n    # OBS! when adding new metadata entries, you also have to parse them in FileHandler.get_parquet_metadata\n    # otherwise they will not be read.\n    # Metadata fields\n\n    # Id column name\n    ID_COLUMN_NAME = \"ID\"\n\n    # Required bools\n    IS_MAX_LEVEL = \"IsMaxLevel\"\n    IS_ZERO_ONE_PROFILE = \"IsZeroOneProfile\"\n    IS_52_WEEK_YEARS = \"Is52WeekYears\"\n    EXTRAPOLATE_FISRT_POINT = \"ExtrapolateFirstPoint\"\n    EXTRAPOLATE_LAST_POINT = \"ExtrapolateLastPoint\"\n\n    # reference period\n    REF_PERIOD_START_YEAR = \"RefPeriodStartYear\"\n    REF_PERIOD_NUM_YEARS = \"RefPeriodNumberOfYears\"\n\n    START = \"StartDateTime\"\n    FREQUENCY = \"Frequency\"\n    NUM_POINTS = \"NumberOfPoints\"\n    TIMEZONE = \"TimeZone\"\n\n    UNIT = \"Unit\"\n\n    METADATA_TYPES = bool | int | str | datetime | timedelta | tzinfo | None\n    METADATA_TYPES_TUPLE = (bool, int, str, datetime, timedelta, tzinfo, type(None))\n\n    # reference_period = \"ReferencePeriod\"\n\n    B_IS_MAX_LEVEL = IS_MAX_LEVEL.encode(ENCODING)\n    B_IS_ZERO_ONE_PROFILE = IS_ZERO_ONE_PROFILE.encode(ENCODING)\n    B_IS_52_WEEK_YEARS = IS_52_WEEK_YEARS.encode(ENCODING)\n    B_ID_COLUMN_NAME = ID_COLUMN_NAME.encode(ENCODING)\n    B_EXTRAPOLATE_FISRT_POINT = EXTRAPOLATE_FISRT_POINT.encode(ENCODING)\n    B_EXTRAPOLATE_LAST_POINT = EXTRAPOLATE_LAST_POINT.encode(ENCODING)\n\n    # reference period\n    B_REF_PERIOD_START_YEAR = REF_PERIOD_START_YEAR.encode(ENCODING)\n    B_REF_PERIOD_NUM_YEARS = REF_PERIOD_NUM_YEARS.encode(ENCODING)\n\n    B_START = START.encode(ENCODING)\n    B_FREQUENCY = FREQUENCY.encode(ENCODING)\n    B_NUM_POINTS = NUM_POINTS.encode(ENCODING)\n    B_TIMEZONE = TIMEZONE.encode(ENCODING)\n    B_UNIT = UNIT.encode(ENCODING)\n\n    str_keys_to_bytes_map: ClassVar[dict[str, bytes]] = {\n        ID_COLUMN_NAME: B_ID_COLUMN_NAME,\n        IS_MAX_LEVEL: B_IS_MAX_LEVEL,\n        IS_ZERO_ONE_PROFILE: B_IS_ZERO_ONE_PROFILE,\n        IS_52_WEEK_YEARS: B_IS_52_WEEK_YEARS,\n        EXTRAPOLATE_FISRT_POINT: B_EXTRAPOLATE_FISRT_POINT,\n        EXTRAPOLATE_LAST_POINT: B_EXTRAPOLATE_LAST_POINT,\n        REF_PERIOD_START_YEAR: B_REF_PERIOD_START_YEAR,\n        REF_PERIOD_NUM_YEARS: B_REF_PERIOD_NUM_YEARS,\n        START: B_START,\n        FREQUENCY: B_FREQUENCY,\n        NUM_POINTS: B_NUM_POINTS,\n        TIMEZONE: B_TIMEZONE,\n        UNIT: B_UNIT,\n    }\n\n    strict_bools_cast: ClassVar[set[str]] = {\n        IS_52_WEEK_YEARS,\n        EXTRAPOLATE_FISRT_POINT,\n        EXTRAPOLATE_LAST_POINT,\n    }\n    keys_cast_methods: ClassVar[dict[str, Callable | type]] = {\n        ID_COLUMN_NAME: str,\n        IS_MAX_LEVEL: bool,\n        IS_ZERO_ONE_PROFILE: bool,\n        REF_PERIOD_START_YEAR: int,\n        REF_PERIOD_NUM_YEARS: int,\n        START: pd.to_datetime,\n        FREQUENCY: pd.to_timedelta,\n        NUM_POINTS: int,\n        TIMEZONE: pytz.timezone,\n        UNIT: str,\n    }\n\n    @staticmethod\n    def cast_meta(\n        raw_meta: dict[str | bytes, str | bytes | int | bool | None],\n    ) -&gt; tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]:\n        \"\"\"\n        Decode possible binary keys and values and cast values of metadata dict to their defined types.\n\n        Args:\n            raw_meta (dict[str  |  bytes, str  |  bytes  |  int  |  bool  |  None]): Dictionary to decode and cast.\n\n        Returns:\n            tuple[dict[str, Any], set[str]]: Decoded and cast dictionary, set of missing keys.\n\n        \"\"\"\n        tvmn = TimeVectorMetadataNames\n        str_bytes_map = tvmn.str_keys_to_bytes_map\n        cast_meta = {key: raw_meta[key] for key in set(str_bytes_map.keys()) | set(str_bytes_map.values()) if key in raw_meta}\n        str_to_bytes_meta = tvmn.bytes_keys_to_str(cast_meta)\n        cast_meta = str_to_bytes_meta if str_to_bytes_meta else cast_meta  # Keys were bytes and we decode to str.\n\n        missing_keys: set[str] = {key for key in str_bytes_map if key not in cast_meta}\n\n        # Update with cast values for strict bools and others.\n        cast_meta.update({key: tvmn.cast_strict_bool_value(cast_meta[key]) for key in tvmn.strict_bools_cast if key in cast_meta})\n        cast_meta.update({key: tvmn.cast_value(cast_meta[key], cast_method) for key, cast_method in tvmn.keys_cast_methods.items() if key in cast_meta})\n\n        return cast_meta, missing_keys\n\n    @staticmethod\n    def str_keys_to_bytes(raw_meta: dict[str, bytes]) -&gt; dict[bytes, bytes]:\n        return {bytes_name: raw_meta[str_name] for str_name, bytes_name in TimeVectorMetadataNames.str_keys_to_bytes_map.items() if str_name in raw_meta}\n\n    @staticmethod\n    def bytes_keys_to_str(raw_meta: dict[bytes, bytes]) -&gt; dict[str, bytes]:\n        return {str_name: raw_meta[bytes_name] for str_name, bytes_name in TimeVectorMetadataNames.str_keys_to_bytes_map.items() if bytes_name in raw_meta}\n\n    @staticmethod\n    def cast_value(value: str | bytes | None, cast_function: Callable | type) -&gt; object | None:\n        \"\"\"\n        Cast a string value into new type, but always return None if value is None or \"None\".\n\n        Args:\n            value (str | None): A string value or None.\n            cast_function (Union[Callable, type]): Function or type with which to cast the value into.\n\n        Raises:\n            RuntimeError: If anything goes wrong in the cast_function.\n\n        Returns:\n            object|None: Value as new type or None.\n\n        \"\"\"\n        if isinstance(value, bytes):\n            if cast_function is bool:\n                return None if value == b\"None\" else value == b\"True\"\n            value = value.decode(encoding=TimeVectorMetadataNames.ENCODING)\n\n        if value is None or value in {\"None\", \"\"}:  # Handle missing values\n            return None\n        try:\n            return cast_function(value)\n        except Exception as e:\n            msg = f\"Could not cast metadata value: {value}. Casting method: {cast_function}\"\n            raise RuntimeError(msg) from e\n\n    @staticmethod\n    def cast_strict_bool_value(value: str | bool | bytes) -&gt; bool:\n        if isinstance(value, bytes):\n            return value == b\"True\"\n        return bool(value)\n</code></pre>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames.TimeVectorMetadataNames.cast_meta","title":"<code>cast_meta(raw_meta: dict[str | bytes, str | bytes | int | bool | None]) -&gt; tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]</code>  <code>staticmethod</code>","text":"<p>Decode possible binary keys and values and cast values of metadata dict to their defined types.</p> <p>Parameters:</p> Name Type Description Default <code>raw_meta</code> <code>dict[str | bytes, str | bytes | int | bool | None]</code> <p>Dictionary to decode and cast.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]</code> <p>tuple[dict[str, Any], set[str]]: Decoded and cast dictionary, set of missing keys.</p> Source code in <code>framdata/database_names/TimeVectorMetadataNames.py</code> <pre><code>@staticmethod\ndef cast_meta(\n    raw_meta: dict[str | bytes, str | bytes | int | bool | None],\n) -&gt; tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]:\n    \"\"\"\n    Decode possible binary keys and values and cast values of metadata dict to their defined types.\n\n    Args:\n        raw_meta (dict[str  |  bytes, str  |  bytes  |  int  |  bool  |  None]): Dictionary to decode and cast.\n\n    Returns:\n        tuple[dict[str, Any], set[str]]: Decoded and cast dictionary, set of missing keys.\n\n    \"\"\"\n    tvmn = TimeVectorMetadataNames\n    str_bytes_map = tvmn.str_keys_to_bytes_map\n    cast_meta = {key: raw_meta[key] for key in set(str_bytes_map.keys()) | set(str_bytes_map.values()) if key in raw_meta}\n    str_to_bytes_meta = tvmn.bytes_keys_to_str(cast_meta)\n    cast_meta = str_to_bytes_meta if str_to_bytes_meta else cast_meta  # Keys were bytes and we decode to str.\n\n    missing_keys: set[str] = {key for key in str_bytes_map if key not in cast_meta}\n\n    # Update with cast values for strict bools and others.\n    cast_meta.update({key: tvmn.cast_strict_bool_value(cast_meta[key]) for key in tvmn.strict_bools_cast if key in cast_meta})\n    cast_meta.update({key: tvmn.cast_value(cast_meta[key], cast_method) for key, cast_method in tvmn.keys_cast_methods.items() if key in cast_meta})\n\n    return cast_meta, missing_keys\n</code></pre>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames.TimeVectorMetadataNames.cast_value","title":"<code>cast_value(value: str | bytes | None, cast_function: Callable | type) -&gt; object | None</code>  <code>staticmethod</code>","text":"<p>Cast a string value into new type, but always return None if value is None or \"None\".</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>A string value or None.</p> required <code>cast_function</code> <code>Union[Callable, type]</code> <p>Function or type with which to cast the value into.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If anything goes wrong in the cast_function.</p> <p>Returns:</p> Type Description <code>object | None</code> <p>object|None: Value as new type or None.</p> Source code in <code>framdata/database_names/TimeVectorMetadataNames.py</code> <pre><code>@staticmethod\ndef cast_value(value: str | bytes | None, cast_function: Callable | type) -&gt; object | None:\n    \"\"\"\n    Cast a string value into new type, but always return None if value is None or \"None\".\n\n    Args:\n        value (str | None): A string value or None.\n        cast_function (Union[Callable, type]): Function or type with which to cast the value into.\n\n    Raises:\n        RuntimeError: If anything goes wrong in the cast_function.\n\n    Returns:\n        object|None: Value as new type or None.\n\n    \"\"\"\n    if isinstance(value, bytes):\n        if cast_function is bool:\n            return None if value == b\"None\" else value == b\"True\"\n        value = value.decode(encoding=TimeVectorMetadataNames.ENCODING)\n\n    if value is None or value in {\"None\", \"\"}:  # Handle missing values\n        return None\n    try:\n        return cast_function(value)\n    except Exception as e:\n        msg = f\"Could not cast metadata value: {value}. Casting method: {cast_function}\"\n        raise RuntimeError(msg) from e\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames","title":"<code>TransmissionNames</code>","text":"<p>Defines the TransmissionNames class and related Pandera schemas.</p> <p>These describe validate Transmission attributes and metadata tables in the energy model database.</p>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionMetadataSchema","title":"<code>TransmissionMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Transmission.Grid file.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class TransmissionMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Transmission.Grid file.\"\"\"\n\n    @pa.dataframe_check\n    @classmethod\n    def check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Check that the 'unit' value is a string for the rows where 'attribute' is 'Capacity' and 'Loss'.\n\n        Args:\n            df (Dataframe): DataFrame used to check value for \"unit\".\n\n        Returns:\n            Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n        \"\"\"\n        return check_unit_is_str_for_attributes(df, [TransmissionNames.capacity_col, TransmissionNames.tariff_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionMetadataSchema.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that the 'unit' value is a string for the rows where 'attribute' is 'Capacity' and 'Loss'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame used to check value for \"unit\".</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Check that the 'unit' value is a string for the rows where 'attribute' is 'Capacity' and 'Loss'.\n\n    Args:\n        df (Dataframe): DataFrame used to check value for \"unit\".\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return check_unit_is_str_for_attributes(df, [TransmissionNames.capacity_col, TransmissionNames.tariff_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames","title":"<code>TransmissionNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Container class for describing the Transmission attribute table's names and structure.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class TransmissionNames(_BaseComponentsNames):\n    \"\"\"Container class for describing the Transmission attribute table's names and structure.\"\"\"\n\n    id_col = \"TransmissionID\"\n    from_node_col = \"FromNode\"\n    to_node_col = \"ToNode\"\n    capacity_col = \"Capacity\"\n    loss_col = \"Loss\"\n    tariff_col = \"Tariff\"\n    max_op_bound_col = \"MaxOperationalBound\"\n    min_op_bound_col = \"MinOperationalBound\"\n    ramp_up_col = \"RampUp\"\n    ramp_down_col = \"RampDown\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        from_node_col,\n        to_node_col,\n        capacity_col,\n        loss_col,\n        tariff_col,\n        max_op_bound_col,\n        min_op_bound_col,\n        ramp_up_col,\n        ramp_down_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        from_node_col,\n        to_node_col,\n        capacity_col,\n        loss_col,\n        tariff_col,\n        max_op_bound_col,\n        min_op_bound_col,\n        ramp_up_col,\n        ramp_down_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Transmission]:\n        \"\"\"\n        Create a transmission unit component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Transmission object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n        Returns:\n            dict[str, Transmission]: A dictionary with the transmission_id as key and the transmission unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            TransmissionNames.capacity_col,\n            TransmissionNames.loss_col,\n            TransmissionNames.tariff_col,\n            TransmissionNames.max_op_bound_col,\n            TransmissionNames.min_op_bound_col,\n            TransmissionNames.ramp_up_col,\n            TransmissionNames.ramp_down_col,\n        ]\n\n        arg_user_code = TransmissionNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        ramp_up = None if arg_user_code[TransmissionNames.ramp_up_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_up_col])\n        ramp_down = None if arg_user_code[TransmissionNames.ramp_down_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_down_col])\n        loss = None if arg_user_code[TransmissionNames.loss_col] is None else Loss(level=arg_user_code[TransmissionNames.loss_col])\n\n        tariff = None if arg_user_code[TransmissionNames.tariff_col] is None else Cost(level=arg_user_code[TransmissionNames.tariff_col])\n\n        min_capacity = (\n            None\n            if arg_user_code[TransmissionNames.min_op_bound_col] is None\n            else MaxFlowVolume(\n                level=arg_user_code[TransmissionNames.capacity_col],\n                profile=arg_user_code[TransmissionNames.min_op_bound_col],\n            )\n        )\n\n        transmission = Transmission(\n            from_node=row[indices[TransmissionNames.from_node_col]],\n            to_node=row[indices[TransmissionNames.to_node_col]],\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[TransmissionNames.capacity_col],\n                profile=arg_user_code[TransmissionNames.max_op_bound_col],\n            ),\n            min_capacity=min_capacity,\n            loss=loss,\n            tariff=tariff,\n            ramp_up=ramp_up,\n            ramp_down=ramp_down,\n        )\n        TransmissionNames._add_meta(transmission, row, indices, meta_columns)\n\n        return {row[indices[TransmissionNames.id_col]]: transmission}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Transmission attribute data.\n\n        \"\"\"\n        return TransmissionSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Transmission.Grid file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Transmission metadata.\n\n        \"\"\"\n        return TransmissionMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Transmission schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return {\n            \"check_internal_line_error\": (\"Transmission line is internal (FromNode equals ToNode).\", False),\n        }\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Transmission schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        check_name = \"check_internal_line_error\"\n        if check_name in errors[TransmissionNames.COL_CHECK].to_numpy():\n            check_rows = errors.loc[\n                (errors[TransmissionNames.COL_CHECK] == check_name)\n                &amp; (\n                    errors[TransmissionNames.COL_COLUMN].isin(\n                        [TransmissionNames.from_node_col, TransmissionNames.to_node_col],\n                    )\n                )\n            ]\n            check_rows.loc[:, TransmissionNames.COL_COLUMN] = f\"{TransmissionNames.from_node_col}, {TransmissionNames.to_node_col}\"\n            check_rows = check_rows.drop_duplicates()\n            errors = errors[~(errors[TransmissionNames.COL_CHECK] == check_name)]\n            errors = pd.concat([errors, check_rows], ignore_index=True)\n\n        return errors\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Transmission]</code>  <code>staticmethod</code>","text":"<p>Create a transmission unit component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Transmission object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]] | None</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Transmission]</code> <p>dict[str, Transmission]: A dictionary with the transmission_id as key and the transmission unit as value.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Transmission]:\n    \"\"\"\n    Create a transmission unit component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Transmission object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n    Returns:\n        dict[str, Transmission]: A dictionary with the transmission_id as key and the transmission unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        TransmissionNames.capacity_col,\n        TransmissionNames.loss_col,\n        TransmissionNames.tariff_col,\n        TransmissionNames.max_op_bound_col,\n        TransmissionNames.min_op_bound_col,\n        TransmissionNames.ramp_up_col,\n        TransmissionNames.ramp_down_col,\n    ]\n\n    arg_user_code = TransmissionNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    ramp_up = None if arg_user_code[TransmissionNames.ramp_up_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_up_col])\n    ramp_down = None if arg_user_code[TransmissionNames.ramp_down_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_down_col])\n    loss = None if arg_user_code[TransmissionNames.loss_col] is None else Loss(level=arg_user_code[TransmissionNames.loss_col])\n\n    tariff = None if arg_user_code[TransmissionNames.tariff_col] is None else Cost(level=arg_user_code[TransmissionNames.tariff_col])\n\n    min_capacity = (\n        None\n        if arg_user_code[TransmissionNames.min_op_bound_col] is None\n        else MaxFlowVolume(\n            level=arg_user_code[TransmissionNames.capacity_col],\n            profile=arg_user_code[TransmissionNames.min_op_bound_col],\n        )\n    )\n\n    transmission = Transmission(\n        from_node=row[indices[TransmissionNames.from_node_col]],\n        to_node=row[indices[TransmissionNames.to_node_col]],\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[TransmissionNames.capacity_col],\n            profile=arg_user_code[TransmissionNames.max_op_bound_col],\n        ),\n        min_capacity=min_capacity,\n        loss=loss,\n        tariff=tariff,\n        ramp_up=ramp_up,\n        ramp_down=ramp_down,\n    )\n    TransmissionNames._add_meta(transmission, row, indices, meta_columns)\n\n    return {row[indices[TransmissionNames.id_col]]: transmission}\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Transmission attribute data.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Transmission attribute data.\n\n    \"\"\"\n    return TransmissionSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Transmission.Grid file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Transmission metadata.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Transmission.Grid file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Transmission metadata.\n\n    \"\"\"\n    return TransmissionMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema","title":"<code>TransmissionSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class TransmissionSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.\"\"\"\n\n    TransmissionID: Series[str] = pa.Field(unique=True, nullable=False)\n    FromNode: Series[str] = pa.Field(nullable=False)\n    ToNode: Series[str] = pa.Field(nullable=False)\n    Capacity: Series[Any] = pa.Field(nullable=False)\n    Loss: Series[Any] = pa.Field(nullable=True)\n    Tariff: Series[Any] = pa.Field(nullable=True)\n    MaxOperationalBound: Series[Any] = pa.Field(nullable=True)\n    MinOperationalBound: Series[Any] = pa.Field(nullable=True)\n    RampUp: Series[Any] = pa.Field(nullable=True)\n    RampDown: Series[Any] = pa.Field(nullable=True)\n\n    @pa.check(TransmissionNames.capacity_col)\n    @classmethod\n    def dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n        return dtype_str_int_float(series)\n\n    @pa.check(\n        TransmissionNames.loss_col,\n        TransmissionNames.tariff_col,\n        TransmissionNames.max_op_bound_col,\n        TransmissionNames.min_op_bound_col,\n        TransmissionNames.ramp_up_col,\n        TransmissionNames.ramp_down_col,\n    )\n    @classmethod\n    def dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n        return dtype_str_int_float_none(series)\n\n    @pa.check(TransmissionNames.capacity_col)\n    @classmethod\n    def numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n        return numeric_values_greater_than_or_equal_to(series, 0)\n\n    @pa.check(TransmissionNames.loss_col)\n    @classmethod\n    def numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n        return numeric_values_are_between_or_equal_to(series, 0, 1)\n\n    @pa.dataframe_check\n    @classmethod\n    def check_internal_line_error(cls, dataframe: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Raise warning if origin node is the same as destination node, in which case we have an internal line.\n\n        Args:\n            dataframe (pd.DataFrame): DataFrame to check.\n\n        Returns:\n            Series[bool]: Series of boolean values denoting if each element has passed the check.\n\n        \"\"\"\n        return dataframe[TransmissionNames.from_node_col] != dataframe[TransmissionNames.to_node_col]\n\n    class Config:\n        \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n        unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.Config","title":"<code>Config</code>","text":"<p>Schema-wide configuration for the DemandSchema class.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class Config:\n    \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n    unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.check_internal_line_error","title":"<code>check_internal_line_error(dataframe: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Raise warning if origin node is the same as destination node, in which case we have an internal line.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>DataFrame to check.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values denoting if each element has passed the check.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_internal_line_error(cls, dataframe: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Raise warning if origin node is the same as destination node, in which case we have an internal line.\n\n    Args:\n        dataframe (pd.DataFrame): DataFrame to check.\n\n    Returns:\n        Series[bool]: Series of boolean values denoting if each element has passed the check.\n\n    \"\"\"\n    return dataframe[TransmissionNames.from_node_col] != dataframe[TransmissionNames.to_node_col]\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int or float.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(TransmissionNames.capacity_col)\n@classmethod\ndef dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n    return dtype_str_int_float(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int, float or None.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(\n    TransmissionNames.loss_col,\n    TransmissionNames.tariff_col,\n    TransmissionNames.max_op_bound_col,\n    TransmissionNames.min_op_bound_col,\n    TransmissionNames.ramp_up_col,\n    TransmissionNames.ramp_down_col,\n)\n@classmethod\ndef dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n    return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.numeric_values_are_between_or_equal_to_0_and_1","title":"<code>numeric_values_are_between_or_equal_to_0_and_1(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are between zero and one or equal to zero and one.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(TransmissionNames.loss_col)\n@classmethod\ndef numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n    return numeric_values_are_between_or_equal_to(series, 0, 1)\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.numeric_values_greater_than_or_equal_to_0","title":"<code>numeric_values_greater_than_or_equal_to_0(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are greater than or equal to zero.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(TransmissionNames.capacity_col)\n@classmethod\ndef numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n    return numeric_values_greater_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames","title":"<code>WindSolarNames</code>","text":"<p>Classes defining Wind and Solar tables and how to create Components from them.</p>"},{"location":"reference/#framdata.database_names.WindSolarNames.SolarNames","title":"<code>SolarNames</code>","text":"<p>               Bases: <code>WindSolarNames</code></p> <p>Class representing the names and structure of Solar tables, and method for creating Solar Component objects.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class SolarNames(WindSolarNames):\n    \"\"\"Class representing the names and structure of Solar tables, and method for creating Solar Component objects.\"\"\"\n\n    id_col = \"SolarID\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        WindSolarNames.power_node_col,\n        WindSolarNames.profile_col,\n        WindSolarNames.capacity_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Solar]:\n        \"\"\"\n        Create a Solar Component from a row in the Solar.Generators table.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one solar object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n        Returns:\n            dict[str, Solar]: A dictionary with the id as key and the solar unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            SolarNames.profile_col,\n            SolarNames.capacity_col,\n        ]\n\n        arg_user_code = SolarNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        solar = Solar(\n            power_node=row[indices[SolarNames.power_node_col]],\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[SolarNames.capacity_col],\n                profile=arg_user_code[SolarNames.profile_col],\n            ),\n            voc=None,\n        )\n\n        SolarNames._add_meta(solar, row, indices, meta_columns)\n\n        return {row[indices[SolarNames.id_col]]: solar}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.SolarNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Solar]</code>  <code>staticmethod</code>","text":"<p>Create a Solar Component from a row in the Solar.Generators table.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one solar object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]] | None</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Solar]</code> <p>dict[str, Solar]: A dictionary with the id as key and the solar unit as value.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Solar]:\n    \"\"\"\n    Create a Solar Component from a row in the Solar.Generators table.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one solar object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n    Returns:\n        dict[str, Solar]: A dictionary with the id as key and the solar unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        SolarNames.profile_col,\n        SolarNames.capacity_col,\n    ]\n\n    arg_user_code = SolarNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    solar = Solar(\n        power_node=row[indices[SolarNames.power_node_col]],\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[SolarNames.capacity_col],\n            profile=arg_user_code[SolarNames.profile_col],\n        ),\n        voc=None,\n    )\n\n    SolarNames._add_meta(solar, row, indices, meta_columns)\n\n    return {row[indices[SolarNames.id_col]]: solar}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindNames","title":"<code>WindNames</code>","text":"<p>               Bases: <code>WindSolarNames</code></p> <p>Class representing the names and structure of Wind tables, and method for creating Wind Component objects.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindNames(WindSolarNames):\n    \"\"\"Class representing the names and structure of Wind tables, and method for creating Wind Component objects.\"\"\"\n\n    id_col = \"WindID\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        WindSolarNames.power_node_col,\n        WindSolarNames.profile_col,\n        WindSolarNames.capacity_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Wind]:\n        \"\"\"\n        Create a Wind Component from a row in the Wind.Generators table.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Wind object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n        Returns:\n            dict[str, Wind]: A dictionary with the wind_id as key and the wind unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            WindNames.profile_col,\n            WindNames.capacity_col,\n        ]\n\n        arg_user_code = WindNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        wind = Wind(\n            power_node=row[indices[WindNames.power_node_col]],\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[WindNames.capacity_col],\n                profile=arg_user_code[WindNames.profile_col],\n            ),\n            voc=None,\n        )\n        WindNames._add_meta(wind, row, indices, meta_columns)\n\n        return {row[indices[WindNames.id_col]]: wind}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Wind]</code>  <code>staticmethod</code>","text":"<p>Create a Wind Component from a row in the Wind.Generators table.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Wind object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]] | None</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Wind]</code> <p>dict[str, Wind]: A dictionary with the wind_id as key and the wind unit as value.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Wind]:\n    \"\"\"\n    Create a Wind Component from a row in the Wind.Generators table.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Wind object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n    Returns:\n        dict[str, Wind]: A dictionary with the wind_id as key and the wind unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        WindNames.profile_col,\n        WindNames.capacity_col,\n    ]\n\n    arg_user_code = WindNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    wind = Wind(\n        power_node=row[indices[WindNames.power_node_col]],\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[WindNames.capacity_col],\n            profile=arg_user_code[WindNames.profile_col],\n        ),\n        voc=None,\n    )\n    WindNames._add_meta(wind, row, indices, meta_columns)\n\n    return {row[indices[WindNames.id_col]]: wind}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarMetadataSchema","title":"<code>WindSolarMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Standard Pandera DataFrameModel schema for metadata in the Wind and Solar files.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindSolarMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Standard Pandera DataFrameModel schema for metadata in the Wind and Solar files.\"\"\"\n\n    @pa.dataframe_check\n    @classmethod\n    def check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n        Args:\n            df (Dataframe): DataFrame used to check value for \"unit\".\n\n        Returns:\n            Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n        \"\"\"\n        return check_unit_is_str_for_attributes(df, [WindSolarNames.capacity_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarMetadataSchema.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame used to check value for \"unit\".</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n    Args:\n        df (Dataframe): DataFrame used to check value for \"unit\".\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return check_unit_is_str_for_attributes(df, [WindSolarNames.capacity_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarNames","title":"<code>WindSolarNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Class representing the names and structure of Wind and Solar tables.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindSolarNames(_BaseComponentsNames):\n    \"\"\"Class representing the names and structure of Wind and Solar tables.\"\"\"\n\n    power_node_col = \"PowerNode\"\n    profile_col = \"Profile\"\n    type_col = \"TechnologyType\"\n    capacity_col = \"Capacity\"\n\n    ref_columns: ClassVar[list[str]] = [\n        power_node_col,\n        profile_col,\n        capacity_col,\n    ]\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in a Wind and Solar file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Wind and Solar attribute data.\n\n        \"\"\"\n        return WindSolarSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in a Wind and Solar file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n        \"\"\"\n        return WindSolarMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Wind and Solar schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Wind and Solar schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in a Wind and Solar file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Wind and Solar attribute data.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in a Wind and Solar file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Wind and Solar attribute data.\n\n    \"\"\"\n    return WindSolarSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in a Wind and Solar file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in a Wind and Solar file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n    \"\"\"\n    return WindSolarMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarSchema","title":"<code>WindSolarSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Standard Pandera DataFrameModel schema for attribute data in the Wind and Solar files.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindSolarSchema(pa.DataFrameModel):\n    \"\"\"Standard Pandera DataFrameModel schema for attribute data in the Wind and Solar files.\"\"\"\n\n    ID: Series[str] = pa.Field(unique=True, nullable=False)\n    Capacity: Series[Any] = pa.Field(nullable=False)\n    PowerNode: Series[str] = pa.Field(nullable=False)\n    Profile: Series[Any] = pa.Field(nullable=True)\n\n    @pa.check(WindSolarNames.capacity_col)\n    @classmethod\n    def dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n        return dtype_str_int_float(series)\n\n    @pa.check(WindSolarNames.profile_col)\n    @classmethod\n    def dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n        return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarSchema.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int or float.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@pa.check(WindSolarNames.capacity_col)\n@classmethod\ndef dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n    return dtype_str_int_float(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarSchema.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int, float or None.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@pa.check(WindSolarNames.profile_col)\n@classmethod\ndef dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n    return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.YamlNames","title":"<code>YamlNames</code>","text":"<p>Define names and fields used in yaml files.</p>"},{"location":"reference/#framdata.database_names.YamlNames.YamlNames","title":"<code>YamlNames</code>","text":"<p>Contain names in yaml files.</p> Source code in <code>framdata/database_names/YamlNames.py</code> <pre><code>class YamlNames:\n    \"\"\"Contain names in yaml files.\"\"\"\n\n    encoding = \"utf-8\"\n\n    metadata_field = \"Metadata\"\n    x_field = \"X\"\n    y_field = \"Y\"\n\n    # ========= Metadata fields =========\n    attribute = \"Attribute\"\n    description = \"Description\"\n    dtype = \"Dtype\"\n    unit = \"Unit\"\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names","title":"<code>nodes_names</code>","text":"<p>Define class for handling tables with Nodes.</p>"},{"location":"reference/#framdata.database_names.nodes_names.EmissionNodesNames","title":"<code>EmissionNodesNames</code>","text":"<p>               Bases: <code>NodesNames</code></p> <p>Class representing the names and structure of emission nodes tables.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class EmissionNodesNames(NodesNames):\n    \"\"\"Class representing the names and structure of emission nodes tables.\"\"\"\n\n    filename = \"Emission.Nodes\"\n\n    tax_col = \"Tax\"  # deprecated?\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.FuelNodesNames","title":"<code>FuelNodesNames</code>","text":"<p>               Bases: <code>NodesNames</code></p> <p>Class representing the names and structure of fuel nodes tables.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class FuelNodesNames(NodesNames):\n    \"\"\"Class representing the names and structure of fuel nodes tables.\"\"\"\n\n    filename = \"Fuel.Nodes\"\n\n    emission_coefficient_col = \"EmissionCoefficient\"\n    tax_col = \"Tax\"  # deprecated?\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesMetadataSchema","title":"<code>NodesMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Standard Pandera DataFrameModel schema for metadata in the Nodes files.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class NodesMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Standard Pandera DataFrameModel schema for metadata in the Nodes files.\"\"\"\n\n    @pa.dataframe_check\n    @classmethod\n    def check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n        Args:\n            df (Dataframe): DataFrame used to check value for \"unit\".\n\n        Returns:\n            Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n        \"\"\"\n        return check_unit_is_str_for_attributes(df, [NodesNames.price_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesMetadataSchema.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame used to check value for \"unit\".</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n    Args:\n        df (Dataframe): DataFrame used to check value for \"unit\".\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return check_unit_is_str_for_attributes(df, [NodesNames.price_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames","title":"<code>NodesNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Class representing the names and structure of nodes tables, and the convertion of the table to Node objects.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class NodesNames(_BaseComponentsNames):\n    \"\"\"Class representing the names and structure of nodes tables, and the convertion of the table to Node objects.\"\"\"\n\n    id_col = \"NodeID\"\n\n    commodity_col = \"Commodity\"\n    nice_name = \"NiceName\"\n    price_col = \"ExogenousPrice\"\n    profile_col = \"PriceProfile\"\n    exogenous_col = \"IsExogenous\"\n\n    columns: ClassVar[list[str]] = [id_col, nice_name, commodity_col, price_col, profile_col, exogenous_col]\n\n    ref_columns: ClassVar[list[str]] = [price_col, profile_col]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; tuple[dict[str, Node], list[str]]:\n        \"\"\"\n        Create a node object from direct parameters.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Node object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n        Returns:\n            dict[str, Node]: Dictionary of node id and the Node object.\n\n        \"\"\"\n        columns_to_parse = [\n            NodesNames.price_col,\n            NodesNames.profile_col,\n        ]\n\n        arg_user_code = NodesNames._parse_args(row, indices, columns_to_parse, meta_data)\n        price = None\n        if arg_user_code[NodesNames.price_col] is not None:\n            price = Price(\n                level=arg_user_code[NodesNames.price_col],\n                profile=arg_user_code[NodesNames.profile_col],\n            )\n\n        node = Node(\n            row[indices[NodesNames.commodity_col]],\n            is_exogenous=row[indices[NodesNames.exogenous_col]],\n            price=price,\n        )\n        NodesNames._add_meta(node, row, indices, meta_columns)\n        return {str(row[indices[NodesNames.id_col]]): node}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in a Nodes file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Nodes attribute data.\n\n        \"\"\"\n        return NodesSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in a Nodes file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n        \"\"\"\n        return NodesMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Nodes schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return {\n            NodesSchema.check_exogenous_price.__name__: (\"Missing price value for exogenous Node.\", True),\n        }\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Nodes schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        if NodesSchema.check_exogenous_price.__name__ in errors[NodesNames.COL_CHECK].to_numpy():\n            check_rows = errors.loc[errors[NodesNames.COL_CHECK] == NodesSchema.check_exogenous_price.__name__]\n            errors = errors[~(errors[NodesNames.COL_CHECK] == NodesSchema.check_exogenous_price.__name__)]\n            price_exogenous_cols = [NodesNames.id_col, NodesNames.price_col, NodesNames.exogenous_col]\n            check_description_str = check_rows[NodesNames.COL_CHECK_DESC].unique()[0]\n            price_exogenous_rows = []\n\n            for idx in check_rows[NodesNames.COL_IDX].unique():\n                check_case = check_rows[check_rows[NodesNames.COL_IDX] == idx]\n\n                for col in price_exogenous_cols:\n                    if col not in list(check_case[NodesNames.COL_COLUMN].unique()):\n                        price_exogenous_rows.append(\n                            [\n                                col,\n                                NodesSchema.check_exogenous_price.__name__,\n                                None,\n                                idx,\n                                check_description_str,\n                                True,\n                            ],\n                        )\n            errors = pd.concat([errors, pd.DataFrame(price_exogenous_rows, columns=errors.columns)], ignore_index=True)\n        return errors\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; tuple[dict[str, Node], list[str]]</code>  <code>staticmethod</code>","text":"<p>Create a node object from direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Node object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>list[str]</code> <p>Set of columns which defines memberships in meta groups for aggregation.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Node], list[str]]</code> <p>dict[str, Node]: Dictionary of node id and the Node object.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; tuple[dict[str, Node], list[str]]:\n    \"\"\"\n    Create a node object from direct parameters.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Node object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n    Returns:\n        dict[str, Node]: Dictionary of node id and the Node object.\n\n    \"\"\"\n    columns_to_parse = [\n        NodesNames.price_col,\n        NodesNames.profile_col,\n    ]\n\n    arg_user_code = NodesNames._parse_args(row, indices, columns_to_parse, meta_data)\n    price = None\n    if arg_user_code[NodesNames.price_col] is not None:\n        price = Price(\n            level=arg_user_code[NodesNames.price_col],\n            profile=arg_user_code[NodesNames.profile_col],\n        )\n\n    node = Node(\n        row[indices[NodesNames.commodity_col]],\n        is_exogenous=row[indices[NodesNames.exogenous_col]],\n        price=price,\n    )\n    NodesNames._add_meta(node, row, indices, meta_columns)\n    return {str(row[indices[NodesNames.id_col]]): node}\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in a Nodes file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Nodes attribute data.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in a Nodes file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Nodes attribute data.\n\n    \"\"\"\n    return NodesSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in a Nodes file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in a Nodes file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n    \"\"\"\n    return NodesMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesSchema","title":"<code>NodesSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Standard Pandera DataFrameModel schema for attribute data in the Nodes files.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class NodesSchema(pa.DataFrameModel):\n    \"\"\"Standard Pandera DataFrameModel schema for attribute data in the Nodes files.\"\"\"\n\n    NodeID: Series[str] = pa.Field(unique=True, nullable=False)\n    Commodity: Series[str] = pa.Field(nullable=False)\n    ExogenousPrice: Series[Any] = pa.Field(nullable=True)\n    PriceProfile: Series[Any] = pa.Field(nullable=True)\n    IsExogenous: Series[bool] = pa.Field(nullable=False)\n\n    @pa.check(NodesNames.price_col, NodesNames.profile_col)\n    @classmethod\n    def dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n        return dtype_str_int_float_none(series)\n\n    @pa.dataframe_check\n    @classmethod\n    def check_exogenous_price(cls, df: DataFrame) -&gt; Series[bool]:\n        \"\"\"Check that all elastic demand values are present if one or more is.\"\"\"\n        price = df[NodesNames.price_col]\n        exogenous = df[NodesNames.exogenous_col]\n        return ~pd.Series(price.isna() &amp; exogenous)  # return should be false when price is None and Node is exogenous.\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesSchema.check_exogenous_price","title":"<code>check_exogenous_price(df: DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that all elastic demand values are present if one or more is.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_exogenous_price(cls, df: DataFrame) -&gt; Series[bool]:\n    \"\"\"Check that all elastic demand values are present if one or more is.\"\"\"\n    price = df[NodesNames.price_col]\n    exogenous = df[NodesNames.exogenous_col]\n    return ~pd.Series(price.isna() &amp; exogenous)  # return should be false when price is None and Node is exogenous.\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesSchema.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int, float or None.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@pa.check(NodesNames.price_col, NodesNames.profile_col)\n@classmethod\ndef dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n    return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.PowerNodesNames","title":"<code>PowerNodesNames</code>","text":"<p>               Bases: <code>NodesNames</code></p> <p>Class representing the names and structure of power nodes tables.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class PowerNodesNames(NodesNames):\n    \"\"\"Class representing the names and structure of power nodes tables.\"\"\"\n\n    filename = \"Power.Nodes\"\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions","title":"<code>validation_functions</code>","text":"<p>Module containing registered custom check functions used by Pandera schema classes.</p>"},{"location":"reference/#framdata.database_names.validation_functions.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame, attribute_names: list[str]) -&gt; Series[bool]</code>","text":"<p>Check if 'Unit' column values are strings for the rows where the 'Attribute' column matches specific attributes.</p> <p>This function checks whether the values in the 'Unit' column are strings for rows where the 'Attribute' column matches any of the specified attribute names. Rows that do not match the specified attributes are considered valid by default. This function is commonly used by subclasses of 'AttributeMetadataSchema' to validate that a unit is given for certain attributes in the metadata belonging to a Component.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the columns to validate.</p> required <code>attribute_names</code> <code>list[str]</code> <p>A list with the names of the attributes to check in the 'Attribute' column.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: A boolean Series indicating whether each row passes the validation. Rows where the 'Attribute'</p> <code>Series[bool]</code> <p>column does not match the specified attribute are automatically marked as valid.</p> Example <p>Given the following DataFrame:</p> attribute unit Volume MWh Temperature None Capacity None <p>And <code>attribute_names = [\"Volume\", \"Capacity\"]</code>, the method will validate that the 'Unit' column contains strings for rows where 'attribute' is \"Volume\" and \"Capacity\". The resulting Series will be:</p> validation_result True True False Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef check_unit_is_str_for_attributes(df: pd.DataFrame, attribute_names: list[str]) -&gt; Series[bool]:\n    \"\"\"\n    Check if 'Unit' column values are strings for the rows where the 'Attribute' column matches specific attributes.\n\n    This function checks whether the values in the 'Unit' column are strings for rows where the 'Attribute' column\n    matches any of the specified attribute names. Rows that do not match the specified attributes are considered valid\n    by default. This function is commonly used by subclasses of 'AttributeMetadataSchema' to validate that a unit is\n    given for certain attributes in the metadata belonging to a Component.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the columns to validate.\n        attribute_names (list[str]): A list with the names of the attributes to check in the 'Attribute' column.\n\n    Returns:\n        Series[bool]: A boolean Series indicating whether each row passes the validation. Rows where the 'Attribute'\n        column does not match the specified attribute are automatically marked as valid.\n\n    Example:\n        Given the following DataFrame:\n\n        | attribute   | unit       |\n        |-------------|------------|\n        | Volume      | MWh        |\n        | Temperature | None       |\n        | Capacity    | None       |\n\n        And `attribute_names = [\"Volume\", \"Capacity\"]`, the method will validate that the 'Unit' column contains strings\n        for rows where 'attribute' is \"Volume\" and \"Capacity\". The resulting Series will be:\n\n        | validation_result |\n        |-------------------|\n        | True              |\n        | True              |\n        | False             |\n\n    \"\"\"\n    is_attribute_rows = df[_AttributeMetadataNames.attribute].isin(attribute_names)\n    unit_is_str = df[_AttributeMetadataNames.unit].apply(lambda x: isinstance(x, str))\n    return ~is_attribute_rows | unit_is_str\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>","text":"<p>Check if the series contains only str, int or float values.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"\n    Check if the series contains only str, int or float values.\n\n    Args:\n        series (Series[Any]): Series to check.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return series.apply(lambda value: isinstance(value, str | int | float))\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>","text":"<p>Check if the series contains only str, int, float or None values.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"\n    Check if the series contains only str, int, float or None values.\n\n    Args:\n        series (Series[Any]): Series to check.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return series.apply(lambda value: isinstance(value, str | int | float | type(None)))\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.numeric_values_are_between_or_equal_to","title":"<code>numeric_values_are_between_or_equal_to(series: Series[Any], min_value: int | float, max_value: int | float) -&gt; Series[bool]</code>","text":"<p>Check if values are between or equal to a min and max value if they are of type int or float.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <code>min_value</code> <code>int | float</code> <p>Value that the elements in the series should be greater than or equal.</p> required <code>max_value</code> <code>int | float</code> <p>Value that the elements in the series should be less than or equal.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef numeric_values_are_between_or_equal_to(\n    series: Series[Any],\n    min_value: int | float,\n    max_value: int | float,\n) -&gt; Series[bool]:\n    \"\"\"\n    Check if values are between or equal to a min and max value if they are of type int or float.\n\n    Args:\n        series (Series[Any]): Series to check.\n        min_value (int | float): Value that the elements in the series should be greater than or equal.\n        max_value (int | float): Value that the elements in the series should be less than or equal.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    if not isinstance(min_value, (int | float)) and not isinstance(max_value, (int | float)):\n        message = \"min and max value must be of type int or float.\"\n        raise ValueError(message)\n    return series.apply(lambda x: min_value &lt;= x &lt;= max_value if isinstance(x, (int | float)) else True)\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.numeric_values_greater_than","title":"<code>numeric_values_greater_than(series: Series[Any], min_value: int | float) -&gt; Series[bool]</code>","text":"<p>Check if values are greater than or equal to min_value if they are of type int or float.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <code>min_value</code> <code>int | float</code> <p>Value that the elements in the series should be greater than or equal.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef numeric_values_greater_than(series: Series[Any], min_value: int | float) -&gt; Series[bool]:\n    \"\"\"\n    Check if values are greater than or equal to min_value if they are of type int or float.\n\n    Args:\n        series (Series[Any]): Series to check.\n        min_value (int | float): Value that the elements in the series should be greater than or equal.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    if not isinstance(min_value, (int | float)):\n        message = \"min_value must be of type int or float.\"\n        raise ValueError(message)\n    return series.apply(lambda x: x &gt; min_value if isinstance(x, (int | float)) else True)\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.numeric_values_greater_than_or_equal_to","title":"<code>numeric_values_greater_than_or_equal_to(series: Series[Any], min_value: int | float) -&gt; Series[bool]</code>","text":"<p>Check if values are greater than or equal to min_value if they are of type int or float.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <code>min_value</code> <code>int | float</code> <p>Value that the elements in the series should be greater than or equal.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef numeric_values_greater_than_or_equal_to(series: Series[Any], min_value: int | float) -&gt; Series[bool]:\n    \"\"\"\n    Check if values are greater than or equal to min_value if they are of type int or float.\n\n    Args:\n        series (Series[Any]): Series to check.\n        min_value (int | float): Value that the elements in the series should be greater than or equal.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    if not isinstance(min_value, (int | float)):\n        message = \"min_value must be of type int or float.\"\n        raise ValueError(message)\n    return series.apply(lambda x: x &gt;= min_value if isinstance(x, (int | float)) else True)\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.numeric_values_less_than_or_equal_to","title":"<code>numeric_values_less_than_or_equal_to(series: Series[Any], max_value: int | float) -&gt; Series[bool]</code>","text":"<p>Check if values are less than or equal to max_value if they are of type int or float.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <code>max_value</code> <code>int | float</code> <p>Value that the elements in the series should be greater than or equal.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef numeric_values_less_than_or_equal_to(series: Series[Any], max_value: int | float) -&gt; Series[bool]:\n    \"\"\"\n    Check if values are less than or equal to max_value if they are of type int or float.\n\n    Args:\n        series (Series[Any]): Series to check.\n        max_value (int | float): Value that the elements in the series should be greater than or equal.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    if not isinstance(max_value, (int | float)):\n        message = \"max_value must be of type int or float.\"\n        raise ValueError(message)\n    return series.apply(lambda x: x &lt;= max_value if isinstance(x, (int | float)) else True)\n</code></pre>"},{"location":"reference/#framdata.file_editors","title":"<code>file_editors</code>","text":""},{"location":"reference/#framdata.file_editors.NVEFileEditor","title":"<code>NVEFileEditor</code>","text":"<p>Contain class with common functionality for editing files.</p>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor","title":"<code>NVEFileEditor</code>","text":"<p>               Bases: <code>Base</code></p> <p>Parent class with common functionality for classes concerned with editing FRAM files.</p> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>class NVEFileEditor(Base):\n    \"\"\"Parent class with common functionality for classes concerned with editing FRAM files.\"\"\"\n\n    def __init__(self, source: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n        Args:\n            source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n        \"\"\"\n        super().__init__()\n\n        self._check_type(source, (Path, str, type(None)))\n        self._source = None if source is None else Path(source)\n\n    def get_source(self) -&gt; Path:\n        \"\"\"Get the source file path of the editor.\"\"\"\n        return self._source\n\n    def set_source(self, source: Path) -&gt; None:\n        \"\"\"Set the source file path of the editor.\"\"\"\n        self._check_type(source, (Path, str))\n        self._source = Path(source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor.__init__","title":"<code>__init__(source: Path | str | None = None) -&gt; None</code>","text":"<p>Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | None</code> <p>Path to parquet file with timevectors. Defaults to None.</p> <code>None</code> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>def __init__(self, source: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n    Args:\n        source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n    \"\"\"\n    super().__init__()\n\n    self._check_type(source, (Path, str, type(None)))\n    self._source = None if source is None else Path(source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor.get_source","title":"<code>get_source() -&gt; Path</code>","text":"<p>Get the source file path of the editor.</p> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>def get_source(self) -&gt; Path:\n    \"\"\"Get the source file path of the editor.\"\"\"\n    return self._source\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor.set_source","title":"<code>set_source(source: Path) -&gt; None</code>","text":"<p>Set the source file path of the editor.</p> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>def set_source(self, source: Path) -&gt; None:\n    \"\"\"Set the source file path of the editor.\"\"\"\n    self._check_type(source, (Path, str))\n    self._source = Path(source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor","title":"<code>NVEH5TimeVectorEditor</code>","text":"<p>Contains class for editing time vectors in H5 files.</p>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor","title":"<code>NVEH5TimeVectorEditor</code>","text":"<p>               Bases: <code>NVEFileEditor</code></p> <p>Class with functionality concerned with editing time vectors and their metadata in H5 files.</p> Structure of the NVE h5 files <ul> <li>common_index dataset: Contains a numpy array with index applied to all vectors missing a specific index.</li> <li>index group of datasets: Contains indexes coupled to specific vectors by the vector IDs.</li> <li>common_metadata group: Contains dictionary with metadata applied to all vectors missing a specific metadata dictionary.</li> <li>metadata group of groups: Contains metadata dictionaries coupled to specific vectors by the vector IDs.</li> <li>vectors group of datasets: Contains numpy arrays with the vector values.</li> </ul> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>class NVEH5TimeVectorEditor(NVEFileEditor):\n    \"\"\"\n    Class with functionality concerned with editing time vectors and their metadata in H5 files.\n\n    Structure of the NVE h5 files:\n     - common_index dataset: Contains a numpy array with index applied to all vectors missing a specific index.\n     - index group of datasets: Contains indexes coupled to specific vectors by the vector IDs.\n     - common_metadata group: Contains dictionary with metadata applied to all vectors missing a specific metadata dictionary.\n     - metadata group of groups: Contains metadata dictionaries coupled to specific vectors by the vector IDs.\n     - vectors group of datasets: Contains numpy arrays with the vector values.\n\n    \"\"\"\n\n    def __init__(self, source: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n        Args:\n            source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n        \"\"\"\n        super().__init__(source)\n\n        meta_tuple = ({}, None) if self._source is None or not self._source.exists() else self._read_data(H5Names.METADATA_GROUP, True)\n        self._metadata, self._common_metadata = meta_tuple\n        index_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.INDEX_GROUP, False)\n        self._index, self._common_index = index_tuple\n        self._index = {k: v.astype(str) for k, v in self._index.items()}\n\n        vectors_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.VECTORS_GROUP, False)\n        self._vectors, __ = vectors_tuple\n\n    def get_metadata(self, vector_id: str) -&gt; None | dict:\n        \"\"\"Get a copy of the metadata of all vectors in the h5 file.\"\"\"\n        try:\n            return self._metadata[vector_id].copy()\n        except KeyError as e:\n            f\"Found no ID '{vector_id}' in metadata.\"\n            raise KeyError from e\n\n    def set_metadata(self, vector_id: str, metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None:\n        \"\"\"Set the metadata dictionary of a specific vector (overwrites existing).\"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(metadata, dict)\n        for key, value in metadata.items():\n            self._check_type(key, str)\n            self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n        self._metadata[vector_id] = metadata\n\n    def set_metadata_by_key(self, vector_id: str, key: str, value: TvMn.METADATA_TYPES) -&gt; None:\n        \"\"\"Set a field (new or overwrite) in the metadata of a vector.\"\"\"\n        self._check_type(key, str)\n        self._check_type(vector_id, str)\n        self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n        if vector_id not in self._metadata or not isinstance(self._metadata[vector_id], dict):\n            self._metadata[vector_id] = {}\n        self._metadata[vector_id][key] = value\n\n    def get_common_metadata(self) -&gt; None | dict:\n        \"\"\"Get a copy of the common metadata of vectors in the h5 file.\"\"\"\n        return self._common_metadata if self._common_metadata is None else self._common_metadata.copy()\n\n    def set_common_metadata(self, metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None:\n        \"\"\"Set the common metadata dictionary (overwrites existing).\"\"\"\n        self._check_type(metadata, dict)\n        self._common_metadata = metadata\n\n    def set_common_metadata_by_key(self, key: str, value: TvMn.METADATA_TYPES) -&gt; None:\n        \"\"\"Set a field (new or overwrite) in the common metadata.\"\"\"\n        self._check_type(key, str)\n        self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n        if self._common_metadata is None:\n            self._common_metadata = {}\n        self._common_metadata[key] = value\n\n    def set_index(self, vector_id: str, index: NDArray) -&gt; None:\n        \"\"\"\n        Set the index of a vector.\n\n        Index is paired with a vector of the same vector_id.\n\n        \"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(index, np.ndarray)\n        self._index[vector_id] = index\n\n    def get_index(self, vector_id: str) -&gt; NDArray:\n        \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n        try:\n            return self._index[vector_id]\n        except KeyError as e:\n            f\"Found no ID '{vector_id}' among indexes.\"\n            raise KeyError from e\n\n    def set_common_index(self, values: NDArray) -&gt; None:\n        \"\"\"Set the common index which will be used for vectors which have not specified their own index by its ID.\"\"\"\n        self._check_type(values, np.ndarray)\n        self._common_index = values\n\n    def get_common_index(self) -&gt; NDArray | None:\n        \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n        return self._common_index\n\n    def set_vector(self, vector_id: str, values: NDArray) -&gt; None:\n        \"\"\"Set vector values.\"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(values, np.ndarray)\n        self._vectors[vector_id] = values\n\n    def get_vector(self, vector_id: str) -&gt; NDArray:\n        \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n        try:\n            return self._vectors[vector_id]\n        except KeyError as e:\n            msg = f\"Found no ID '{vector_id}' among vectors.\"\n            raise KeyError(msg) from e\n\n    def get_vector_ids(self) -&gt; list[str]:\n        \"\"\"Get the IDs of all vectors available in the file.\"\"\"\n        return list(self._vectors.keys())\n\n    def save_to_h5(self, path: Path | str) -&gt; None:\n        \"\"\"\n        Store the data to h5 file.\n\n        Args:\n            path (Path | str): Path to save the file. Overwrites existing files.\n\n        Raises:\n            KeyError: If common index is None and there are vectors missing specific index.\n            KeyError: If common metadata is None and there are vectors missing specific metadata.\n\n        \"\"\"\n        self._check_type(path, (Path, str))\n        path = Path(path)\n\n        self._check_missing_indexes()\n        self._check_missing_metadata()\n\n        with h5py.File(path, mode=\"w\") as f:\n            if self._common_metadata is not None:\n                common_meta_group = f.create_group(H5Names.COMMON_PREFIX + H5Names.METADATA_GROUP)\n                self._write_meta_to_group(common_meta_group, self._common_metadata)\n            if self._common_index is not None:\n                f.create_dataset(H5Names.COMMON_PREFIX + H5Names.INDEX_GROUP, data=self._common_index.astype(bytes))\n\n            if self._metadata:\n                meta_group = f.create_group(H5Names.METADATA_GROUP)\n                for vector_id, meta in self._metadata.items():\n                    vm_group = meta_group.create_group(vector_id)\n                    self._write_meta_to_group(vm_group, meta)\n\n            if self._index:\n                index_group = f.create_group(H5Names.INDEX_GROUP)\n                for vector_id, index in self._index.items():\n                    index_group.create_dataset(vector_id, data=index.astype(bytes))\n\n            if self._vectors:\n                vector_group = f.create_group(H5Names.VECTORS_GROUP)\n                for vector_id, vector in self._vectors.items():\n                    vector_group.create_dataset(vector_id, data=vector)\n\n    def _check_missing_indexes(self) -&gt; None:\n        missing_index = {v for v in self._vectors if v not in self._index}\n        if self._common_index is None and len(missing_index) != 0:\n            msg = f\"Found vectors missing indexes and common index is not set: {missing_index}.\"\n            raise KeyError(msg)\n\n    def _check_missing_metadata(self) -&gt; None:\n        missing_meta = {v for v in self._vectors if v not in self._metadata}\n        if self._common_metadata is None and len(missing_meta) != 0:\n            msg = f\"Found vectors missing metadata and common metadata is not set: {missing_meta}.\"\n            raise KeyError(msg)\n\n    def _write_meta_to_group(self, meta_group: h5py.Group, metadata: dict) -&gt; None:\n        for k, v in metadata.items():\n            meta_group.create_dataset(k, data=str(v).encode(TvMn.ENCODING))\n\n    def _read_data(\n        self,\n        group_name: str,\n        cast_meta: bool,\n    ) -&gt; tuple[dict[str, dict[str, TvMn.METADATA_TYPES]] | dict[str, dict[str, NDArray]], dict[str, TvMn.METADATA_TYPES] | dict[str, NDArray]]:\n        common_field = H5Names.COMMON_PREFIX + group_name\n        data = {}\n        common_data = None\n        with h5py.File(self._source, mode=\"r\") as f:\n            if group_name in f and isinstance(f[group_name], h5py.Group):\n                group = f[group_name]\n                data.update(\n                    {\n                        vector_id: TvMn.cast_meta(self._read_datasets(vector_data)) if cast_meta else self._read_datasets(vector_data)\n                        for vector_id, vector_data in group.items()\n                    },\n                )\n\n            if common_field in f and isinstance(f[common_field], h5py.Group):\n                datasets = self._read_datasets(f[common_field])\n                common_data, __ = TvMn.cast_meta(datasets) if cast_meta else (datasets, None)\n            elif common_field in f and isinstance(f[common_field], h5py.Dataset):\n                common_data = f[common_field][()]\n\n        return data, common_data\n\n    def _read_datasets(self, field: h5py.Group | h5py.Dataset) -&gt; dict | NDArray | bytes:\n        if isinstance(field, h5py.Dataset):\n            return field[()]\n        datasets = {}\n        for key, val in field.items():\n            if isinstance(val, h5py.Dataset):\n                datasets[key] = val[()]\n            else:\n                msg = f\"Expected only {h5py.Dataset} in field, but found {type(val)}\"\n                raise TypeError(msg)\n\n        return datasets\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.__init__","title":"<code>__init__(source: Path | str | None = None) -&gt; None</code>","text":"<p>Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | None</code> <p>Path to parquet file with timevectors. Defaults to None.</p> <code>None</code> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def __init__(self, source: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n    Args:\n        source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n    \"\"\"\n    super().__init__(source)\n\n    meta_tuple = ({}, None) if self._source is None or not self._source.exists() else self._read_data(H5Names.METADATA_GROUP, True)\n    self._metadata, self._common_metadata = meta_tuple\n    index_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.INDEX_GROUP, False)\n    self._index, self._common_index = index_tuple\n    self._index = {k: v.astype(str) for k, v in self._index.items()}\n\n    vectors_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.VECTORS_GROUP, False)\n    self._vectors, __ = vectors_tuple\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_common_index","title":"<code>get_common_index() -&gt; NDArray | None</code>","text":"<p>Return a copy of a given index as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_common_index(self) -&gt; NDArray | None:\n    \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n    return self._common_index\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_common_metadata","title":"<code>get_common_metadata() -&gt; None | dict</code>","text":"<p>Get a copy of the common metadata of vectors in the h5 file.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_common_metadata(self) -&gt; None | dict:\n    \"\"\"Get a copy of the common metadata of vectors in the h5 file.\"\"\"\n    return self._common_metadata if self._common_metadata is None else self._common_metadata.copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_index","title":"<code>get_index(vector_id: str) -&gt; NDArray</code>","text":"<p>Return a copy of a given index as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; NDArray:\n    \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n    try:\n        return self._index[vector_id]\n    except KeyError as e:\n        f\"Found no ID '{vector_id}' among indexes.\"\n        raise KeyError from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; None | dict</code>","text":"<p>Get a copy of the metadata of all vectors in the h5 file.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; None | dict:\n    \"\"\"Get a copy of the metadata of all vectors in the h5 file.\"\"\"\n    try:\n        return self._metadata[vector_id].copy()\n    except KeyError as e:\n        f\"Found no ID '{vector_id}' in metadata.\"\n        raise KeyError from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_vector","title":"<code>get_vector(vector_id: str) -&gt; NDArray</code>","text":"<p>Return a copy of a given vector as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_vector(self, vector_id: str) -&gt; NDArray:\n    \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n    try:\n        return self._vectors[vector_id]\n    except KeyError as e:\n        msg = f\"Found no ID '{vector_id}' among vectors.\"\n        raise KeyError(msg) from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_vector_ids","title":"<code>get_vector_ids() -&gt; list[str]</code>","text":"<p>Get the IDs of all vectors available in the file.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_vector_ids(self) -&gt; list[str]:\n    \"\"\"Get the IDs of all vectors available in the file.\"\"\"\n    return list(self._vectors.keys())\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.save_to_h5","title":"<code>save_to_h5(path: Path | str) -&gt; None</code>","text":"<p>Store the data to h5 file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to save the file. Overwrites existing files.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If common index is None and there are vectors missing specific index.</p> <code>KeyError</code> <p>If common metadata is None and there are vectors missing specific metadata.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def save_to_h5(self, path: Path | str) -&gt; None:\n    \"\"\"\n    Store the data to h5 file.\n\n    Args:\n        path (Path | str): Path to save the file. Overwrites existing files.\n\n    Raises:\n        KeyError: If common index is None and there are vectors missing specific index.\n        KeyError: If common metadata is None and there are vectors missing specific metadata.\n\n    \"\"\"\n    self._check_type(path, (Path, str))\n    path = Path(path)\n\n    self._check_missing_indexes()\n    self._check_missing_metadata()\n\n    with h5py.File(path, mode=\"w\") as f:\n        if self._common_metadata is not None:\n            common_meta_group = f.create_group(H5Names.COMMON_PREFIX + H5Names.METADATA_GROUP)\n            self._write_meta_to_group(common_meta_group, self._common_metadata)\n        if self._common_index is not None:\n            f.create_dataset(H5Names.COMMON_PREFIX + H5Names.INDEX_GROUP, data=self._common_index.astype(bytes))\n\n        if self._metadata:\n            meta_group = f.create_group(H5Names.METADATA_GROUP)\n            for vector_id, meta in self._metadata.items():\n                vm_group = meta_group.create_group(vector_id)\n                self._write_meta_to_group(vm_group, meta)\n\n        if self._index:\n            index_group = f.create_group(H5Names.INDEX_GROUP)\n            for vector_id, index in self._index.items():\n                index_group.create_dataset(vector_id, data=index.astype(bytes))\n\n        if self._vectors:\n            vector_group = f.create_group(H5Names.VECTORS_GROUP)\n            for vector_id, vector in self._vectors.items():\n                vector_group.create_dataset(vector_id, data=vector)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_common_index","title":"<code>set_common_index(values: NDArray) -&gt; None</code>","text":"<p>Set the common index which will be used for vectors which have not specified their own index by its ID.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_common_index(self, values: NDArray) -&gt; None:\n    \"\"\"Set the common index which will be used for vectors which have not specified their own index by its ID.\"\"\"\n    self._check_type(values, np.ndarray)\n    self._common_index = values\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_common_metadata","title":"<code>set_common_metadata(metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None</code>","text":"<p>Set the common metadata dictionary (overwrites existing).</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_common_metadata(self, metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None:\n    \"\"\"Set the common metadata dictionary (overwrites existing).\"\"\"\n    self._check_type(metadata, dict)\n    self._common_metadata = metadata\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_common_metadata_by_key","title":"<code>set_common_metadata_by_key(key: str, value: TvMn.METADATA_TYPES) -&gt; None</code>","text":"<p>Set a field (new or overwrite) in the common metadata.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_common_metadata_by_key(self, key: str, value: TvMn.METADATA_TYPES) -&gt; None:\n    \"\"\"Set a field (new or overwrite) in the common metadata.\"\"\"\n    self._check_type(key, str)\n    self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n    if self._common_metadata is None:\n        self._common_metadata = {}\n    self._common_metadata[key] = value\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_index","title":"<code>set_index(vector_id: str, index: NDArray) -&gt; None</code>","text":"<p>Set the index of a vector.</p> <p>Index is paired with a vector of the same vector_id.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_index(self, vector_id: str, index: NDArray) -&gt; None:\n    \"\"\"\n    Set the index of a vector.\n\n    Index is paired with a vector of the same vector_id.\n\n    \"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(index, np.ndarray)\n    self._index[vector_id] = index\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_metadata","title":"<code>set_metadata(vector_id: str, metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None</code>","text":"<p>Set the metadata dictionary of a specific vector (overwrites existing).</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_metadata(self, vector_id: str, metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None:\n    \"\"\"Set the metadata dictionary of a specific vector (overwrites existing).\"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(metadata, dict)\n    for key, value in metadata.items():\n        self._check_type(key, str)\n        self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n    self._metadata[vector_id] = metadata\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_metadata_by_key","title":"<code>set_metadata_by_key(vector_id: str, key: str, value: TvMn.METADATA_TYPES) -&gt; None</code>","text":"<p>Set a field (new or overwrite) in the metadata of a vector.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_metadata_by_key(self, vector_id: str, key: str, value: TvMn.METADATA_TYPES) -&gt; None:\n    \"\"\"Set a field (new or overwrite) in the metadata of a vector.\"\"\"\n    self._check_type(key, str)\n    self._check_type(vector_id, str)\n    self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n    if vector_id not in self._metadata or not isinstance(self._metadata[vector_id], dict):\n        self._metadata[vector_id] = {}\n    self._metadata[vector_id][key] = value\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_vector","title":"<code>set_vector(vector_id: str, values: NDArray) -&gt; None</code>","text":"<p>Set vector values.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_vector(self, vector_id: str, values: NDArray) -&gt; None:\n    \"\"\"Set vector values.\"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(values, np.ndarray)\n    self._vectors[vector_id] = values\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor","title":"<code>NVEParquetTimeVectorEditor</code>","text":"<p>Contains class for editing time vectors in parquet files.</p>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor","title":"<code>NVEParquetTimeVectorEditor</code>","text":"<p>               Bases: <code>NVEFileEditor</code></p> <p>Class for managing time vectors and their metadata stored in parquet files.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>class NVEParquetTimeVectorEditor(NVEFileEditor):\n    \"\"\"Class for managing time vectors and their metadata stored in parquet files.\"\"\"\n\n    def __init__(self, source: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n        Args:\n            source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n        \"\"\"\n        super().__init__(source)\n        self._metadata = {} if self._source is None or not self._source.exists() else self._read_metadata()\n        self._data = pd.DataFrame() if self._source is None or not self._source.exists() else pd.read_parquet(self._source)\n\n    def save_to_parquet(self, path: Path | str) -&gt; None:\n        \"\"\"\n        Save the edited dataframe and metadata to parquet file.\n\n        Args:\n            path (Path): Path to save tha file to. Must be defined to force user to explicitly overwrite the original file if they want.\n\n        \"\"\"\n        self._check_type(path, (Path, str))\n        path = Path(path)\n        table = pa.Table.from_pandas(self._data)\n\n        # ensure binary strings with defined encoding, since parquet encodes metadata anyway\n        schema_with_meta = table.schema.with_metadata({str(k).encode(TvMn.ENCODING): str(v).encode(TvMn.ENCODING) for k, v in self._metadata.items()})\n        table = pa.Table.from_pandas(self._data, schema=schema_with_meta)\n\n        pq.write_table(table, path)\n\n    def get_metadata(self) -&gt; dict:\n        \"\"\"Get a copy of the metadata of the vectors in the parquet file.\"\"\"\n        return self._metadata.copy()\n\n    def set_metadata(self, metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None:\n        \"\"\"Set the metadata dictionary (overwrites existing).\"\"\"\n        self._check_type(metadata, dict)\n        for key, value in metadata.items():\n            self._check_type(key, str)\n            self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n        self._metadata = metadata\n\n    def set_metadata_by_key(self, key: str, value: TvMn.METADATA_TYPES) -&gt; None:\n        \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n        self._check_type(key, str)\n        self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n        self._metadata[key] = value\n\n    def set_vector(self, vector_id: str, values: NDArray | pd.Series) -&gt; None:\n        \"\"\"Set a whole vector in the time vector table.\"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(values, (np.ndarray, pd.Series))\n        if not self._data.empty and len(values) != len(self._data):\n            message = f\"Series values has different size than the other vectors in the table.\\nLength values: {len(values)}\\nLength vectors: {len(self._data)}\"\n            raise IndexError(message)\n        self._data[vector_id] = values\n\n    def get_vector(self, vector_id: str) -&gt; pd.Series:\n        \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n        try:\n            return self._data[vector_id].copy()\n        except KeyError as e:\n            f\"Found no vector named '{vector_id}' in table at {self._source}.\"\n            raise KeyError from e\n\n    def get_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Return a copy of all of the vector table as a pandas dataframe.\"\"\"\n        return self._data.copy()\n\n    def set_dataframe(self, dataframe: pd.DataFrame) -&gt; None:\n        \"\"\"Set the dataframe of the editor.\"\"\"\n        self._check_type(dataframe, pd.DataFrame)\n        self._data = dataframe\n\n    def get_vector_ids(self) -&gt; list[str]:\n        \"\"\"Get the IDs of all vectors.\"\"\"\n        return [c for c in self._data.columns if c != TvMn.DATETIME_COL]\n\n    def set_index_column(self, index: NDArray | pd.Series) -&gt; None:\n        \"\"\"Set the index column.\"\"\"\n        self._check_type(index, (np.ndarray, pd.Series))\n        if not self._data.empty and len(index) != len(self._data):\n            message = f\"Series index has different size than the other vectors in the table.\\nLength index: {len(index)}\\nLength vectors: {len(self._data)}\"\n            raise IndexError(message)\n        self._data[TvMn.DATETIME_COL] = index\n\n    def get_index_column(self) -&gt; pd.Series:\n        \"\"\"Get the datetime column of the dataframe.\"\"\"\n        if TvMn.DATETIME_COL not in self._data:\n            message = f\"Table at {self._source} does not have an index column. Index column must exist and be named '{TvMn.DATETIME_COL}'.\"\n            raise KeyError(message)\n        return self._data[TvMn.DATETIME_COL].copy()\n\n    def _read_metadata(self) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        if self._source is None:\n            message = \"Must set a source before reading file.\"\n            raise ValueError(message)\n        metadata = pq.ParquetFile(self._source).schema_arrow.metadata\n\n        cast_meta, __ = TvMn.cast_meta(metadata)  # ignore missing keys\n        return cast_meta\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.__init__","title":"<code>__init__(source: Path | str | None = None) -&gt; None</code>","text":"<p>Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | None</code> <p>Path to parquet file with timevectors. Defaults to None.</p> <code>None</code> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def __init__(self, source: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n    Args:\n        source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n    \"\"\"\n    super().__init__(source)\n    self._metadata = {} if self._source is None or not self._source.exists() else self._read_metadata()\n    self._data = pd.DataFrame() if self._source is None or not self._source.exists() else pd.read_parquet(self._source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_dataframe","title":"<code>get_dataframe() -&gt; pd.DataFrame</code>","text":"<p>Return a copy of all of the vector table as a pandas dataframe.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Return a copy of all of the vector table as a pandas dataframe.\"\"\"\n    return self._data.copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_index_column","title":"<code>get_index_column() -&gt; pd.Series</code>","text":"<p>Get the datetime column of the dataframe.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_index_column(self) -&gt; pd.Series:\n    \"\"\"Get the datetime column of the dataframe.\"\"\"\n    if TvMn.DATETIME_COL not in self._data:\n        message = f\"Table at {self._source} does not have an index column. Index column must exist and be named '{TvMn.DATETIME_COL}'.\"\n        raise KeyError(message)\n    return self._data[TvMn.DATETIME_COL].copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_metadata","title":"<code>get_metadata() -&gt; dict</code>","text":"<p>Get a copy of the metadata of the vectors in the parquet file.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_metadata(self) -&gt; dict:\n    \"\"\"Get a copy of the metadata of the vectors in the parquet file.\"\"\"\n    return self._metadata.copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_vector","title":"<code>get_vector(vector_id: str) -&gt; pd.Series</code>","text":"<p>Return a copy of a given vector as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_vector(self, vector_id: str) -&gt; pd.Series:\n    \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n    try:\n        return self._data[vector_id].copy()\n    except KeyError as e:\n        f\"Found no vector named '{vector_id}' in table at {self._source}.\"\n        raise KeyError from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_vector_ids","title":"<code>get_vector_ids() -&gt; list[str]</code>","text":"<p>Get the IDs of all vectors.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_vector_ids(self) -&gt; list[str]:\n    \"\"\"Get the IDs of all vectors.\"\"\"\n    return [c for c in self._data.columns if c != TvMn.DATETIME_COL]\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.save_to_parquet","title":"<code>save_to_parquet(path: Path | str) -&gt; None</code>","text":"<p>Save the edited dataframe and metadata to parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to save tha file to. Must be defined to force user to explicitly overwrite the original file if they want.</p> required Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def save_to_parquet(self, path: Path | str) -&gt; None:\n    \"\"\"\n    Save the edited dataframe and metadata to parquet file.\n\n    Args:\n        path (Path): Path to save tha file to. Must be defined to force user to explicitly overwrite the original file if they want.\n\n    \"\"\"\n    self._check_type(path, (Path, str))\n    path = Path(path)\n    table = pa.Table.from_pandas(self._data)\n\n    # ensure binary strings with defined encoding, since parquet encodes metadata anyway\n    schema_with_meta = table.schema.with_metadata({str(k).encode(TvMn.ENCODING): str(v).encode(TvMn.ENCODING) for k, v in self._metadata.items()})\n    table = pa.Table.from_pandas(self._data, schema=schema_with_meta)\n\n    pq.write_table(table, path)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_dataframe","title":"<code>set_dataframe(dataframe: pd.DataFrame) -&gt; None</code>","text":"<p>Set the dataframe of the editor.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_dataframe(self, dataframe: pd.DataFrame) -&gt; None:\n    \"\"\"Set the dataframe of the editor.\"\"\"\n    self._check_type(dataframe, pd.DataFrame)\n    self._data = dataframe\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_index_column","title":"<code>set_index_column(index: NDArray | pd.Series) -&gt; None</code>","text":"<p>Set the index column.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_index_column(self, index: NDArray | pd.Series) -&gt; None:\n    \"\"\"Set the index column.\"\"\"\n    self._check_type(index, (np.ndarray, pd.Series))\n    if not self._data.empty and len(index) != len(self._data):\n        message = f\"Series index has different size than the other vectors in the table.\\nLength index: {len(index)}\\nLength vectors: {len(self._data)}\"\n        raise IndexError(message)\n    self._data[TvMn.DATETIME_COL] = index\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_metadata","title":"<code>set_metadata(metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None</code>","text":"<p>Set the metadata dictionary (overwrites existing).</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_metadata(self, metadata: dict[str, TvMn.METADATA_TYPES]) -&gt; None:\n    \"\"\"Set the metadata dictionary (overwrites existing).\"\"\"\n    self._check_type(metadata, dict)\n    for key, value in metadata.items():\n        self._check_type(key, str)\n        self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n    self._metadata = metadata\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_metadata_by_key","title":"<code>set_metadata_by_key(key: str, value: TvMn.METADATA_TYPES) -&gt; None</code>","text":"<p>Set a field (new or overwrite) in the metadata.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_metadata_by_key(self, key: str, value: TvMn.METADATA_TYPES) -&gt; None:\n    \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n    self._check_type(key, str)\n    self._check_type(value, TvMn.METADATA_TYPES_TUPLE)\n    self._metadata[key] = value\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_vector","title":"<code>set_vector(vector_id: str, values: NDArray | pd.Series) -&gt; None</code>","text":"<p>Set a whole vector in the time vector table.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_vector(self, vector_id: str, values: NDArray | pd.Series) -&gt; None:\n    \"\"\"Set a whole vector in the time vector table.\"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(values, (np.ndarray, pd.Series))\n    if not self._data.empty and len(values) != len(self._data):\n        message = f\"Series values has different size than the other vectors in the table.\\nLength values: {len(values)}\\nLength vectors: {len(self._data)}\"\n        raise IndexError(message)\n    self._data[vector_id] = values\n</code></pre>"},{"location":"reference/#framdata.loaders","title":"<code>loaders</code>","text":""},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader","title":"<code>NVEExcelTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE excel file sources.</p> <p>Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel. Supports the followinf formats:     - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows     - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEExcelTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE excel file sources.\n\n    Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel.\n    Supports the followinf formats:\n        - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows\n        - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".xlsx\"]\n    _DATA_SHEET = \"Data\"\n    _METADATA_SHEET = \"Metadata\"\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_unit(self, vector_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the given time vector.\n\n        Args:\n            vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                             unit.\n\n        Returns:\n            str: Unit of the time vector.\n\n        \"\"\"\n        return self.get_metadata(\"\")[TvMn.UNIT]\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's excel file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = pd.DataFrame()\n        if vector_id not in self._data.columns:\n            is_horizontal = self._is_horizontal_format()\n            column_filter = [vector_id]\n            usecols = None\n            if not is_horizontal:\n                usecols = column_filter\n\n            values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n            if is_horizontal:  # Convert the table to large time series format\n                values_df = self._process_horizontal_format(values_df)\n                values_df = self._enforce_dtypes(values_df, is_horizontal)\n                self._data = values_df\n            else:\n                values_df = self._enforce_dtypes(values_df, is_horizontal)\n                self._data[vector_id] = values_df\n        return self._data[vector_id].to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the excel file's index.\n\n        \"\"\"\n        meta = self.get_metadata(\"\")\n        if self._index is None:\n            self._index = self._create_index(\n                self.get_values(TvMn.DATETIME_COL),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read Excel file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _enforce_dtypes(self, values_df: pd.DataFrame | pd.Series, issmallformat: bool) -&gt; pd.DataFrame:\n        set_dtypes = \"float\"\n        if isinstance(values_df, pd.DataFrame):\n            set_dtypes = {c: \"float\" for c in values_df.columns if c != TvMn.DATETIME_COL}\n\n        # ensure correct dtypes\n        try:\n            return values_df.astype(set_dtypes)\n        except ValueError as e:\n            index_column = TvMn.ID_COLUMN_NAME if issmallformat else TvMn.DATETIME_COL\n            message = f\"Error in {self} while reading file. All columns except '{index_column}' must consist of only float or integer numbers.\"\n            raise RuntimeError(message) from e\n\n    def _process_horizontal_format(self, horizontal_format_df: pd.DataFrame) -&gt; pd.DataFrame:\n        # We have to read the whole file to find the correct series\n\n        # Rename the id column name and then transpose to get the correct format\n        # Since the columns are counted as indices when transposing, we need to reset the index (but keep the DateTime\n        # column)\n        reformat_df = horizontal_format_df.rename(columns={TvMn.ID_COLUMN_NAME: TvMn.DATETIME_COL}).T.reset_index(drop=False)\n\n        # after transposing, column names are set a the first row, which is DateTime, IDs\n        reformat_df.columns = reformat_df.iloc[0]\n        # We reindex by dropping the first row, thus removing the row of DateTime, IDs\n        reformat_df = reformat_df.reindex(reformat_df.index.drop(0)).reset_index(drop=True)\n\n        # Since It is possible to write only year or year-month as timestamp in the table,\n        # we need to reformat to correct datetime format\n        reformat_df[TvMn.DATETIME_COL] = self._to_iso_datetimes(reformat_df[TvMn.DATETIME_COL])\n\n        return reformat_df\n\n    def _to_iso_datetimes(self, series: pd.Series) -&gt; list[datetime]:\n        \"\"\"\n        Convert a series of dates to ISO datetime format.\n\n        Args:\n            series (pd.Series): Series which values will be converted to ISO format.\n\n        Raises:\n            RuntimeError: When an input value which cannot be converted is encountered.\n\n        Returns:\n            list[datetime]: List of formatted datetimes.\n\n        \"\"\"\n        reformatted = []\n        three_segments = 3\n        two_segments = 2\n        one_segment = 1\n        for i in series:\n            new_i = str(i)\n            date_split = len(new_i.split(\"-\"))\n            space_split = len(new_i.split(\" \"))\n            time_split = len(new_i.split(\":\"))\n            try:\n                if date_split == one_segment:  # Only year is defined\n                    # get datetime for first week first day\n                    new_i = datetime.fromisocalendar(int(new_i), 1, 1)\n                elif date_split == two_segments:\n                    # Year and month is defined\n                    new_i = datetime.strptime(new_i + \"-01\", \"%Y-%m-%d\")  # Add first day\n                elif date_split == three_segments and space_split == one_segment and time_split == one_segment:\n                    # days defined but not time\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d\")\n                elif date_split == three_segments and space_split == two_segments and time_split == one_segment:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H\")\n                elif date_split == three_segments and space_split == two_segments and time_split == two_segments:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M\")\n                elif date_split == three_segments and space_split == two_segments and time_split == three_segments:\n                    # Assume time is defined\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M:%S\")\n                else:\n                    msg = f\"Could not convert value '{new_i}' to datetime format.\"\n                    raise ValueError(msg)\n            except Exception as e:\n                msg = f\"Loader {self} could not convert value '{new_i}' to datetime format. Check formatting, for example number of spaces.\"\n                raise RuntimeError(msg) from e\n            reformatted.append(new_i)\n        return sorted(reformatted)\n\n    def _is_horizontal_format(self) -&gt; bool:\n        \"\"\"Determine if the file strucure is the NVE small format.\"\"\"\n        column_names = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n        return TvMn.ID_COLUMN_NAME in column_names\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is not None:\n            return self._content_ids\n        try:\n            if self._is_horizontal_format():\n                self._content_ids = pd.read_excel(\n                    self.get_source(),\n                    usecols=[TvMn.ID_COLUMN_NAME],\n                    sheet_name=self._DATA_SHEET,\n                )[TvMn.ID_COLUMN_NAME].tolist()\n            else:\n                columns_list = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n                columns_list.remove(TvMn.DATETIME_COL)\n                self._content_ids = columns_list\n        except ValueError as e:\n            message = f\"{self}: found problem with TimeVector IDs.\"\n            raise RuntimeError(message) from e\n\n        return self._content_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Excel file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; ListTimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE excel files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>ListTimeIndex</code> <p>TimeIndex object describing the excel file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the excel file's index.\n\n    \"\"\"\n    meta = self.get_metadata(\"\")\n    if self._index is None:\n        self._index = self._create_index(\n            self.get_values(TvMn.DATETIME_COL),\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read Excel file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read Excel file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_unit","title":"<code>get_unit(vector_id: str) -&gt; str</code>","text":"<p>Get the unit of the given time vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of a time vector. Not used since all time vectors in the NVE excel files have the same              unit.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the time vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_unit(self, vector_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the given time vector.\n\n    Args:\n        vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                         unit.\n\n    Returns:\n        str: Unit of the time vector.\n\n    \"\"\"\n    return self.get_metadata(\"\")[TvMn.UNIT]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's excel file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's excel file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = pd.DataFrame()\n    if vector_id not in self._data.columns:\n        is_horizontal = self._is_horizontal_format()\n        column_filter = [vector_id]\n        usecols = None\n        if not is_horizontal:\n            usecols = column_filter\n\n        values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n        if is_horizontal:  # Convert the table to large time series format\n            values_df = self._process_horizontal_format(values_df)\n            values_df = self._enforce_dtypes(values_df, is_horizontal)\n            self._data = values_df\n        else:\n            values_df = self._enforce_dtypes(values_df, is_horizontal)\n            self._data[vector_id] = values_df\n    return self._data[vector_id].to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader","title":"<code>NVEH5TimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE HDF5 file sources.</p> <p>Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.</p> Specialized to the following format <ul> <li>index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.</li> <li>common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in                                the index group. Also used on purpose if many or all vectors have the same index.</li> <li>metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.</li> <li>common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.</li> <li>vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an                         index or metadata.</li> </ul> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEH5TimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE HDF5 file sources.\n\n    Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.\n\n    Specialized to the following format:\n        - index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.\n        - common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in\n                                       the index group. Also used on purpose if many or all vectors have the same index.\n        - metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.\n        - common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.\n        - vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an\n                                index or metadata.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".h5\", \".hdf5\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or HDF5 file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n            validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        self._file_pointer = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n        return self._data[vector_id]\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n                self._index = self._create_index(\n                    datetimes=self._read_index(vector_id),\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n            index_array = self._read_index(vector_id) if meta[TvMn.START] is None or meta[TvMn.NUM_POINTS] is None else None\n            start = meta[TvMn.START] if index_array is None else index_array[0].item()\n            num_points = meta[TvMn.NUM_POINTS] if index_array is None else index_array.size\n\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def _read_index(self, vector_id: str) -&gt; NDArray[np.datetime64]:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            decoded_index = np.char.decode(self._read_vector_field(h5f, H5Names.INDEX_GROUP, vector_id, h5py.Dataset)[()].astype(np.bytes_), encoding=\"utf-8\")\n            return decoded_index.astype(np.datetime64)\n\n    def _read_vector_field(\n        self,\n        h5file: h5py.File,\n        field_name: str,\n        vector_id: str,\n        field_type: type[h5py.Dataset | h5py.Group],\n        use_fallback: bool = True,\n    ) -&gt; h5py.Dataset | h5py.Group:\n        error = \"\"\n        if field_name in h5file:  # check if group_name exists\n            main_group = h5file[field_name]\n            if not isinstance(main_group, h5py.Group):\n                message = f\"{self} expected '{field_name}' to be a {h5py.Group} in {h5file}. Got {type(main_group)}.\"\n                raise TypeError(message)\n\n            if vector_id in main_group:\n                vector_field = main_group[vector_id]\n                if not isinstance(vector_field, field_type):\n                    message = f\"{self} expected '{vector_id}' to be a {field_type} in {h5file}. Got {type(vector_field)}\"\n                    raise TypeError(message)\n                return vector_field\n            error = f\"'{vector_id}' was not found in '{field_name}' group\"\n        else:\n            error = f\"'{field_name}' was not found in file\"\n\n        no_fallback_message = f\"{self} expected '{vector_id}' in {h5py.Group} '{field_name}' \"\n        if not use_fallback:\n            no_fallback_message += f\"but {error}.\"\n            raise KeyError(no_fallback_message)\n\n        fallback_name = H5Names.COMMON_PREFIX + field_name\n        if fallback_name in h5file:  # check if common_ + group_name exists\n            fallback_field = h5file[fallback_name]\n            if not isinstance(fallback_field, field_type):\n                message = f\"{self} expected '{fallback_field}' to be a {field_type} in {h5file}. Got {type(fallback_field)}.\"\n                raise TypeError(message)\n            return fallback_field\n\n        message = (\n            no_fallback_message\n            + f\"or a fallback {field_type} '{fallback_name}' in H5 file but \"\n            + f\"{error},\"\n            + f\" and fallback {field_type} '{fallback_name}' not found in file.\"\n        )\n        raise KeyError(message)\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            errors = set()\n            meta = {}\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n                for k, m in meta_group.items():\n                    if isinstance(m, h5py.Dataset):\n                        meta[k] = m[()]\n                    else:\n                        errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n            self._report_errors(errors)\n            self._meta = self._process_meta(meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            if H5Names.VECTORS_GROUP in h5f:\n                return list(h5f[H5Names.VECTORS_GROUP].keys())\n            message = f\"{self} required key '{H5Names.VECTORS_GROUP}' was not found in file.\"\n            raise KeyError(message)\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to a H5 file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or HDF5 file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to HDF5 file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or HDF5 file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n        validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    self._file_pointer = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n            self._index = self._create_index(\n                datetimes=self._read_index(vector_id),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n        index_array = self._read_index(vector_id) if meta[TvMn.START] is None or meta[TvMn.NUM_POINTS] is None else None\n        start = meta[TvMn.START] if index_array is None else index_array[0].item()\n        num_points = meta[TvMn.NUM_POINTS] if index_array is None else index_array.size\n\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        errors = set()\n        meta = {}\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n            for k, m in meta_group.items():\n                if isinstance(m, h5py.Dataset):\n                    meta[k] = m[()]\n                else:\n                    errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n        self._report_errors(errors)\n        self._meta = self._process_meta(meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n    return self._data[vector_id]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader","title":"<code>NVEParquetTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE parquet file sources.</p> <p>Meant for large time vectors. All vectors in the file must have the same lenghts and metadata. Supports format:     - 'Vertical' with one index collumn (DateTime) and the others containing vector values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEParquetTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE parquet file sources.\n\n    Meant for large time vectors. All vectors in the file must have the same lenghts and metadata.\n    Supports format:\n        - 'Vertical' with one index collumn (DateTime) and the others containing vector values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".parquet\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or parquet file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            table = pq.read_table(self.get_source(), columns=[vector_id])\n            self._data[vector_id] = table[vector_id].to_numpy()\n        # if self._data is None:\n        #     self._data = pq.read_table(self.get_source())\n        return self._data[vector_id]  # .to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n                datetime_index = pd.DatetimeIndex(\n                    pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                    tz=meta[TvMn.TIMEZONE],\n                ).tolist()\n                self._index = self._create_index(\n                    datetimes=datetime_index,\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n\n            parquet_file = None\n            if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n                parquet_file = pq.ParquetFile(self.get_source())\n                start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n            else:\n                start = meta[TvMn.START]\n\n            if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n                if parquet_file is None:\n                    parquet_file = pq.ParquetFile(self.get_source())\n                num_points = parquet_file.metadata.num_rows\n            else:\n                num_points = meta[TvMn.NUM_POINTS]\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        parquet_file = pq.ParquetFile(self.get_source())\n        time_vector_ids: list[str] = parquet_file.schema_arrow.names\n        time_vector_ids.remove(TvMn.DATETIME_COL)\n        return time_vector_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Parquet file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or parquet file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to parquet file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or parquet file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n            datetime_index = pd.DatetimeIndex(\n                pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                tz=meta[TvMn.TIMEZONE],\n            ).tolist()\n            self._index = self._create_index(\n                datetimes=datetime_index,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n\n        parquet_file = None\n        if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n            parquet_file = pq.ParquetFile(self.get_source())\n            start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n        else:\n            start = meta[TvMn.START]\n\n        if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n            if parquet_file is None:\n                parquet_file = pq.ParquetFile(self.get_source())\n            num_points = parquet_file.metadata.num_rows\n        else:\n            num_points = meta[TvMn.NUM_POINTS]\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        table = pq.read_table(self.get_source(), columns=[vector_id])\n        self._data[vector_id] = table[vector_id].to_numpy()\n    # if self._data is None:\n    #     self._data = pq.read_table(self.get_source())\n    return self._data[vector_id]  # .to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader","title":"<code>NVEYamlTimeVectoroader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE YAML file sources.</p> <p>Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each file. Supported format:     - Metadata: field containing dictionary with metadata for all vectors.     - Other fields are vector IDs with lists for x and y axes.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEYamlTimeVectoroader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE YAML file sources.\n\n    Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each\n    file.\n    Supported format:\n        - Metadata: field containing dictionary with metadata for all vectors.\n        - Other fields are vector IDs with lists for x and y axes.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".yaml\", \".yml\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._content_ids: list[str] = None\n\n        self._values_label: str = None\n        self._index_label: str = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of vector.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        values_list = self._data[vector_id][self._values_label]\n        if len(values_list) == 0:\n            message = f\"Time vector {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n        return np.asarray(values_list)\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get index of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with index of vector.\n\n        \"\"\"\n        meta = self.get_metadata(vector_id)  # also parses data\n        try:\n            datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n        except ValueError as e:\n            message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n            raise ValueError(message) from e\n\n        if len(datetime_list) == 0:\n            message = f\"Index of {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n\n        if (len(datetime_list) == 1 or self.get_values(vector_id).size == 1) and meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]:\n            # Even though _create_index can now handle ConstantTimeIndexes,\n            # we need to consider that YAML time vectors can have the extra end date for its final period stored in its index.\n            # That would lead to _create_time_index not creating a constant one when it should.\n            # We may remove this feature in the future.\n            return ConstantTimeIndex()\n\n        args = (\n            datetime_list,\n            meta[TvMn.IS_52_WEEK_YEARS],\n            meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n        if len(datetime_list) == len(self.get_values(vector_id)) + 1:\n            return ListTimeIndex(*args)\n        # create index with added end datetime\n        return self._create_index(*args)\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read YAML file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is None:\n            if self._data is None:\n                self._parse_file()\n            ids_list = list(self._data.keys())\n            ids_list.remove(YamlNames.metadata_field)\n            self._content_ids = ids_list\n        return self._content_ids\n\n    def _parse_file(self) -&gt; None:\n        with self.get_source().open(encoding=YamlNames.encoding) as f:\n            d = yaml.safe_load(f)\n            self._x_meta = d[YamlNames.metadata_field][YamlNames.x_field]\n            self._y_meta = d[YamlNames.metadata_field][YamlNames.y_field]\n\n            self._values_label = self._x_meta[YamlNames.attribute]\n            self._index_label = self._y_meta[YamlNames.attribute]\n\n            self._data = d\n\n    def _date_to_datetime(self, value: date | datetime) -&gt; datetime:\n        if isinstance(value, date):\n            value = datetime(value.year, value.month, value.day)\n        elif not isinstance(value, datetime):\n            message = \"Value must be date or datetime.\"\n            raise ValueError(message)\n        return value\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n\n        self._content_ids = None\n\n        self._values_label = None\n        self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Yaml file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._content_ids: list[str] = None\n\n    self._values_label: str = None\n    self._index_label: str = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n\n    self._content_ids = None\n\n    self._values_label = None\n    self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get index of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>TimeIndex</code> <p>Numpy array with index of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get index of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with index of vector.\n\n    \"\"\"\n    meta = self.get_metadata(vector_id)  # also parses data\n    try:\n        datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n    except ValueError as e:\n        message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n        raise ValueError(message) from e\n\n    if len(datetime_list) == 0:\n        message = f\"Index of {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n\n    if (len(datetime_list) == 1 or self.get_values(vector_id).size == 1) and meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]:\n        # Even though _create_index can now handle ConstantTimeIndexes,\n        # we need to consider that YAML time vectors can have the extra end date for its final period stored in its index.\n        # That would lead to _create_time_index not creating a constant one when it should.\n        # We may remove this feature in the future.\n        return ConstantTimeIndex()\n\n    args = (\n        datetime_list,\n        meta[TvMn.IS_52_WEEK_YEARS],\n        meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n        meta[TvMn.EXTRAPOLATE_LAST_POINT],\n    )\n\n    if len(datetime_list) == len(self.get_values(vector_id)) + 1:\n        return ListTimeIndex(*args)\n    # create index with added end datetime\n    return self._create_index(*args)\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read YAML file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read YAML file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get values of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of vector.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    values_list = self._data[vector_id][self._values_label]\n    if len(values_list) == 0:\n        message = f\"Time vector {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n    return np.asarray(values_list)\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader","title":"<code>NVETimeVectorLoader</code>","text":"<p>Loader for NVE time vector data.</p> <p>This module provides the NVETimeVectorLoader class, which extends FileLoader and TimeVectorLoader to handle metadata and validation for time vector data from NVE parquet files.</p>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader","title":"<code>NVETimeVectorLoader</code>","text":"<p>               Bases: <code>FileLoader</code>, <code>TimeVectorLoader</code></p> <p>Common interface for metadata in NVE TimeVectorLoaders.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>class NVETimeVectorLoader(FileLoader, TimeVectorLoader):\n    \"\"\"Common interface for metadata in NVE TimeVectorLoaders.\"\"\"\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Initialize NVETimeVectorLoader with source and optional relative location.\n\n        Args:\n            source (Path | str): Path or string to the source file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Relative location, defaults to None.\n\n        \"\"\"\n        super().__init__(source, relative_loc)\n\n        self._data: dict[str, NDArray] = None\n        self._meta: dict[str, bool | int | str | datetime | timedelta | tzinfo] = None\n\n        self._require_whole_years = require_whole_years\n\n    def is_max_level(self, vector_id: str) -&gt; bool | None:\n        \"\"\"\n        Check if the time vector is classified as a max level vector.\n\n        Args:\n            vector_id (str): ID of the time vector.\n\n        Returns:\n            bool | None: True if max level, False otherwise, or None if not specified.\n\n        \"\"\"\n        return self.get_metadata(vector_id)[TvMn.IS_MAX_LEVEL]\n\n    def is_zero_one_profile(self, vector_id: str) -&gt; bool | None:\n        \"\"\"\n        Check if the time vector is classified as a zero-one profile vector.\n\n        Args:\n            vector_id (str): ID of the time vector.\n\n        Returns:\n            bool | None: True if zero-one profile, False otherwise, or None if not specified.\n\n        \"\"\"\n        return self.get_metadata(vector_id)[TvMn.IS_ZERO_ONE_PROFILE]\n\n    def get_unit(self, vector_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the given time vector.\n\n        Args:\n            vector_id (str): ID of a time vector. Not used since all time vectors in the NVE parquet files have the same\n                             unit.\n\n        Returns:\n            str: Unit of the time vector.\n\n        \"\"\"\n        return self.get_metadata(vector_id)[TvMn.UNIT]\n\n    def get_reference_period(self, vector_id: str) -&gt; ReferencePeriod | None:\n        \"\"\"\n        Get Reference perod from metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            ValueError: If only one of start year or number of years are set in metadata.\n\n        Returns:\n            ReferencePeriod | None\n\n        \"\"\"\n        start_year = self.get_metadata(vector_id)[TvMn.REF_PERIOD_START_YEAR]\n        num_years = self.get_metadata(vector_id)[TvMn.REF_PERIOD_NUM_YEARS]\n\n        ref_period = None\n        if start_year and num_years:\n            ref_period = ReferencePeriod(start_year=start_year, num_years=num_years)\n        elif start_year or num_years:\n            message = (\n                f\"{self}: Both {TvMn.REF_PERIOD_START_YEAR} and {TvMn.REF_PERIOD_NUM_YEARS} must be provided for a valid reference period.\"\n                \"Alternatively, both must be None for undefined reference period.\"\n            )\n            raise ValueError(message)\n        return ref_period\n\n    def validate_vectors(self) -&gt; None:\n        \"\"\"\n        Validate data in all vectors contained in the Loader.\n\n        Conditions validated:\n            - If vector contains negative values.\n            (- If vector is a zero one profile and contains values outside the unit interval.) * not in use currently\n\n        Raises:\n            ValueError: When conditions are violated.\n\n        \"\"\"\n        errors = set()\n        for vector_id in self.get_ids():\n            errors |= self._validate_vector(vector_id)\n\n        if errors:\n            message = f\"Found errors in {self}:\"\n            for e in errors:\n                message += f\"\\n - {e}.\"\n\n            raise ValueError(message)\n\n    def _process_meta(self, raw_meta: dict[str | bytes, str | bytes | int | bool | None]) -&gt; dict[str, Any]:\n        processed_meta, missing_keys = TvMn.cast_meta(raw_meta)\n\n        optional_keys = {TvMn.ID_COLUMN_NAME, TvMn.FREQUENCY, TvMn.NUM_POINTS, TvMn.START}\n        missing_keys -= optional_keys\n\n        if missing_keys:\n            msg = f\"{self} could not find keys: {missing_keys} in metadata of file {self.get_source()}. Metadata: {processed_meta}\"\n            raise KeyError(msg)\n\n        return processed_meta\n\n    def _validate_vector(self, vector_id: str) -&gt; set[str]:\n        index = self.get_index(vector_id)\n        values = self.get_values(vector_id)\n\n        errors = set()\n\n        # validate index length\n        if index.get_num_periods() not in range(values.size - 1, values.size + 1):  # Since ListTimeIndex objects' num_periods can vary.\n            errors.add(f\"{vector_id} - {type(index)} with {index.get_num_periods()} periods and vector with size ({values.size}) do not match.\")\n\n        # validate negative and missing values\n        negatives = values &lt; 0\n        if np.any(negatives):\n            errors.add(f\"{vector_id} contains {negatives.sum()} negative values.\")\n        nans = np.isnan(values)\n        if np.any(nans):\n            errors.add(f\"{vector_id} contains {nans.sum()} nan values.\")\n\n        # validate that index is whole years if required\n        if self._require_whole_years and not index.is_whole_years():\n            errors.add(f\"{vector_id} is required to contain whole years but its index ({index}) is not classified as is_whole_years.\")\n\n        # outside_unit_interval = ((0 &lt;= values) &amp; (values &lt;= 1))\n        # if self.is_zero_one_profile(vector_id) and outside_unit_interval.any():\n        #     num_outside_range = outside_unit_interval.sum()\n        #     errors.add(f\"{vector_id} is classified as a zero one vector but contains {num_outside_range} values outside the range 0, 1.\")\n\n        # if not self.is_zero_one_profile(vector_id):\n        #     ref_period = self.get_reference_period(vector_id)\n        #     ref_start_date = ref_period.get_start_year()\n\n        #     index = self.get_index(vector_id)\n\n        return errors\n\n    def _create_index(\n        self,\n        datetimes: list[datetime] | NDArray[np.datetime64],\n        is_52_week_years: bool,\n        extrapolate_first_point: bool,\n        extrapolate_last_point: bool,\n    ) -&gt; ListTimeIndex | FixedFrequencyTimeIndex:\n        \"\"\"\n        Check if the index has a fixed frequency and creates wither a Fixed- or List-TimeVectorIndex based on this.\n\n        If a list index is created, the first datetime of the year following the actual final index is added as the end of the final period.\n        For example:\n            - Actual input index: [2023-1-2, 2029-12-31, 2035-1-1, 2040-1-2, 2050-1-3]\n            - Output ListTimeIndex: [2023-1-2, 2029-12-31, 2035-1-1, 2040-1-2, 2050-1-3, 2051-1-2]\n\n        \"\"\"\n        dt64_arrray = np.array(datetimes).astype(\"datetime64[us]\")  # convert to microseconds to match resolution of python tatetime\n\n        if dt64_arrray.size == 1 and extrapolate_first_point and extrapolate_last_point:\n            return ConstantTimeIndex()\n\n        diff_array = np.diff(dt64_arrray)  # get period durations between points\n        unique_array = np.unique(diff_array)  # get unique durations\n\n        if unique_array.size == 1 and dt64_arrray.size &gt; 1:  # Fixed frequency and more than one value\n            dt64_start: np.datetime64 = dt64_arrray[0]\n            td64_period_duration: np.timedelta64 = unique_array[0]\n            return FixedFrequencyTimeIndex(\n                start_time=dt64_start.item(),\n                period_duration=td64_period_duration.item(),\n                num_periods=dt64_arrray.size,\n                is_52_week_years=is_52_week_years,\n                extrapolate_first_point=extrapolate_first_point,\n                extrapolate_last_point=extrapolate_last_point,\n            )\n\n        # add end date to final period\n        dt_list = datetimes if isinstance(datetimes, list) else datetimes.astype(\"datetime64[us]\").astype(datetime).tolist()\n        end_year = dt_list[-1].isocalendar().year + 1\n        end_dt = datetime.fromisocalendar(end_year, 1, 1)\n\n        if len(dt_list) == 1:\n            start_dt = dt_list[0]\n            period_duration = end_dt - start_dt\n            return SinglePeriodTimeIndex(\n                start_time=start_dt,\n                period_duration=period_duration,\n                is_52_week_years=is_52_week_years,\n                extrapolate_first_point=extrapolate_first_point,\n                extrapolate_last_point=extrapolate_last_point,\n            )\n\n        return ListTimeIndex(\n            datetime_list=[*dt_list, end_dt],\n            is_52_week_years=is_52_week_years,\n            extrapolate_first_point=extrapolate_first_point,\n            extrapolate_last_point=extrapolate_last_point,\n        )\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None) -&gt; None</code>","text":"<p>Initialize NVETimeVectorLoader with source and optional relative location.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Path or string to the source file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Relative location, defaults to None.</p> <code>None</code> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Initialize NVETimeVectorLoader with source and optional relative location.\n\n    Args:\n        source (Path | str): Path or string to the source file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Relative location, defaults to None.\n\n    \"\"\"\n    super().__init__(source, relative_loc)\n\n    self._data: dict[str, NDArray] = None\n    self._meta: dict[str, bool | int | str | datetime | timedelta | tzinfo] = None\n\n    self._require_whole_years = require_whole_years\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.get_reference_period","title":"<code>get_reference_period(vector_id: str) -&gt; ReferencePeriod | None</code>","text":"<p>Get Reference perod from metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If only one of start year or number of years are set in metadata.</p> <p>Returns:</p> Type Description <code>ReferencePeriod | None</code> <p>ReferencePeriod | None</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def get_reference_period(self, vector_id: str) -&gt; ReferencePeriod | None:\n    \"\"\"\n    Get Reference perod from metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        ValueError: If only one of start year or number of years are set in metadata.\n\n    Returns:\n        ReferencePeriod | None\n\n    \"\"\"\n    start_year = self.get_metadata(vector_id)[TvMn.REF_PERIOD_START_YEAR]\n    num_years = self.get_metadata(vector_id)[TvMn.REF_PERIOD_NUM_YEARS]\n\n    ref_period = None\n    if start_year and num_years:\n        ref_period = ReferencePeriod(start_year=start_year, num_years=num_years)\n    elif start_year or num_years:\n        message = (\n            f\"{self}: Both {TvMn.REF_PERIOD_START_YEAR} and {TvMn.REF_PERIOD_NUM_YEARS} must be provided for a valid reference period.\"\n            \"Alternatively, both must be None for undefined reference period.\"\n        )\n        raise ValueError(message)\n    return ref_period\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.get_unit","title":"<code>get_unit(vector_id: str) -&gt; str</code>","text":"<p>Get the unit of the given time vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of a time vector. Not used since all time vectors in the NVE parquet files have the same              unit.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the time vector.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def get_unit(self, vector_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the given time vector.\n\n    Args:\n        vector_id (str): ID of a time vector. Not used since all time vectors in the NVE parquet files have the same\n                         unit.\n\n    Returns:\n        str: Unit of the time vector.\n\n    \"\"\"\n    return self.get_metadata(vector_id)[TvMn.UNIT]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.is_max_level","title":"<code>is_max_level(vector_id: str) -&gt; bool | None</code>","text":"<p>Check if the time vector is classified as a max level vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of the time vector.</p> required <p>Returns:</p> Type Description <code>bool | None</code> <p>bool | None: True if max level, False otherwise, or None if not specified.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def is_max_level(self, vector_id: str) -&gt; bool | None:\n    \"\"\"\n    Check if the time vector is classified as a max level vector.\n\n    Args:\n        vector_id (str): ID of the time vector.\n\n    Returns:\n        bool | None: True if max level, False otherwise, or None if not specified.\n\n    \"\"\"\n    return self.get_metadata(vector_id)[TvMn.IS_MAX_LEVEL]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.is_zero_one_profile","title":"<code>is_zero_one_profile(vector_id: str) -&gt; bool | None</code>","text":"<p>Check if the time vector is classified as a zero-one profile vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of the time vector.</p> required <p>Returns:</p> Type Description <code>bool | None</code> <p>bool | None: True if zero-one profile, False otherwise, or None if not specified.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def is_zero_one_profile(self, vector_id: str) -&gt; bool | None:\n    \"\"\"\n    Check if the time vector is classified as a zero-one profile vector.\n\n    Args:\n        vector_id (str): ID of the time vector.\n\n    Returns:\n        bool | None: True if zero-one profile, False otherwise, or None if not specified.\n\n    \"\"\"\n    return self.get_metadata(vector_id)[TvMn.IS_ZERO_ONE_PROFILE]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.validate_vectors","title":"<code>validate_vectors() -&gt; None</code>","text":"<p>Validate data in all vectors contained in the Loader.</p> Conditions validated <ul> <li>If vector contains negative values. (- If vector is a zero one profile and contains values outside the unit interval.) * not in use currently</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>When conditions are violated.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def validate_vectors(self) -&gt; None:\n    \"\"\"\n    Validate data in all vectors contained in the Loader.\n\n    Conditions validated:\n        - If vector contains negative values.\n        (- If vector is a zero one profile and contains values outside the unit interval.) * not in use currently\n\n    Raises:\n        ValueError: When conditions are violated.\n\n    \"\"\"\n    errors = set()\n    for vector_id in self.get_ids():\n        errors |= self._validate_vector(vector_id)\n\n    if errors:\n        message = f\"Found errors in {self}:\"\n        for e in errors:\n            message += f\"\\n - {e}.\"\n\n        raise ValueError(message)\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders","title":"<code>curve_loaders</code>","text":"<p>Contains class for loading Curve data from NVE yaml files.</p>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader","title":"<code>NVEYamlCurveLoader</code>","text":"<p>               Bases: <code>FileLoader</code>, <code>CurveLoader</code></p> <p>Handle reading of Curve data from a yaml File of NVE specific format.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>class NVEYamlCurveLoader(FileLoader, CurveLoader):\n    \"\"\"Handle reading of Curve data from a yaml File of NVE specific format.\"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list[str]] = [\".yaml\", \".yml\"]\n\n    def __init__(self, source: Path | str, relative_loc: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Handle reading of curves from a single yaml file.\n\n        Args:\n            source (Path | str): Absolute Path to database or yaml file path.\n            relative_loc (Optional[Union[Path, str]], optional): Path to yaml file relative to source. Defaults to None.\n\n        \"\"\"\n        super().__init__(source, relative_loc)\n\n        self._data = None\n        self._x_meta: str = None\n        self._y_meta: str = None\n\n        self._x_label: str = None\n        self._y_label: str = None\n\n    def get_x_axis(self, curve_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of x axis.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of x axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return np.asarray(self._data[curve_id][self._x_label])\n\n    def get_y_axis(self, curve_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of y axis.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of y axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return np.asarray(self._data[curve_id][self._y_label])\n\n    def get_x_unit(self, curve_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the x axis for the specified curve.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            str: Unit of the x axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return self._x_meta[YamlNames.unit]\n\n    def get_y_unit(self, curve_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the y axis for the specified curve.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            str: Unit of the y axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return self._y_meta[YamlNames.unit]\n\n    def get_metadata(self, content_id: str) -&gt; dict:\n        \"\"\"\n        Retrieve metadata for the specified content ID.\n\n        Args:\n            content_id (str): Unique identifier for the content.\n\n        Returns:\n            dict: Metadata associated with the content.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return self._data[YamlNames.metadata_field]\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is None:\n            if self._data is None:\n                self._parse_file()\n            ids_list = list(self._data.keys())\n            ids_list.remove(YamlNames.metadata_field)\n            self._content_ids = ids_list\n        return self._content_ids\n\n    def _parse_file(self) -&gt; None:\n        with self.get_source().open(encoding=YamlNames.encoding) as f:\n            d = yaml.safe_load(f)\n            self._x_meta = d[YamlNames.metadata_field][YamlNames.x_field]\n            self._y_meta = d[YamlNames.metadata_field][YamlNames.y_field]\n\n            self._x_label = self._x_meta[YamlNames.attribute]\n            self._y_label = self._y_meta[YamlNames.attribute]\n\n            self._data = d\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._x_meta = None\n        self._y_meta = None\n\n        self._x_label = None\n        self._y_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.__init__","title":"<code>__init__(source: Path | str, relative_loc: Path | str | None = None) -&gt; None</code>","text":"<p>Handle reading of curves from a single yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or yaml file path.</p> required <code>relative_loc</code> <code>Optional[Union[Path, str]]</code> <p>Path to yaml file relative to source. Defaults to None.</p> <code>None</code> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def __init__(self, source: Path | str, relative_loc: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Handle reading of curves from a single yaml file.\n\n    Args:\n        source (Path | str): Absolute Path to database or yaml file path.\n        relative_loc (Optional[Union[Path, str]], optional): Path to yaml file relative to source. Defaults to None.\n\n    \"\"\"\n    super().__init__(source, relative_loc)\n\n    self._data = None\n    self._x_meta: str = None\n    self._y_meta: str = None\n\n    self._x_label: str = None\n    self._y_label: str = None\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._x_meta = None\n    self._y_meta = None\n\n    self._x_label = None\n    self._y_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_metadata","title":"<code>get_metadata(content_id: str) -&gt; dict</code>","text":"<p>Retrieve metadata for the specified content ID.</p> <p>Parameters:</p> Name Type Description Default <code>content_id</code> <code>str</code> <p>Unique identifier for the content.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Metadata associated with the content.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_metadata(self, content_id: str) -&gt; dict:\n    \"\"\"\n    Retrieve metadata for the specified content ID.\n\n    Args:\n        content_id (str): Unique identifier for the content.\n\n    Returns:\n        dict: Metadata associated with the content.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return self._data[YamlNames.metadata_field]\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_x_axis","title":"<code>get_x_axis(curve_id: str) -&gt; NDArray</code>","text":"<p>Get values of x axis.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of x axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_x_axis(self, curve_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of x axis.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of x axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return np.asarray(self._data[curve_id][self._x_label])\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_x_unit","title":"<code>get_x_unit(curve_id: str) -&gt; str</code>","text":"<p>Get the unit of the x axis for the specified curve.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the x axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_x_unit(self, curve_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the x axis for the specified curve.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        str: Unit of the x axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return self._x_meta[YamlNames.unit]\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_y_axis","title":"<code>get_y_axis(curve_id: str) -&gt; NDArray</code>","text":"<p>Get values of y axis.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of y axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_y_axis(self, curve_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of y axis.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of y axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return np.asarray(self._data[curve_id][self._y_label])\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_y_unit","title":"<code>get_y_unit(curve_id: str) -&gt; str</code>","text":"<p>Get the unit of the y axis for the specified curve.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the y axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_y_unit(self, curve_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the y axis for the specified curve.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        str: Unit of the y axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return self._y_meta[YamlNames.unit]\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders","title":"<code>time_vector_loaders</code>","text":"<p>Contain classes for reading time vector data from various file types with formats specific to NVE.</p> This module provides <ul> <li>NVEExcelTimeVectorLoader: Handle time vectors in excel files.</li> <li>NVEH5TimeVectorLoader: Handle time vectors in HDF5 files.</li> <li>NVEYamlTimeVectorLoader: Handle time vectors in Yaml files.</li> <li>NVEParquetTieVectorLoader: Handle time vectors in Parquet files.</li> </ul>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader","title":"<code>NVEExcelTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE excel file sources.</p> <p>Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel. Supports the followinf formats:     - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows     - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEExcelTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE excel file sources.\n\n    Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel.\n    Supports the followinf formats:\n        - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows\n        - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".xlsx\"]\n    _DATA_SHEET = \"Data\"\n    _METADATA_SHEET = \"Metadata\"\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_unit(self, vector_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the given time vector.\n\n        Args:\n            vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                             unit.\n\n        Returns:\n            str: Unit of the time vector.\n\n        \"\"\"\n        return self.get_metadata(\"\")[TvMn.UNIT]\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's excel file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = pd.DataFrame()\n        if vector_id not in self._data.columns:\n            is_horizontal = self._is_horizontal_format()\n            column_filter = [vector_id]\n            usecols = None\n            if not is_horizontal:\n                usecols = column_filter\n\n            values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n            if is_horizontal:  # Convert the table to large time series format\n                values_df = self._process_horizontal_format(values_df)\n                values_df = self._enforce_dtypes(values_df, is_horizontal)\n                self._data = values_df\n            else:\n                values_df = self._enforce_dtypes(values_df, is_horizontal)\n                self._data[vector_id] = values_df\n        return self._data[vector_id].to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the excel file's index.\n\n        \"\"\"\n        meta = self.get_metadata(\"\")\n        if self._index is None:\n            self._index = self._create_index(\n                self.get_values(TvMn.DATETIME_COL),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read Excel file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _enforce_dtypes(self, values_df: pd.DataFrame | pd.Series, issmallformat: bool) -&gt; pd.DataFrame:\n        set_dtypes = \"float\"\n        if isinstance(values_df, pd.DataFrame):\n            set_dtypes = {c: \"float\" for c in values_df.columns if c != TvMn.DATETIME_COL}\n\n        # ensure correct dtypes\n        try:\n            return values_df.astype(set_dtypes)\n        except ValueError as e:\n            index_column = TvMn.ID_COLUMN_NAME if issmallformat else TvMn.DATETIME_COL\n            message = f\"Error in {self} while reading file. All columns except '{index_column}' must consist of only float or integer numbers.\"\n            raise RuntimeError(message) from e\n\n    def _process_horizontal_format(self, horizontal_format_df: pd.DataFrame) -&gt; pd.DataFrame:\n        # We have to read the whole file to find the correct series\n\n        # Rename the id column name and then transpose to get the correct format\n        # Since the columns are counted as indices when transposing, we need to reset the index (but keep the DateTime\n        # column)\n        reformat_df = horizontal_format_df.rename(columns={TvMn.ID_COLUMN_NAME: TvMn.DATETIME_COL}).T.reset_index(drop=False)\n\n        # after transposing, column names are set a the first row, which is DateTime, IDs\n        reformat_df.columns = reformat_df.iloc[0]\n        # We reindex by dropping the first row, thus removing the row of DateTime, IDs\n        reformat_df = reformat_df.reindex(reformat_df.index.drop(0)).reset_index(drop=True)\n\n        # Since It is possible to write only year or year-month as timestamp in the table,\n        # we need to reformat to correct datetime format\n        reformat_df[TvMn.DATETIME_COL] = self._to_iso_datetimes(reformat_df[TvMn.DATETIME_COL])\n\n        return reformat_df\n\n    def _to_iso_datetimes(self, series: pd.Series) -&gt; list[datetime]:\n        \"\"\"\n        Convert a series of dates to ISO datetime format.\n\n        Args:\n            series (pd.Series): Series which values will be converted to ISO format.\n\n        Raises:\n            RuntimeError: When an input value which cannot be converted is encountered.\n\n        Returns:\n            list[datetime]: List of formatted datetimes.\n\n        \"\"\"\n        reformatted = []\n        three_segments = 3\n        two_segments = 2\n        one_segment = 1\n        for i in series:\n            new_i = str(i)\n            date_split = len(new_i.split(\"-\"))\n            space_split = len(new_i.split(\" \"))\n            time_split = len(new_i.split(\":\"))\n            try:\n                if date_split == one_segment:  # Only year is defined\n                    # get datetime for first week first day\n                    new_i = datetime.fromisocalendar(int(new_i), 1, 1)\n                elif date_split == two_segments:\n                    # Year and month is defined\n                    new_i = datetime.strptime(new_i + \"-01\", \"%Y-%m-%d\")  # Add first day\n                elif date_split == three_segments and space_split == one_segment and time_split == one_segment:\n                    # days defined but not time\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d\")\n                elif date_split == three_segments and space_split == two_segments and time_split == one_segment:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H\")\n                elif date_split == three_segments and space_split == two_segments and time_split == two_segments:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M\")\n                elif date_split == three_segments and space_split == two_segments and time_split == three_segments:\n                    # Assume time is defined\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M:%S\")\n                else:\n                    msg = f\"Could not convert value '{new_i}' to datetime format.\"\n                    raise ValueError(msg)\n            except Exception as e:\n                msg = f\"Loader {self} could not convert value '{new_i}' to datetime format. Check formatting, for example number of spaces.\"\n                raise RuntimeError(msg) from e\n            reformatted.append(new_i)\n        return sorted(reformatted)\n\n    def _is_horizontal_format(self) -&gt; bool:\n        \"\"\"Determine if the file strucure is the NVE small format.\"\"\"\n        column_names = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n        return TvMn.ID_COLUMN_NAME in column_names\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is not None:\n            return self._content_ids\n        try:\n            if self._is_horizontal_format():\n                self._content_ids = pd.read_excel(\n                    self.get_source(),\n                    usecols=[TvMn.ID_COLUMN_NAME],\n                    sheet_name=self._DATA_SHEET,\n                )[TvMn.ID_COLUMN_NAME].tolist()\n            else:\n                columns_list = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n                columns_list.remove(TvMn.DATETIME_COL)\n                self._content_ids = columns_list\n        except ValueError as e:\n            message = f\"{self}: found problem with TimeVector IDs.\"\n            raise RuntimeError(message) from e\n\n        return self._content_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Excel file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; ListTimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE excel files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>ListTimeIndex</code> <p>TimeIndex object describing the excel file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the excel file's index.\n\n    \"\"\"\n    meta = self.get_metadata(\"\")\n    if self._index is None:\n        self._index = self._create_index(\n            self.get_values(TvMn.DATETIME_COL),\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read Excel file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read Excel file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_unit","title":"<code>get_unit(vector_id: str) -&gt; str</code>","text":"<p>Get the unit of the given time vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of a time vector. Not used since all time vectors in the NVE excel files have the same              unit.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the time vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_unit(self, vector_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the given time vector.\n\n    Args:\n        vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                         unit.\n\n    Returns:\n        str: Unit of the time vector.\n\n    \"\"\"\n    return self.get_metadata(\"\")[TvMn.UNIT]\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's excel file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's excel file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = pd.DataFrame()\n    if vector_id not in self._data.columns:\n        is_horizontal = self._is_horizontal_format()\n        column_filter = [vector_id]\n        usecols = None\n        if not is_horizontal:\n            usecols = column_filter\n\n        values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n        if is_horizontal:  # Convert the table to large time series format\n            values_df = self._process_horizontal_format(values_df)\n            values_df = self._enforce_dtypes(values_df, is_horizontal)\n            self._data = values_df\n        else:\n            values_df = self._enforce_dtypes(values_df, is_horizontal)\n            self._data[vector_id] = values_df\n    return self._data[vector_id].to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader","title":"<code>NVEH5TimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE HDF5 file sources.</p> <p>Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.</p> Specialized to the following format <ul> <li>index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.</li> <li>common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in                                the index group. Also used on purpose if many or all vectors have the same index.</li> <li>metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.</li> <li>common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.</li> <li>vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an                         index or metadata.</li> </ul> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEH5TimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE HDF5 file sources.\n\n    Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.\n\n    Specialized to the following format:\n        - index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.\n        - common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in\n                                       the index group. Also used on purpose if many or all vectors have the same index.\n        - metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.\n        - common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.\n        - vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an\n                                index or metadata.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".h5\", \".hdf5\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or HDF5 file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n            validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        self._file_pointer = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n        return self._data[vector_id]\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n                self._index = self._create_index(\n                    datetimes=self._read_index(vector_id),\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n            index_array = self._read_index(vector_id) if meta[TvMn.START] is None or meta[TvMn.NUM_POINTS] is None else None\n            start = meta[TvMn.START] if index_array is None else index_array[0].item()\n            num_points = meta[TvMn.NUM_POINTS] if index_array is None else index_array.size\n\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def _read_index(self, vector_id: str) -&gt; NDArray[np.datetime64]:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            decoded_index = np.char.decode(self._read_vector_field(h5f, H5Names.INDEX_GROUP, vector_id, h5py.Dataset)[()].astype(np.bytes_), encoding=\"utf-8\")\n            return decoded_index.astype(np.datetime64)\n\n    def _read_vector_field(\n        self,\n        h5file: h5py.File,\n        field_name: str,\n        vector_id: str,\n        field_type: type[h5py.Dataset | h5py.Group],\n        use_fallback: bool = True,\n    ) -&gt; h5py.Dataset | h5py.Group:\n        error = \"\"\n        if field_name in h5file:  # check if group_name exists\n            main_group = h5file[field_name]\n            if not isinstance(main_group, h5py.Group):\n                message = f\"{self} expected '{field_name}' to be a {h5py.Group} in {h5file}. Got {type(main_group)}.\"\n                raise TypeError(message)\n\n            if vector_id in main_group:\n                vector_field = main_group[vector_id]\n                if not isinstance(vector_field, field_type):\n                    message = f\"{self} expected '{vector_id}' to be a {field_type} in {h5file}. Got {type(vector_field)}\"\n                    raise TypeError(message)\n                return vector_field\n            error = f\"'{vector_id}' was not found in '{field_name}' group\"\n        else:\n            error = f\"'{field_name}' was not found in file\"\n\n        no_fallback_message = f\"{self} expected '{vector_id}' in {h5py.Group} '{field_name}' \"\n        if not use_fallback:\n            no_fallback_message += f\"but {error}.\"\n            raise KeyError(no_fallback_message)\n\n        fallback_name = H5Names.COMMON_PREFIX + field_name\n        if fallback_name in h5file:  # check if common_ + group_name exists\n            fallback_field = h5file[fallback_name]\n            if not isinstance(fallback_field, field_type):\n                message = f\"{self} expected '{fallback_field}' to be a {field_type} in {h5file}. Got {type(fallback_field)}.\"\n                raise TypeError(message)\n            return fallback_field\n\n        message = (\n            no_fallback_message\n            + f\"or a fallback {field_type} '{fallback_name}' in H5 file but \"\n            + f\"{error},\"\n            + f\" and fallback {field_type} '{fallback_name}' not found in file.\"\n        )\n        raise KeyError(message)\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            errors = set()\n            meta = {}\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n                for k, m in meta_group.items():\n                    if isinstance(m, h5py.Dataset):\n                        meta[k] = m[()]\n                    else:\n                        errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n            self._report_errors(errors)\n            self._meta = self._process_meta(meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            if H5Names.VECTORS_GROUP in h5f:\n                return list(h5f[H5Names.VECTORS_GROUP].keys())\n            message = f\"{self} required key '{H5Names.VECTORS_GROUP}' was not found in file.\"\n            raise KeyError(message)\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to a H5 file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or HDF5 file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to HDF5 file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or HDF5 file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n        validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    self._file_pointer = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n            self._index = self._create_index(\n                datetimes=self._read_index(vector_id),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n        index_array = self._read_index(vector_id) if meta[TvMn.START] is None or meta[TvMn.NUM_POINTS] is None else None\n        start = meta[TvMn.START] if index_array is None else index_array[0].item()\n        num_points = meta[TvMn.NUM_POINTS] if index_array is None else index_array.size\n\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        errors = set()\n        meta = {}\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n            for k, m in meta_group.items():\n                if isinstance(m, h5py.Dataset):\n                    meta[k] = m[()]\n                else:\n                    errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n        self._report_errors(errors)\n        self._meta = self._process_meta(meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n    return self._data[vector_id]\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader","title":"<code>NVEParquetTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE parquet file sources.</p> <p>Meant for large time vectors. All vectors in the file must have the same lenghts and metadata. Supports format:     - 'Vertical' with one index collumn (DateTime) and the others containing vector values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEParquetTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE parquet file sources.\n\n    Meant for large time vectors. All vectors in the file must have the same lenghts and metadata.\n    Supports format:\n        - 'Vertical' with one index collumn (DateTime) and the others containing vector values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".parquet\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or parquet file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            table = pq.read_table(self.get_source(), columns=[vector_id])\n            self._data[vector_id] = table[vector_id].to_numpy()\n        # if self._data is None:\n        #     self._data = pq.read_table(self.get_source())\n        return self._data[vector_id]  # .to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n                datetime_index = pd.DatetimeIndex(\n                    pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                    tz=meta[TvMn.TIMEZONE],\n                ).tolist()\n                self._index = self._create_index(\n                    datetimes=datetime_index,\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n\n            parquet_file = None\n            if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n                parquet_file = pq.ParquetFile(self.get_source())\n                start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n            else:\n                start = meta[TvMn.START]\n\n            if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n                if parquet_file is None:\n                    parquet_file = pq.ParquetFile(self.get_source())\n                num_points = parquet_file.metadata.num_rows\n            else:\n                num_points = meta[TvMn.NUM_POINTS]\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        parquet_file = pq.ParquetFile(self.get_source())\n        time_vector_ids: list[str] = parquet_file.schema_arrow.names\n        time_vector_ids.remove(TvMn.DATETIME_COL)\n        return time_vector_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Parquet file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or parquet file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to parquet file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or parquet file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n            datetime_index = pd.DatetimeIndex(\n                pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                tz=meta[TvMn.TIMEZONE],\n            ).tolist()\n            self._index = self._create_index(\n                datetimes=datetime_index,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n\n        parquet_file = None\n        if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n            parquet_file = pq.ParquetFile(self.get_source())\n            start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n        else:\n            start = meta[TvMn.START]\n\n        if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n            if parquet_file is None:\n                parquet_file = pq.ParquetFile(self.get_source())\n            num_points = parquet_file.metadata.num_rows\n        else:\n            num_points = meta[TvMn.NUM_POINTS]\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        table = pq.read_table(self.get_source(), columns=[vector_id])\n        self._data[vector_id] = table[vector_id].to_numpy()\n    # if self._data is None:\n    #     self._data = pq.read_table(self.get_source())\n    return self._data[vector_id]  # .to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader","title":"<code>NVEYamlTimeVectoroader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE YAML file sources.</p> <p>Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each file. Supported format:     - Metadata: field containing dictionary with metadata for all vectors.     - Other fields are vector IDs with lists for x and y axes.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEYamlTimeVectoroader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE YAML file sources.\n\n    Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each\n    file.\n    Supported format:\n        - Metadata: field containing dictionary with metadata for all vectors.\n        - Other fields are vector IDs with lists for x and y axes.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".yaml\", \".yml\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._content_ids: list[str] = None\n\n        self._values_label: str = None\n        self._index_label: str = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of vector.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        values_list = self._data[vector_id][self._values_label]\n        if len(values_list) == 0:\n            message = f\"Time vector {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n        return np.asarray(values_list)\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get index of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with index of vector.\n\n        \"\"\"\n        meta = self.get_metadata(vector_id)  # also parses data\n        try:\n            datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n        except ValueError as e:\n            message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n            raise ValueError(message) from e\n\n        if len(datetime_list) == 0:\n            message = f\"Index of {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n\n        if (len(datetime_list) == 1 or self.get_values(vector_id).size == 1) and meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]:\n            # Even though _create_index can now handle ConstantTimeIndexes,\n            # we need to consider that YAML time vectors can have the extra end date for its final period stored in its index.\n            # That would lead to _create_time_index not creating a constant one when it should.\n            # We may remove this feature in the future.\n            return ConstantTimeIndex()\n\n        args = (\n            datetime_list,\n            meta[TvMn.IS_52_WEEK_YEARS],\n            meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n        if len(datetime_list) == len(self.get_values(vector_id)) + 1:\n            return ListTimeIndex(*args)\n        # create index with added end datetime\n        return self._create_index(*args)\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read YAML file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is None:\n            if self._data is None:\n                self._parse_file()\n            ids_list = list(self._data.keys())\n            ids_list.remove(YamlNames.metadata_field)\n            self._content_ids = ids_list\n        return self._content_ids\n\n    def _parse_file(self) -&gt; None:\n        with self.get_source().open(encoding=YamlNames.encoding) as f:\n            d = yaml.safe_load(f)\n            self._x_meta = d[YamlNames.metadata_field][YamlNames.x_field]\n            self._y_meta = d[YamlNames.metadata_field][YamlNames.y_field]\n\n            self._values_label = self._x_meta[YamlNames.attribute]\n            self._index_label = self._y_meta[YamlNames.attribute]\n\n            self._data = d\n\n    def _date_to_datetime(self, value: date | datetime) -&gt; datetime:\n        if isinstance(value, date):\n            value = datetime(value.year, value.month, value.day)\n        elif not isinstance(value, datetime):\n            message = \"Value must be date or datetime.\"\n            raise ValueError(message)\n        return value\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n\n        self._content_ids = None\n\n        self._values_label = None\n        self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Yaml file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._content_ids: list[str] = None\n\n    self._values_label: str = None\n    self._index_label: str = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n\n    self._content_ids = None\n\n    self._values_label = None\n    self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get index of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>TimeIndex</code> <p>Numpy array with index of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get index of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with index of vector.\n\n    \"\"\"\n    meta = self.get_metadata(vector_id)  # also parses data\n    try:\n        datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n    except ValueError as e:\n        message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n        raise ValueError(message) from e\n\n    if len(datetime_list) == 0:\n        message = f\"Index of {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n\n    if (len(datetime_list) == 1 or self.get_values(vector_id).size == 1) and meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]:\n        # Even though _create_index can now handle ConstantTimeIndexes,\n        # we need to consider that YAML time vectors can have the extra end date for its final period stored in its index.\n        # That would lead to _create_time_index not creating a constant one when it should.\n        # We may remove this feature in the future.\n        return ConstantTimeIndex()\n\n    args = (\n        datetime_list,\n        meta[TvMn.IS_52_WEEK_YEARS],\n        meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n        meta[TvMn.EXTRAPOLATE_LAST_POINT],\n    )\n\n    if len(datetime_list) == len(self.get_values(vector_id)) + 1:\n        return ListTimeIndex(*args)\n    # create index with added end datetime\n    return self._create_index(*args)\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read YAML file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read YAML file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get values of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of vector.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    values_list = self._data[vector_id][self._values_label]\n    if len(values_list) == 0:\n        message = f\"Time vector {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n    return np.asarray(values_list)\n</code></pre>"}]}